1

Sélection de modèle en régression linéaire

Sélection de modèle en régression linéaire

Les données sont supposées provenir de l’observation d’un échantillon statistique de taille n (n > p + 1) de R(p+1) :
(x1i , . . . , xji , . . . , xpi , yi) i = 1, . . . , n.

Résumé
Le modèle linéaire gaussien ou régression multiple est considéré avec pour objectif la prévision d’une variable quantitative par un ensemble de variables quantitatives ou un mélande de quantitatives et qualitatives (analyse de covariance). Recherche d’un modèle parcimonieux assurant un bon équilibre entre la qualité de l’ajustement et la variance des paramètres aﬁn de minimiser le risque empirique. Algorithmes (backward, forward, stepwise...) de sélection de modèle par sélection de variables et minimisation de critères pénalisés (Cp, AIC, BIC). Algorithmes de sélection de modèle par pénalisation ridge, Lasso, elastic net. Retour à l’introduction. Tous les tutoriels sont disponibles sur le dépôt : github.com/wikistat
1 Régression multiple
Les modèles classiques de régession (linéaire, logistique) sont anciens et moins l’occasion de battage médiatique que ceux récents issus de l’apprentissage machine. Néanmoins, compte tenu de leur robustesse, de leur stabilité face à des ﬂuctuations des échantillons, de leur capacité à passer à l’échelle des données massives... tout ceci fait qu’ils restent toujours très utilisés en production notamment lorsque la fonction à modéliser est bien linéaire et qu’il serait contre productif de chercher plus compliqué.
1.1 Modèle
Une variable quantitative Y dite à expliquer (ou encore, réponse, exogène, dépendante) est mise en relation avec p variables quantitatives X1, . . . , Xp dites explicatives (ou encore de contrôle, endogènes, indépendantes, régresseurs, prédicteurs).

L’écriture du modèle linéaire dans cette situation conduit à supposer que l’espérance de Y appartient au sous-espace de Rn engendré par {1, X1, . . . , Xp} où 1 désigne le vecteur de Rn constitué de 1s. C’est-à-dire que les (p + 1) variables aléatoires vériﬁent :
Yi = β0 + β1Xi1 + β2Xi2 + · · · + βpXip + εi i = 1, 2, . . . , n
avec les hypothèses suivantes :
1. Les εi sont des termes d’erreur indépendants et identiquement distribués ; E(εi) = 0, V ar(ε) = σ2I.
2. Les termes Xj sont supposés déterministes (facteurs contrôlés) ou bien l’erreur ε est indépendante de la distribution conjointe de X1, . . . , Xp. On écrit dans ce dernier cas que :
E(Y|X1, . . . , Xp) = β0+β1X1+β2X2+· · ·+βpXp et V ar(Y|X1, . . . , Xp) = σ2.
3. Les paramètres inconnus β0, . . . , βp sont supposés constants.
4. En option, pour l’étude spéciﬁque des lois des estimateurs, une quatrième hypothèse considère la normalité de la variable d’erreur ε (N (0, σ2I)). Les εi sont alors i.i.d. de loi N (0, σ2).
Les données sont rangées dans une matrice X(n × (p + 1)) de terme général Xij, dont la première colonne contient le vecteur 1 (X0i = 1), et dans un vecteur Y de terme général Yi. En notant les vecteurs ε = [ε1 · · · εp] et β = [β0β1 · · · βp] , le modèle s’écrit matriciellement :
Y = Xβ + ε.
1.2 Estimation
Conditionnellement à la connaissance des valeurs des Xj, les paramètres inconnus du modèle : le vecteur β et σ2 (paramètre de nuisance), sont estimés par minimisation des carrés des écarts (M.C.) ou encore, en supposant (4.), par maximisation de la vraisemblance (M.V.). Les estimateurs ont alors les mêmes expressions, l’hypothèse de normalité et l’utilisation de la vraisemblance conférant à ces derniers des propriétés complémentaires.

2

Sélection de modèle en régression linéaire

1.3 Estimation par moindres carrés

1.4 Propriétés

L’expression à minimiser sur β ∈ Rp+1 s’écrit :

n
(Yi − β0 − β1Xi1 − · · · − βpXip)2 =
i=1

Y − Xβ 2

= Y Y − 2β X Y + β X Xβ.

Par dérivation matricielle de la dernière équation on obtient les équations normales :
X Y − X Xβ = 0

Les estimateurs des M.C. β0, β1, . . . , βp sont des estimateurs sans biais : E(β) = β, et, parmi les estimateurs sans biais fonctions linéaires des Yi, ils sont de variance minimum (théorème de Gauss-Markov) ; ils sont donc BLUE : best linear unbiaised estimators. Sous hypothèse de normalité, les estimateurs du M.V. sont uniformément meilleurs (efﬁcaces) et coïncident avec ceux des moindres carrés.

On montre que la matrice de covariance des estimateurs se met sous la

forme

E[(β − β)(β − β) ] = σ2(X X)−1,

dont la solution correspond bien à un minimum car la matrice hessienne 2X X est semi déﬁnie-positive.
Nous faisons l’hypothèse supplémentaire que la matrice X X est inversible, c’est-à-dire que la matrice X est de rang (p + 1) et donc qu’il n’existe pas de colinéarité entre ses colonnes. Si cette hypothèse n’est pas vériﬁée, il sufﬁt en principe de supprimer des colonnes de X et donc des variables du modèle. Une approche de réduction de dimension (régression ridge, Lasso, PLS ...) est à mettre en œuvre.
Alors, l’estimation des paramètres βj est donnée par :
β = (X X)−1X Y

et les valeurs ajustées (ou estimées, prédites) de Y ont pour expression :

−1
Y = Xβ = X(X X) X Y = HY

où H = X(X X)−1X (hat matrix). Géométriquement, c’est la matrice de projection orthogonale dans Rn sur le sous-espace Vect(X) engendré par les
vecteurs colonnes de X.

On note

e = Y − Y = Y − Xβ = (I − H)Y

celle des prédicteurs est E[(Y − Xβ)(Y − Xβ) ] = σ2H

et celle des estimateurs des résidus est E[ee ] = σ2(I − H)

tandis qu’un estimateur sans biais de σ2 est fourni par :

σ2 =

e2

Y − Xβ 2

SSE

=

=

.

n−p−1 n−p−1 n−p−1

Ainsi, les termes σ2hii sont des estimations des variances des prédicteurs Yi.
Conséquence importante : si la matrice X X est mal conditionnée (déterminant proche de 0), son inversion fait apparaître des termes très élevés sur la diagonale et conduit donc à des variances très importantes des estimations des paramètres.

1.5 Sommes des carrés

SSE est la somme des carrés des résidus (sum of squared errors),

SSE =

2
Y−Y =

e 2.

le vecteur des résidus ; c’est la projection de Y sur le sous-espace orthogonal de Vect(X) dans Rn.

On déﬁnit également la somme totale des carrés (total sum of squares) par SST = Y − Y1 2 = Y Y − nY2

3

Sélection de modèle en régression linéaire

et la somme des carrés de la régression (regression sum of squares) par SSR = Y − Y1 2 = Y Y − nY2 = Y HY − nY2 = β X Y − nY2.

suit une loi de Student à (n − p − 1) degrés de liberté. Cette statistique est donc utilisée pour tester une hypothèse H0 : βj = a ou pour construire un intervalle de conﬁance de niveau 100(1 − α)% :

On vériﬁe alors : SST = SSR + SSE.

βj ± tα/2;(n−p−1)σj2.

1.6 Coefﬁcient de détermination
On appelle coefﬁcient de détermination le rapport
R2 = SSR SST
qui est donc la part de variation de Y expliquée par le modèle de régression. Géométriquement, c’est un rapport de carrés de longueur de deux vecteurs. C’est donc le cosinus carré de l’angle entre ces vecteurs : Y et sa projection Y sur Vect(X).
La quantité R est appelée coefﬁcient de corrélation multiple entre Y et les variables explicatives, c’est le coefﬁcient de corrélation usuel entre Y et sa prévision Y.
Par construction, le coefﬁcient de détermination croît avec le nombre p de vartiables.
1.7 Inférence dans le cas gaussien
En principe, l’hypothèse optionnelle (4.) de normalité des erreurs est nécessaire pour cette section. En pratique, des résultats asymptotiques, donc valides pour de grands échantillons, ainsi que des études de simulation, montrent que cette hypothèse n’est pas celle dont la violation est la plus pénalisante pour la ﬁabilité des modèles.
Inférence sur les coefﬁcients
Pour chaque coefﬁcient βj on note σj2 l’estimateur de la variance de βj obtenu en prenant j-ème terme diagonal de la matrice σ2(X X)−1. On montre que la statistique
βj − βj σj

Attention, cette statistique concerne un coefﬁcient et ne permet pas d’inférer conjointement sur d’autres coefﬁcients car leurs estimateurs sont corrélés. De plus elle dépend des absences ou présences des autres variables Xk dans le modèle. Par exemple, dans le cas particulier de deux variables X1 et X2 très corrélées, chaque variable, en l’absence de l’autre, peut apparaître avec un coefﬁcient signiﬁcativement différent de 0 ; mais, si les deux sont présentes dans le modèle, l’une peut apparaître avec un coefﬁcient insigniﬁant.
De façon plus générale, si c désigne un vecteur non nul de (p+1) constantes réelles, il est possible de tester la valeur d’une combinaison linéaire c β des paramètres en considérant l’hypothèse nulle H0 : c β = a ; a connu. Sous H0, la statistique
cβ−a (σ2c (X X)−1c)1/2
suit une loi de Student à (n − p − 1) degrés de liberté.

Inférence sur le modèle

Le modèle peut être testé globalement. Sous l’hypothèse nulle H0 : β1 = β2 = . . . = βp = 0, la statistique

SSR/p

MSR

=

SSE/(n − p − 1) MSE

suit une loi de Fisher avec p et (n − p − 1) degrés de liberté. Les résultats sont habituellement présentés dans un tableau d’analyse de la variance” sous la forme suivante :

Source de

Somme des

variation

d.d.l. carrés

Variance

F

Régression

p

SSR

MSR=SSR/p

MSR/MSE

Erreur

n−p−1

SSE

MSE=SSE/(n − p − 1)

Total

n−1

SST

4

Sélection de modèle en régression linéaire

Inférence sur un modèle réduit

Le test précédent amène à rejeter H0 dès que l’une des variables Xj est liée à Y. Il est donc d’un intérêt limité. Il est souvent plus utile de tester un modèle réduit c’est-à-dire dans lequel certains coefﬁcients, à l’exception de la constante, sont nuls contre le modèle complet avec toutes les variables. En

ayant éventuellement réordonné les variables, on considère l’hypothèse nulle

H0 : β1 = β2 = . . . = βq = 0, q < p.

Notons respectivement SSRq, SSEq, Rq2 les sommes de carrés et le coefﬁcient de détermination du modèle réduit à (p − q) variables. Sous H0, la

statistique

(SSR − SSE/(n

SSRq )/q − p − 1)

=

(1

(R2 − Rq2)/q − R2)/(n − p −

1)

suit une loi de Fisher à q et (n − p − 1) degrés de liberté.

Dans le cas particulier où q = 1 (βj = 0), la F -statistique est alors le carré de la t-statistique de l’inférence sur un paramètre et conduit donc au même test.

• homoscédasticité : variance σ2 des résidus constante, • linéarité du modèle : paramètres βj constant, • absence de points inﬂuents par la distance de Cook :

1

Di

=

s2(p

+

(y 1)

−

y(i))

(y

−

y(i)),

• éventuellement la normalité des résidus, • le conditionnement de la matrice X X. Tracer le graphe des résidus standardisés en fonction des valeurs ajustés montre leur plus ou moins bonne répartition autour de l’axe y = 0. La forme de ce nuage est susceptible de dénoncer une absence de linéarité ou une hétéroscédasticité.
Le conditionnement de la matrice X X est indiqué par le rapport κ = λ1/λp où λ1, . . . , λp sont les valeurs propres de la matrice des corrélations R rangées par ordre décroissant. Ainsi, des problèmes de variances excessives voire même de précision numérique apparaissent dès que les dernières valeurs propres sont relativement trop petites.

1.8 Prévision
Connaissant les valeurs des variables Xj pour une nouvelle observation : x0 = [x10, x20, . . . , xp0] appartenant au domaine dans lequel l’hypothèse de linéarité reste valide, une prévision, notée y0 de Y ou E(Y) est donnée par :
y0 = β0 + β1x10 + · · · + βpxp0.
Les intervalles de conﬁance des prévisions de Y et E(Y), pour une valeur x0 ∈ Rp et en posant v0 = (1|x0) ∈ Rp+1, sont respectivement
y0 ± tα/2;(n−p−1)σ(1 + v0(X X)−1v0)1/2, y0 ± tα/2;(n−p−1)σ(v0(X X)−1v0)1/2.
Les variances de ces prévisions, comme celles des estimations des paramètres, dépendent directement du conditionnement de la matrice X X.

1.10 Exemple
Les données sont extraites de Jobson (1991)[3] et décrivent les résultats comptables de 40 entreprises du Royaume Uni.

RETCAP WCFTDT LOGSALE LOGASST CURRAT QUIKRAT NFATAST FATTOT PAYOUT WCFTCL GEARRAT CAPINT INVTAST

Return on capital employed Ratio of working capital ﬂow to total debt Log to base 10 of total sales Log to base 10 of total assets Current ratio Quick ratio Ratio of net ﬁxed assets to total assets Gross sixed assets to total assets Payout ratio Ratio of working capital ﬂow to total current liabilities Gearing ratio (debt-equity ratio) Capital intensity (ratio of total sales to total assets) Ratio of total inventories to total assets

1.9 Diagnostics

Modèle complet

La validité d’un modèle de régression multiple et donc la ﬁabilité des pré- La procédure SAS/REG fournit les résultats classiques de la régression

visions, dépendent de la bonne vériﬁcation des hypothèses :

multiple.

5

Sélection de modèle en régression linéaire

Analysis of Variance

Sum of

Mean

Source

DF

Squares

Square

F Value

(1)

Model

12

0.55868 (2)

0.04656 (5) 8.408 (7)

Error

27

0.14951 (3)

0.00554 (6)

C Total

39

0.70820 (4)

Root MSE

0.07441 (9) R-square

0.7889 (12)

Dep Mean

0.14275 (10) Adj R-sq

0.6951 (13)

C.V.

52.12940 (11)

Prob>F 0.0001 (8)

(1) degrés de liberté de la loi de Fisher du test global
(2) SSR
(3) SSE ou déviance
(4) SST=SSE+SSR
(5) SSR/DF (6) MSE=SSE/DF est l’estimation de σ2 (7) Statistique F du test de Fisher du modèle global (8) P (fp;n−p−1 > F ) ; H0 est rejetée au niveau α si P < α (9) s =racine de MSE
(10) moyenne empirique de la variable à expliquée (11) Coefﬁcient de variation 100× (9)/(10) (12) Coefﬁcient de détermination R2 (13) Coefﬁcient de détermination ajusté R 2

Parameter Estimates

Parameter

Variable DF

Estimate

(1)

INTERCEP 1

0.188072

WCFTCL

1

0.215130

WCFTDT

1

0.305557

GEARRAT 1

-0.040436

LOGSALE 1

0.118440

LOGASST 1

-0.076960

...

Standard Error
(2) 0.13391661 0.19788455 0.29736579 0.07677092 0.03611612 0.04517414

T for H0:

Parameter=0 Prob>|T|

(3)

(4)

1.404

0.1716

1.087

0.2866

1.028

0.3133

-0.527

0.6027

3.279

0.0029

-1.704

0.0999

Tolerance (5)
. 0.03734409 0.02187972 0.45778579 0.10629382 0.21200778

Variance Inflation
(6) 0.00000000 26.77799793 45.70441500
2.18442778 9.40788501 4.71680805

(1) estimations des paramètres (βj ) (2) écarts-types de ces estimations σj (3) statistique T du test de Student de H0 : βj = 0 (4) P (tn−p−1 > T ) ; H0 est rejetée au niveau α si P < α (5) 1 − R(2j) (6) VIF=1/(1 − R(2j))
Ces résultats soulignent les problèmes de colinéarités. De grands VIF (facteurs d’inﬂation de la variance) sont associés à de grands écart-types des estimations des paramètres. D’autre part les nombreux tests de Student non signiﬁcatifs montrent que trop de variables sont présentes dans le modèle. Cette idée est renforcée par le calcul de l’indice de conditionnement : 8.76623/0.00125.

2 Analyse de covariance (AnCoVa

L’analyse de covariance se situe encore dans le cadre général du modèle linéaire et où une variable quantitative est expliquée par plusieurs variables à la fois quantitatives et qualitatives. Les cas les plus complexes associent plusieurs facteurs (variables qualitatives) avec une structure croisée ou hiérarchique ainsi que plusieurs variables quantitatives intervenant de manière linéaire ou polynomiale. Le principe général, dans un but explicatif ou décisionnel, est toujours d’estimer des modèles intra-groupes et de faire apparaître (tester) des effets différentiels inter-groupes des paramètres des régressions. Ainsi, dans le cas plus simple où seulement une variable parmi les explicatives est quantitative, des tests interrogent l’hétérogénéité des constantes et celle des pentes (interaction) entre différents modèles de régression linéaire.
Ce type de modèle permet également, avec un objectif prédictif, de s’intéresser à la modélisation d’une variable quantitative par un ensemble de variables explicatives à la fois quantitatives et qualitatives.
La possible prise en compte d’interactions entre les variables complique la procédure de sélection de variables.

2.1 Modèle

Le modèle est explicité dans le cas élémentaire où une variable quantitative

Y est expliquée par une variable qualitative T à J niveaux et une variable

quantitative, appelée encore covariable, X. Pour chaque niveau j de T , on

observe nj valeurs X1j, . . . , Xnjj de X et nj valeurs Y1j, . . . , Ynjj de Y ;

n=

J j=1

nj

est

la

taille

de

l’échantillon.

En pratique, avant de lancer une procédure de modélisation et tests, une

démarche exploratoire s’appuyant sur une représentation en couleur (une par

modalité j de T) du nuage de points croisant Y et X et associant les droites de

régression permet de se faire une idée sur les effets respectifs des variables :

parallélisme des droites, étirement, imbrication des sous-nuages.

On suppose que les moyennes conditionnelles E[Y|T ], c’est-à-dire calculées à l’intérieur de chaque cellule, sont dans le sous-espace vectoriel engendré par les variables explicatives quantitatives, ici X. Ceci s’écrit :

Yij = β0j + β1jXij + εij ; j = 1, . . . , J ; i = 1, · · · , nj

6

Sélection de modèle en régression linéaire

où les εij sont i.i.d. suivant une loi centrée de variance σ2 qui sera supposée N (0, σ2) pour la construction des tests.
Notons Y le vecteur des observations [Yij|i = 1, nj; j = 1, J] mis en colonne, x le vecteur [Xij|i = 1, nj; j = 1, J] , ε = [εij|i = 1, nj; j = 1, J] le vecteur des erreurs, 1j les variables indicatrices des niveaux et 1 la colonne de 1s. On note encore x.1j le produit terme à terme des deux vecteurs, c’està-dire le vecteur contenant les observations de x sur les individus prenant le niveau j de T et des zéros ailleurs.
La résolution simultanée des J modèles de régression est simplement obtenue en considérant globalement le modèle :
Y = Xβ + ε
dans lequel X est la matrice n × 2J constituée des blocs [1j|X.1j] ; j = 1, . . . , J. L’estimation de ce modèle global conduit, par bloc, à estimer les modèles de régression dans chacune des cellules.
Comme pour l’analyse de variance (AnOVa), les logiciels opèrent une reparamétrisation faisant apparaître des effets différentiels par rapport au dernier niveau ou par rapport à un effet moyen, aﬁn d’obtenir directement les bonnes hypothèses dans les tests. Ainsi, dans le premier cas, on considère la matrice de même rang (sans la Jème indicatrice)

à chacun des modèles réduits :

(i) (ii) (iii)
(iv)

Y = β0J 1 + (β01 − β0J )11 + · · · + (β0J−1 − β0J )1J−1 + β1J x + ε Y = β0J 1 + (β01 − β0J )11 + · · · + (β0J−1 − β0J )1J−1 + ε Y = β0J 1 + β1J x + (β1j − β1J )x.11 + · · · +
+(β1J−1 − β1J )x.1J−1 + ε Y = β0J 1 + ε

par un test de Fisher. Ceci revient à considérer les hypothèses suivantes : • H0i : pas d’interaction entre variables X et T, β11 = · · · = β1J , les droites partagent la même pente β1J . • H0ii : β11 = · · · = β1J =0 (pas d’effet de x) • H0iii :β01 = · · · = β0J , les droites partagent la même constante à l’origine β0J . • H0iv les variables X et T n’ont aucun effet sur Y.
Commencer par évaluer i ; si le test n’est pas signiﬁcatif, regarder ii qui,
s’il n’est pas non plus signiﬁcatif, conduit à l’absence d’effet de la variable
X. De même, toujours si i n’est pas signiﬁcatif, s’intéresser à iii pour juger de
l’effet du facteur T .

3 Choix de modèle par sélection de variables

X = [1|X|11| · · · |1J−1|x.11| · · · |x.1J−1]

3.1 Introduction

associée aux modèles : Yij = β0J + (β0j − β0J ) + β1J Xij + (β1j − β1J )Xij + εij ; j = 1, . . . , J − 1; i = 1, . . . , nj.
2.2 Tests
Différentes hypothèses sont alors testées en comparant le modèle complet Y = β0J 1 + (β01 − β0J )11 + · · · + (β0J−1 − β0J )1J−1 + β1J x +
+ (β11 − β1J )x.11 + · · · + (β1J−1 − β1J )x.1J−1 + ε

De façon schématique, la pratique de la modélisation statistique vise trois objectifs éventuellement complémentaires.
Descriptif : rechercher de façon exploratoire les liaisons entre Y et d’autres variables, potentiellement explicatives, Xj qui peuvent être nombreuses aﬁn, par exemple d’en sélectionner un sous-ensemble. À cette stratégie, à laquelle peuvent contribuer des Analyses en Composantes Principales, correspond des algorithmes de recherche (pas à pas) moins performants mais économiques en temps de calcul si p est grand.
Attention, si n est petit, et la recherche sufﬁsamment longue avec beaucoup de variables explicatives, il sera toujours possible de trouver un modèle expliquant y ; c’est l’effet data mining dans les modèles économétriques appelé maintenant data snooping.

7

Sélection de modèle en régression linéaire

Explicatif : Le deuxième objectif est sous-tendu par une connaissance a priori du domaine concerné et dont des résultats théoriques peuvent vouloir être conﬁrmés, inﬁrmés ou précisés par l’estimation des paramètres. Dans ce cas, les résultats inférentiels permettent de construire le bon test conduisant à la prise de décision recherchée. Utilisées hors de ce contexte, les statistiques de test n’ont qu’une valeur indicative au même titre que d’autres critères plus empiriques.

Prédictif : Dans le troisième cas, l’accent est mis sur la qualité des prévisions. C’est la situation rencontrée en apprentissage. Ceci conduit à rechercher des modèles parcimonieux c’est-à-dire avec un nombre volontairement restreint de variables explicatives pour réduire la variance. Le modèle ainsi obtenu peut favoriser des estimateurs biaisés au proﬁt d’une variance plus faible même si le théorème de Gauss-Markov indique que, parmi les estimateurs sans biais, celui des moindres carrés est de variance minimum. Un bon modèle n’est donc plus celui qui explique le mieux les données au sens d’un R2 maximum mais celui conduisant aux prévisions les plus ﬁables.

FIGURE 1 – Régression polynomiale : ajustement par, à gauche, y = β0 + β1x + , et à droite, y = β0 + β1x + β2x2 + β3x3 +

Ceci est illustré ceci par un exemple simple (mais pédagogique) en régression polynomiale : Les ﬁgures 1 et 2) représentent un jeu de données simulées : Yi = f (xi) + εi, i = 1, . . . , n et xi ∈ [0, 1] sur lesquelles des polynômes de degrés croissants sont ajustés. L’ajustement du modèle mesuré par le R2 croît logiquement avec le nombre de paramètres et atteint la valeur 1 lorque le polynôme interpole les observations.
Le R2 ne peut-être un bon critère de sélection de modèles ; il ne peut servir qu’à comparer des modèles de même dimension car sinon conduit à sélectionner le modèle le plus complexe, c’est-à-dire celui correspond au plus grand espace de projection, et conduit donc au sur-ajustement.

Il y a principalement deux façons de biaiser un modèle linéaire dans le but de restreindre la variance :
• en réduisant le nombre de variables explicatives et donc en simpliﬁant le modèle (sélection ou pénalisation Lasso l1),
• en contraignant les paramètres du modèle, en les rétrécissant (schrinkage), par une régression ridge qui opère une régularisation par pénalisation l2.

FIGURE 2 – Régression polynomiale : ajustement par, à gauche : y = β0 + β1x + . . . + β5x5 + , et à droite, y = β0 + β1x + . . . + β10x10 + .

8

Sélection de modèle en régression linéaire

3.2 Critères de sélection de variables

De nombreux critères de choix de modèle sont présentés dans la littérature sur la régression linéaire multiple.

Une tradition ancienne, encore présente dans certains livres ou fonctions de logiciels, propose d’utiliser la statistique du test de Fisher de comparaison d’un modèle avec un sous-modèle comme critère de sélection de variables. Attention, la signiﬁcativité de la présence d’une variable basée sur la p-valeur du test de nullité de son coefﬁcient n’est du tout une indication sur l’importance de cette variable pour la qualité de la prévision. Explicatif ou prédictif sont des objectifs différents de la modélisation ; ne pas les confondre. D’autre part, pour éviter le caractère croissant du coefﬁcient de détermination R2 en fonction du nombre de variables, une version pénalisée a été proposée : le R2 ajusté mais qui conduit très généralement à des modèles trop complexes. Ces deux approches : statistique du test de Fisher et R2 ajusté sont à oublier.

D’autres critères sont eux basés sur une qualité de prévision. Le Cp de Mallows, le critère d’information d’Akaïke (AIC), celui bayésien de Sawa (BIC). . . Ils sont équivalents, également avec le R2, lorsque le nombre de variables à sélectionner, ou complexité du modèle, est ﬁxé. Le choix du critère est déterminant lorsqu’il s’agit de comparer des modèles de complexitée différentes. Certains critères se ramènent, dans le cas gaussien, à l’utilisation d’une expression pénalisée de la fonction de vraisemblance aﬁn de favoriser des modèles parcimonieux. En pratique, les plus utilisés ou ceux généralement fournis par les logiciels sont les suivants.

Cp de Mallows

L’indicateur proposé par Mallows (1973)[5] est une estimation de l’erreur

quadratique moyenne de prévision qui s’écrit aussi comme la somme d’une

variance et du carré d’un biais. L’erreur quadratique moyenne de prévision

s’écrit ainsi :

MSE(Yi) = Var(Yi) + [Biais(Yi)]2

puis après sommation et réduction :

FIGURE 3 – Cp de Mallows en fonction du degré du polynôme et modèle sélectionné de degré 3.

En supposant que les estimations du modèle complet sont sans biais et en utilisant des estimateurs de Var(Yi) et σ2, l’expression de l’erreur quadratique moyenne totale standardisée (ou réduite) pour un modèle à j variables expli-
catives s’écrit :

Cp

=

(n − q − 1) MSEj MSE

− [n − 2(q + 1)]

et déﬁnit la valeur du Cp de Mallows pour les q variables considérées. Il est alors d’usage de rechercher un modèle qui minimise le Cp généralement proche de (q + 1). Ceci revient à considérer que le "vrai" modèle complet est
moins ﬁable qu’un modèle réduit donc biaisé mais d’estimation plus précise.

La ﬁgure 3 montre le comportement du Cp dans l’exemple de la régression polynomial. Ce critère décroît avec le biais jusqu’à un choix optimal de dimension 3 avant de ré-augmenter avec la variance.

1 σ2

n

1

MSE(Yi) = σ2

n

1

Var(Yi) + σ2

n
[Biais(Yi)]2.

i=1

i=1

i=1

AIC, BIC et PRESS
Dans le cas du modèle linéaire, et si la variance des observations est supposée connue, le critère AIC (Akaïke’s Information criterion) est équivalent au

9

Sélection de modèle en régression linéaire

critère Cp de Mallows.
Le PRESS de Allen (1974)[?] est l’introduction historique de la validation croisée ou leave one out (loo). On désigne par Y(i) la prévision de Yi calculée sans tenir compte de la ième observation (Yi, Xi1, . . . , Xip), la somme des erreurs quadratiques de prévision (PRESS) est déﬁnie par

Mixte (stepwise) Cet algorithme introduit une étape d’élimination de variable après chaque étape de sélection aﬁn de retirer du modèle d’éventuels variables qui seraient devenues moins indispensables du fait de la présence de celles nouvellement introduites.
Global

1n n

yi − f (−i)(xi)

2

=

1 n

n

i=1

i=1

2
yi − f (xi) . 1 − hii

et permet de comparer les capacités prédictives de deux modèles.
La vignette sur Qualité de prévision et risque donne plus de détails sur ces derniers critères.

3.3 Algorithmes de sélection de variables

Dans le cas général et évidemment le plus courant en pratique, les variables ne sont pas pré-ordonnées par importance. Lorsque p est grand, il n’est pas raisonnable de penser explorer les 2p modèles possibles aﬁn de sélectionner le meilleur au sens de l’un des critères ci-dessus. Différentes stratégies sont donc proposées qui doivent être choisies en fonction de l’objectif recherché, de la valeur de p et des moyens de calcul disponibles. Deux types d’algorithmes sont résumés ci-dessous par ordre croissant de temps de calcul nécessaire c’està-dire par nombre croissant de modèles considérés parmi les 2p et donc par capacité croissante d’optimalité.

Pas à pas

Stratégie correspondant à la fonction StepAIC de R. Comme écrit cidessus, oublier les sélections basées sur la statistique de Fisher.
Sélection (forward) À chaque pas, une variable est ajoutée au modèle. C’est celle qui permet de réduire au mieux le critère AIC du modèle obtenu. La procédure s’arrête lorsque toutes les variables sont introduites ou lorsque AIC ne décroît plus.

L’algorithme de Furnival et Wilson (1974)[2] (librairie leaps de R) est utilisé pour comparer tous les modèles possibles en cherchant à optimiser l’un des critères : Cp, AIC, BIC choisi par l’utilisateur. Par souci d’économie, cet algorithme évite de considérer des modèles de certaines sous-branches de l’arborescence dont on peut savoir a priori qu’ils ne sont pas compétitifs. Cet algorithme afﬁchent le ou les meilleurs modèles de chaque niveau q. Rappel : à q ﬁxé tous les critères sont équivalents mais les choix de q optimal peut différer d’un critère à l’autre. Il n’est pas raisonnable de considérer plus d’une quinzaine de variables avec cet algorithme.
3.4 Sélection en analyse de covariance
Un modèle d’analyse de covariance pose des problèmes spéciﬁques de sélection notamment par la prise en compte possible d’interactions entre variables dans la déﬁnition du modèle. La recherche d’un modèle efﬁcace, donc parcimonieux, peut conduire à négliger des interactions ou effets principaux lorsqu’une faible amélioration du R2 le justiﬁe et même si le test correspondant apparaît comme signiﬁcatif. L’utilisation du Cp est théoriquement possible mais en général ce critère n’est pas calculé car d’utilisation délicate. En effet, il nécessite la considération d’un modèle de référence sans biais ou tout du moins d’un modèle de faible biais pour obtenir une estimation raisonnable de la variance de l’erreur. En régression multiple (toutes les variables explicatives quantitatives), le modèle complet est considéré comme étant celui de faible biais mais en analyse de covariance quels niveaux de complexité des interactions faut-il considérer pour construire le modèle complet jugé de faible biais ? Il est alors plus simple et plus efﬁcace d’utiliser le critère AIC, choix par défaut dans plusieurs logiciels comme R.

Élimination (backward) L’algorithme démarre cette fois du modèle com- L’algorithme de recherche descendant est le plus couramment utilisé avec

plet. À chaque étape, la variable dont l’élimination conduit à l’AIC le la contrainte suivante :

plus faible est supprimée. La procédure s’arrête lorsque AIC ne décroît plus.

un effet principal n’est supprimé qu’à la condition qu’il n’apparaisse plus

10

Sélection de modèle en régression linéaire

dans une interaction.
Voici, à titre d’exemple, une étape intermédiaire d’une sélection de variables pas à pas stepwize avec l’option both de la fonction StepAIC de R. A chaque étape, le critère AIC est évalué par suppression ou rajout de chacune des variables. L’option minimisant le critère AIC est retenue avant de passer à l’étape suivante. Le modèle ne comprend pas d’interactions.

Step: AIC=-60.79 lpsa ~ lcavol + lweight + age + lbph + svi + pgg45

- pgg45 <none> + lcp - age - lbph + gleason - lweight - svi - lcavol

Df Sum of Sq RSS 1 0.6590 45.526
44.867 1 0.6623 44.204 1 1.2649 46.132 1 1.6465 46.513 3 1.2918 43.575 1 3.5646 48.431 1 4.2503 49.117 1 25.4190 70.286

AIC -61.374 -60.788 -60.231 -60.092 -59.293 -57.622 -55.373 -54.009 -19.248

Step: AIC=-61.37 lpsa ~ lcavol + lweight + age + lbph + svi

En effet, supprimer un effet principal qualitatif alors que la variable est présente dans une interaction ne change en rien le modèle car l’espace engendré par l’ensemble des indicatrices sélectionnées reste le même ; la matrice X est construite sous contrainte de rang et retirer une colonne (effet principal) fait automatiquement entrer une indicatrice d’interaction supplémentaire. Le modèle est inchangé mais l’interprétation plus compliquée car le modèle ne se décompose plus en un effet principal puis ses interactions.
3.5 Exemple de sélection
Tous les modèles (parmi les plus intéressants selon l’algorithme de Furnival et Wilson) sont considérés. Seul le meilleur pour chaque niveau, c’està-dire pour chaque valeur p du nombre de variables explicatives sont donnés. Il est alors facile de choisir celui minimisant l’un des critères globaux (Cp ou BIC). Cet exemple calculé avec SAS est choisi pour comparer différents critères.

/ selection=rsquare cp rsquare bic best=1; run;

N = 40

Regression Models for Dependent Variable: RETCAP

R-sq. Adjust. C(p) BIC Variables in Model

In

R-sq

1 0.105 0.081 78.393 -163.2 WCFTCL

2 0.340 0.305 50.323 -173.7 WCFTDT QUIKRAT

3 0.615 0.583 17.181 -191.1 WCFTCL NFATAST CURRAT

4 0.720 0.688 5.714 -199.2 WCFTDT LOGSALE NFATAST CURRAT

5 0.731 0.692 6.304 -198.0 WCFTDT LOGSALE NFATAST QUIKRAT CURRAT

6 0.748 0.702 6.187 -197.2 WCFTDT LOGSALE NFATAST INVTAST QUIKRAT CURRAT

7 0.760 0.707 6.691 -195.7 WCFTDT LOGSALE LOGASST NFATAST FATTOT QUIKRAT CURRAT

8 0.769 0.709 7.507 -193.8 WCFTDT LOGSALE LOGASST NFATAST FATTOT INVTAST QUIKRAT CURRAT

9 0.776 0.708 8.641 -191.5 WCFTCL WCFTDT LOGSALE LOGASST NFATAST FATTOT INVTAST QUIKRAT

CURRAT

10 0.783 0.708 9.744 -189.1 WCFTCL WCFTDT LOGSALE LOGASST NFATAST FATTOT INVTAST PAYOUT

QUIKRAT CURRAT

11 0.786 0.702 11.277 -186.4 WCFTCL WCFTDT LOGSALE LOGASST NFATAST CAPINT FATTOT INVTAST

PAYOUT QUIKRAT CURRAT

12 0.788 0.695 13.000 -183.5 WCFTCL WCFTDT GEARRAT LOGSALE LOGASST NFATAST CAPINT FATTOT

INVTAST PAYOUT QUIKRAT CURRAT

Dans cet exemple, Cp et BIC se comportent de la même façon. Avec peu de variables, le modèle est trop biaisé. Ils atteignent un minimum pour un modèle à 4 variables explicatives puis croissent de nouveau selon la première bissectrice. La maximisation du R2 ajusté conduirait à une solution beaucoup moins parcimonieuse. On note par ailleurs que l’algorithme remplace WCFTCL par WCFTDT. Un algorithme par sélection ne peut pas aboutir à la solution optimale retenue.
4 Régression régularisée ou pénalisée
4.1 Régression ridge
Modèle et estimation
Ayant diagnostiqué un problème mal conditionné mais désirant conserver toutes les variables explicatives pour des raisons d’interprétation, il est possible d’améliorer les propriétés numériques et la variance des estimations en considérant un estimateur biaisé des paramètres par une procédure de régularisation.

options linesize=110 pagesize=30 nodate nonumber; title; proc reg data=sasuser.ukcomp2 ; model RETCAP = WCFTCL WCFTDT GEARRAT LOGSALE
NFATAST CAPINT FATTOT INVTAST PAYOUT

LOGASST QUIKRAT

CURRAT

Soit le modèle linéaire :

Y = Xβ + ,

11

Sélection de modèle en régression linéaire

où

1

X

=

 



1 .

X11 X21
.

X12 X22
.

. . .

XX21pp .



 

,



1 Xn1 Xn2 . Xnp

β0

β1

β

=

 

.

 

,

 

.

 

βp

β1

β2

β

=

 

.

 

 

.

 

βp

où X0 = (1, 1, . . . , 1) , et X désigne la matrice X privée de sa première
colonne. L’estimateur ridge est déﬁni par un critère des moindres carrés, avec une pénalité de type L2 :

DÉFINITION 1. — L’estimateur ridge de β dans le modèle

Y = Xβ + ,

est déﬁni par :



n

p


p

βridge = argminβ∈Rp+1  (Yi − Xi(j)βj )2 + λ βj2 ,

i=1

j=0

j=1

où λ est un paramètre positif, à choisir.

Supposant désormais que X et Y sont centrés, l’estimateur ridge est obtenue en résolvant les équations normales qui s’expriment sous la forme :

X Y = (X X + λIp)β.

Conduisant à :

βridge = (X X + λIp)−1X Y.

La solution est donc explicite et linéaire en Y.

Remarques :

1. X X est une matrice symétrique positive (pour tout vecteur u de Rp, u (X X)u = Xu 2 ≥ 0. Il en résulte que pour tout λ > 0, X X +
λIp est nécessaire inversible.

2. La constante β0 n’intervient pas dans la pénalité, sinon, le choix de l’origine pour Y aurait une inﬂuence sur l’estimation de l’ensemble des paramètres. Alors : β0 = Y ; ajouter une constante à Y ne modiﬁe pas les βj pour j ≥ 1.

3. L’estimateur ridge n’est pas invariant par renormalisation des vecteurs X(j), il est préférable de normaliser (réduire les variables) les vecteurs
avant de minimiser le critère.

4. La régression ridge revient encore à estimer le modèle par les moindres carrés sous la contrainte que la norme du vecteur β des paramètres ne soit pas trop grande :

À noter que le paramètre β0 n’est pas pénalisé.

βridge

=

arg

min
β

Y − Xβ 2 ; β 2 < c .

PROPOSITION 2. — L’estimateur ridge s’exprime aussi sous la forme :

 β1

β2

 

β0ridge = Y¯ ,

 

.

 

. 

= argminβ∈Rp

βp ridge

Y(c) − X(c)β 2 + λ β 2 .

où X(c) désigne la matrice X recentrée (par colonnes) et Y(c) désigne le vecteur Y recentré.

La régression ridge conserve toutes les variables mais, contraignant la norme des paramètres βj, elle les empêche de prendre de trop grandes valeurs et limite ainsi la variance des prévisions.
Optimisation de la pénalisation
La ﬁgure 4 montre quelques résultats obtenus par la méthode ridge en fonction de la valeur de la pénalité λ = l sur l’exemple de la régression polynomiale. Plus la pénalité augmente et plus la solution obtenue est régulière ou encore, plus le biais augmente et la variance diminue. Il y a sur-ajustement

12

Sélection de modèle en régression linéaire

20

10

0

t(x$coef)

−10

−20

FIGURE 4 – Pénalisation ridge du modèle polynomial

avec une pénalité nulle : le modèle passe par tous les points mais oscille dangereusement ; il y a sous-ajustement avec une pénalité trop grande.
Comme dans tout problème de régularisation, le choix de la valeur du paramètre λ est crucial est déterminera le choix de modèle. La validation croisée est généralement utilisée pour optimiser le choix car la lecture du graphique (cf. ﬁgure 5) montrant l’évolution des paramètres en fonction du coefﬁcient ou chemins de régularisation ridge n’est pas sufﬁsante pour déterminer une valeur optimale.
Le principe de la validation croisée qui permet d’estimer sans biais une erreur de prévision est détaillé par ailleurs.

0e+00

1e−04

2e−04

3e−04

x$lambda

4e−04

5e−04

FIGURE 5 – Modèle polynomial : Chemin de régularisation en régression ridge en fonction du paramètre de la pénalisation.

dans la régression ridge). Soit β 1 =

p j=1

|βj

|.

DÉFINITION 3. — L’estimateur Lasso de β dans le modèle

Y = Xβ + ,

4.2 Régression LASSO

est déﬁni par :

La régression ridge permet donc de contourner les problèmes de colinéarité même en présence d’un nombre important de variables explicatives ou prédicteurs (p > n). La principale faiblesse de cette méthode est liée aux difﬁcultés d’interprétation car, sans sélection, toutes les variables sont concernées dans le modèle. D’autres approches par pénalisation permettent également une sélection, c’est le cas de la régression Lasso.
Modèle et estimation
La méthode Lasso (Tibshirani, 1996)[8] correspond à la minimisation d’un critère des moindres carrés avec une pénalité de type l1 (et non plus l2 comme



n

p


p

βLasso = argminβ∈Rp  (Yi − Xi(j)βj )2 + λ |βj | ,

i=1

j=0

j=1

où λ est un paramètre positif, à choisir.

On peut montrer que ceci équivaut au problème de minimisation suivant : βLasso = argminβ, β 1≤t( Y − Xβ 2),
pour un t convenablement choisi.

13

Sélection de modèle en régression linéaire

Comme dans le cas de la régression ridge, le paramètre λ est un paramètre de régularisation :
• Si λ = 0, on retrouve l’estimateur des moindres carrés. • Si λ tend vers l’inﬁni, on annule tous les βˆj, j = 1, . . . , p. La solution obtenue est dite parcimonieuse (sparse en anglais), car elle comporte des coefﬁcients nuls.
Autre pénalisation

(correspondant à des βj différents de 0) ne dépasse pas n et que les variables non inﬂuentes ne soient pas trop corrélées avec celles qui le sont.
4.3 Elastic Net
La méthode Elastic Net permet de combiner la régression ridge et la régression Lasso, en introduisant les deux types de pénalités simultanément.
Le critère à minimiser est :

La méthode Lasso équivaut à minimiser le critère

n
Crit(β) = (Yi − β0 − β1Xi(1) − β2Xi(2) − . . . − βpXi(p))2
i=1

sous la contrainte

p j=1

|βj

|

≤

t,

pour

un

t

>

0.

Le logiciel R introduit une contrainte sous forme d’une borne relative pour

p j=1

|βj

|

:

la

contrainte

s’exprime

sous

la

forme

n
(Yi − β0 − β1Xi(1) − β2Xi(2) − . . . − βpXi(p))2

i=1


p


p

+λ α |βj| + (1 − α) βj2

j=1

j=1

• Pour α = 1, on retrouve la méthode LASSO. • Pour α = 0, on retrouve la régression ridge.

p

p

|βj | ≤ κ |βˆj(0)|,

j=1

j=1

Il y a dans ce dernier cas deux paramètres à optimiser par validation croisée.
4.4 Sélection par réduction de dimension

où βˆ(0) est l’estimateur des moindres carrés et κ ∈ [0, 1].
Avec κ = 1 c’est l’estimateur des moindres carrés (pas de contrainte) et pour κ = 0, tous les βˆj, j ≥ 1, sont nuls (contrainte maximale).
Utilisation de la régression Lasso
La pénalisation est optimisée comme en régression ridge par validation croisée.
Grâce à ses solutions parcimonieuses, cette méthode est surtout utilisée pour sélectionner des variables dans des modèles de grande dimension ; on peut l’utiliser si p > n c’est-à-dire s’il y a plus de variables que d’observations. Bien entendu, dans ce cas, les colonnes de la matrice X ne sont pas linéairement indépendantes. Il n’y a donc pas de solution explicite, on utilise des procédures d’optimisation pour trouver la solution. Il faut néanmoins utiliser la méthode avec précaution lorsque les variables explicatives sont corrélées. Pour que la méthode fonctionne, il faut que le nombre de variables inﬂuentes

Le principe de ces approches consiste à calculer la régression sur un ensemble de variables orthogonales deux à deux. Celles-ci peuvent être obtenues à la suite d’une analyse en composantes principales ou par décomposition en valeur singulière de la matrice X : c’est la régression sur les composantes principales associées aux plus grandes valeurs propres.
L’autre approche ou régression PLS (partial least square consiste à rechercher itérativement une composante linéaire des variables de plus forte covariance avec la variable à expliquer sous une contrainte d’orthogonalité avec les composantes précédentes.
Ce deux méthodes sont développées dans une vignette spéciﬁque.
5 Exemples
5.1 Prévision de la concentration d’ozone

14

Sélection de modèle en régression linéaire

Les données

Les données proviennent des services de Météo-France et s’intéresse à la prévision de la concentration en Ozone dans 5 stations de mesure ; ces sites ont été retenus pour le nombre important de pics de pollution qui ont été détectés dans les périodes considérées (étés 2002, 2003, 2005). Un pic de pollution est déﬁni ici par une concentration dépassant le seuil de 150µg/m3. Météo-France dispose déjà d’une prévision (MOCAGE), à partir d’um modèle physique basé sur les équations du comportement dynamique de l’atmosphère (Navier et Stockes). Cette prévision fait partie du dispositif d’alerte des pouvoirs publics et prévoit donc une concentration de pollution à 17h locale pour le lendemain. L’objet du travail est d’en faire une évaluation statistique puis de l’améliorer en tenant compte d’autres variables ou plutôt d’autres prévisions faites par Météo-France. Il s’agit donc d’intégrer ces informations dans un modèle statistique global.
Les variables

Valeurs observees 0 50 150 250

0 50 100

200

300

Valeurs predites

FIGURE 6 – Ozone : prévision et résidus du modèle MOCAGE de MétéoFrance pour 5 stations.

Certaines variables de concentration ont été transformées aﬁn de rendre symétrique (plus gaussienne) leur distribution.
O3-o Concentration d’ozone effectivement observée ou variable à prédire, 03-pr prévision "mocage" qui sert de variable explicative ; Tempe Température prévue pour le lendemain, vmodule Force du vent prévue pour le lendemain, lno Logarithme de la concentration observée en monoxyde d’azote, lno2 Logarithme de la concentration observée en dioxyde d’azote, rmh20 Racine de la concentration en vapeur d’eau, Jour Variable à deux modalités pour distinguer les jours "ouvrables" (0)
des jours "fériés-WE" (1). Station Une variable qualitative indique la station concernée : Aix-en-
Provence, Rambouillet, Munchhausen, Cadarache, et Plan de Cuques.
Modèle physique

le modèle physique MOCAGE. Ces graphes témoignent de la mauvaise qualité de ce modèle : les résidus ne sont pas répartis de façon symétrique et les deux nuages présentent une légère forme de "banane" signiﬁant que des composantes non linéaires du modèle n’ont pas été prises en compte. D’autre part, la forme d’entonnoir des résidus montrent une forte hétéroscédasticité. Cela signiﬁe que la variance des résidus et donc des prévisions croît avec la valeur. En d’autre terme, la qualité de la prévision se dégrade pour les concentrations élevées justement dans la zone sensible.
Modèle sans interaction
Un premier modèle est estimé avec R :
fit.lm=lm(O3-o~O3-pr+vmodule+lno2+lno+s-rmh2o+ jour+station+TEMPE,data=donne)
Il introduit l’ensemble des variables explicatives mais sans interaction. Les résultats numériques sont fournis ci-dessous.

Les graphiques de la ﬁgure 6 représente la première prévision de la concen-

Coefficients:

tration d’ozone observée, ainsi que ses résidus, c’est-à-dire celle obtenue par

Estimate Std. Error t value Pr(>|t|)

15

Sélection de modèle en régression linéaire

(Intercept) -4.99738

O3_pr

0.62039

vmodule

-1.73179

lno2

-48.17248

lno

50.95171

s_rmh2o

135.88280

jour1

-0.34561

stationAls 9.06874

stationCad 14.31603

stationPla 21.54765

stationRam 6.86130

TEMPE

4.65120

7.87028 0.05255 0.35411 6.19632 5.98541 50.69567 1.85389 3.37517 3.07893 3.74155 3.05338 0.23170

-0.635 0.52559
11.805 < 2e-16 *** -4.891 1.17e-06 *** -7.774 1.83e-14 ***
8.513 < 2e-16 *** 2.680 0.00747 ** -0.186 0.85215
2.687 0.00733 ** 4.650 3.76e-06 *** 5.759 1.12e-08 *** 2.247 0.02484 * 20.074 < 2e-16 ***

Residual standard error: 27.29 on 1028 degrees of freedom

Multiple R-Squared: 0.5616,

Adjusted R-squared: 0.5569

F-statistic: 119.7 on 11 and 1028 DF, p-value: < 2.2e-16

A l’exception de la variable indiquant la nature du jour, l’ensemble des coefﬁcients sont jugés signiﬁcativement différent de zéro mais la qualité de l’ajustement est faible (R2).
Modèle avec interaction
La qualité d’ajustement du modèle précédent n’étant pas très bonne, un autre modèle est considéré en prenant en compte les interactions d’ordre 2 entre les variables. Compte tenu de la complexité du modèle qui un découle, un choix automatique est lancé par élimination successive des termes non signiﬁcatifs (algorithme backward). Le critère optimisé est celui (AIC) d’Akaïke. Plusieurs interactions ont été éliminées au cours de la procédure mais beaucoup subsistent dans le modèle. Attention, les effets principaux lno2, vmodule ne peuvent être retirés car ces variables apparaissent dans une interaction. En revanche on peut s’interroger sur l’opportunité de conserver celle entre la force du vent et la concentration de dioxyde d’azote.

NULL O3_pr station vmodule lno2 s_rmh2o TEMPE O3_pr:station O3_pr:vmodule O3_pr:TEMPE station:vmodule

Df Deviance Resid. Df Resid. Dev

F Pr(>F)

1039 1745605

1 611680

1038 1133925 969.9171 < 2.2e-16 ***

4 39250

1034 1094674 15.5594 2.339e-12 ***

1

1151

1033 1093523 1.8252 0.1769957

1

945

1032 1092578 1.4992 0.2210886

1 24248

1031 1068330 38.4485 8.200e-10 ***

1 248891

1030

819439 394.6568 < 2.2e-16 ***

4 16911

1026

802528 6.7038 2.520e-05 ***

1

8554

1025

793974 13.5642 0.0002428 ***

1 41129

1024

752845 65.2160 1.912e-15 ***

4

7693

1020

745152 3.0497 0.0163595 *

FIGURE 7 – Ozone : Résidus des modèles linéaire et quadratique.

station:lno2

4 12780

station:s_rmh2o 4 19865

station:TEMPE

4 27612

vmodule:lno2

1

1615

vmodule:s_rmh2o 1

2407

lno2:TEMPE

1

4717

s_rmh2o:TEMPE

1 42982

1016 1012 1008 1007 1006 1005 1004

732372 712508 684896 683280 680873 676156 633175

5.0660 0.0004811 *** 7.8746 2.997e-06 *** 10.9458 1.086e-08 *** 2.5616 0.1098033
3.8163 0.0510351 .
7.4794 0.0063507 ** 68.1543 4.725e-16 ***

Ce sont surtout les graphes de la ﬁgure 7 qui renseignent sur l’adéquation des modèles. Le modèle quadratique fournit une forme plus "linéaire" des résidus et un meilleur ajustement avec un R2 de 0,64 mais l’hétéroscédasticité reste présente, d’autres approches s’avèrent nécessaires aﬁn de réduire la variance liée à la prévision des concentrations élevées.
Sélection Lasso
Les résulats précédents ont été obtenus avec R qui propose des algorithmes de sélection de variables classique. Ce n’est pas le cas de la librairie scikit-learn qui se limite à des sélections par pénalisation Lasso mais sans pouvoir intégrer facilement les interactions alors que celle-ci sont justement importantes pour ces données. L’optimisation du paramètre de pénalisation et les chemins de régularisation sont obtenus par validation croisée (ﬁgur 8).
Encore un peu de travail est nécessaire pour obtenir les coefﬁcients ﬁnale-

16

Sélection de modèle en régression linéaire

FIGURE 8 – Ozone : Régression lasso ; chemin de régularisation des paramètres et optimisation de la pénalisation.

ment sélectionnés par la procédure.
5.2 Données de spectrométrie NIR

FIGURE 9 – Cookies : Régression ridge ; chemin de régularisation des paramètres et optimisation de la pénalisation avec scikit-learn en Python.

Objectif
Ce type de problème se rencontre en contrôle de qualité sur une chaîne de fabrication agroalimentaire, ici des biscuits (cookies). Il est nécessaire de contrôler le mélange des ingrédients avant cuisson aﬁn de s’assurer que les proportions en lipides, sucre, farine, eau, sont bien respectées. Il s’agit de savoir s’il est possible de dépister au plus tôt une dérive aﬁn d’intervenir sur les équipements concernés. Les mesures et analyses, faites dans un laboratoire classique de chimie, sont relativement longues et coûteuses ; elles ne peuvent être entreprises pour un suivi régulier ou même en continue de la production. Dans ce contexte, un spectromètre en proche infrarouge (NIR) mesure l’absorbance c’est-à-dire les spectres dans les longueurs d’ondes aﬁn de construire un modèle de prévision de la concentration en sucre.
Les données
Les données originales sont dues à Osbone et al. (1984) [6] et ont été souvent utilisées pour la comparaison de méthodes (Stone et al. 1990 [7], Brown et al. 2001 [1], Krämer et al. 2008 [4]). Elles sont accessibles dans R au sein de la librairie ppls. Les mesures ont été faites sur deux échantillons, l’un de taille

40 prévu pour l’apprentissage, l’autre de taille 32 pour les tests. Pour chacun de ces 72 biscuits, les compositions en lipides, sucre, farine, eau, sont mesurées par une approche classique tandis que le spectre est observé sur toutes les longueurs d’ondes entre 1100 et 2498 nanomètres, régulièrement espacés de 2 nanomètres. Nous avons donc 700 valeurs observées, ou variables potentiellement explicatives, par échantillon de pâte à biscuit.
Résultats par régression pénalisée
Typiquement, cette étude se déroule dans un contexte de très grande dimension avec p >> n. L’étude détaillée de ces données fait l’objet d’un scénario avec le logiciel R.
Voici quelques résultats partiels concernant les méthodes de régression par régression ridge et régression LASSO. La comparaison globale des résultats des différentes approches de modélisation est reportée en conclusion.
Références
[1] P.J. Brown, T. Fearn et M. Vannucci, Bayesian Wavelet Regression on

17

Sélection de modèle en régression linéaire

least squares and principal components regression, Journal of The Royal Statistical Society B 52 (1990), 237–269.
[8] R. Tibshirani, Regression shrinkage and selection via the lasso, J. Royal. Statist. Soc B 58 (1996), 267–288.

FIGURE 10 – Cookies : Régression lasso ; chemin de régularisation des paramètres et optimisation de la pénalisation.
Curves with Applications to a Spectroscopic Calibration Problem, Journal of the American Statistical Society 96 (2001), 398–408. [2] G. M. Furnival et R. W. Wilson, Regression by leaps and bounds, Technometrics 16 (1974), 499–511. [3] J.D. Jobson, Applied Multivariate Data Analysis, t. I : Regression and experimental design, Springer-Verlag, 1991. [4] Nicole Krämer, Anne Laure Boulesteix et Gerhard Tutz, Penalized Partial Least Squares with applications to B-spline transformations and functional data, Chemometrics and Intelligent Laboratory Systems 94 (2008), 60–69. [5] C.L. Mallows, Some Comments on Cp, Technometrics 15 (1973), 661– 675. [6] B. G. Osborne, T. Fearn, A. R. Miller et S. Douglas, Application of Near Infrared Reﬂectance spectroscopy to the compositional analysis of biscuits and biscuit doughs, J. Sci. Food Agric. 35 (1984), 99–105. [7] M. Stone et R. J. Brooks, Continuum regression : cross-validated sequentially constructed prediction embracing ordinary least squares, partial

