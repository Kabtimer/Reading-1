Lesson 5: MDP
Lesson 5: Markov Decision Processes
Emmanuel Hyon1
1Universit´e Paris Nanterre, France LIP6, Sorbonne Universit´e, CNRS, France
RESCOM Summer School, June 2019
1/68

Lesson 5: MDP
Outline
1 Introduction 2 Markov Processes 3 Elements of Markov Decision Processes 4 Resolution of Markov Decision Processes 5 Related models (extension of MDP) 6 Bibliography
2/68

Lesson 5: MDP Introduction
Outline
1 Introduction Stochastic Systems Controlled stochastic systems
3/68

Lesson 5: MDP Introduction Stochastic Systems
Introductory Example
You are on vacation and every morning you observe a groundhog and you record its position.
4/68

Lesson 5: MDP Introduction Stochastic Systems
Introductory Example
You are on vacation and every morning you observe a groundhog and you record its position.
The groundhog moves during the night.
4/68

Lesson 5: MDP Introduction Stochastic Systems
Introductory Example
You are on vacation and every morning you observe a groundhog and you record its position.
The mountain on which is the groundhog changes.
4/68

Lesson 5: MDP Introduction Stochastic Systems
Introductory Example
You are on vacation and every morning you observe a groundhog and you record its position.
The behaviour of the groundhog seems you random.
4/68

Lesson 5: MDP Introduction Stochastic Systems
Introductory Example (ctd.) Dynamical system
We introduce xk which is the record of the position at day k.
The sequence of the daily records is called a path or an episode. It can take several values :
- (M1, M2, M3, M3, M2, M3, M2, M1, M1,...) - (M1, M2, M2, M1, M1, M2, M2, M2, M2,...) - (H, H, H, H, H, H, H, H, H, H, H,......) - (H, M3, M2, M3, H, H, H, M3, M2, M1,...)
The system you observe is a dynamical system: the “state” of the system evolves during time, the state of the next day depends on a transition function:
xk+1 = fk (xk , xk−1, . . . , x1) .
5/68

Lesson 5: MDP Introduction Stochastic Systems
Introductory Example (ctd.) Stochastic dynamical system
When the transition from a state at day k to another state at day k + 1 follows a stochastic transition function it is a stochastic system :
xk+1 = fk (xk , xk−1, . . . , x1, ε(ω)) with ε(ω) is the hazard.
-The transition can depend on the states : e.g. if k is a day corresponding of a winter day there few chances that the state will be diﬀerent of H (home). -The transition can depend on event that are not considered in the states: e.g. the hazard is the presence of an eagle in the sky.
6/68

Lesson 5: MDP Introduction Controlled stochastic systems
A second example
You have now the ability to place on a mountain an eagle or herbs.
7/68

Lesson 5: MDP Introduction Controlled stochastic systems
A second example
You have now the ability to place on a mountain an eagle or herbs.
You change the behaviour of the groundhog. You control the behaviour of the system.
7/68

Lesson 5: MDP Introduction Controlled stochastic systems
A second example
You have now the ability to place on a mountain an eagle or herbs.
You have a controlled stochastic system.
7/68

Lesson 5: MDP Introduction Controlled stochastic systems
A second example
You have now the ability to place on a mountain an eagle or herbs.
The (stochastic) transition function depends on states and actions: xk+1 = fk (xk , xk−1, . . . , x1, ak , . . . , ak−1, ε(ω)) .
7/68

Lesson 5: MDP Introduction Controlled stochastic systems
Controlled stochastic system (ctd)
Two axis for studies of controlled stochastic system Keeping some features of the system during its life, e.g Let the groundhog alive. =⇒ mainly in automatic ﬁeld. Coupled with the evolution of the system it exists rewards (or costs), e.g. you receive a reward every time the groundhog is on mountain M1. =⇒ mainly in Stochastic optimisation ﬁeld.
We are interested here by the stochastic optimisation aspects.
8/68

Lesson 5: MDP Markov Processes
Outline
2 Markov Processes Markov Chain Markov Reward Processes Markov Decision Processes: an example
9/68

Lesson 5: MDP Markov Processes Markov Chain
Example 1 and Markov properties
Consider again the ﬁrst example :
a) We assume that we know the transition probabilities (i.e. the stochastic transition function is known).
10/68

Lesson 5: MDP Markov Processes Markov Chain
Example 1 and Markov properties
Consider again the ﬁrst example :
b) We assume that the transition probabilities only depend on the current state.
10/68

Lesson 5: MDP Markov Processes Markov Chain
Example 1 and Markov properties
Consider again the ﬁrst example :
c) We also assume that we know the initial place.
10/68

Lesson 5: MDP Markov Processes Markov Chain
Markov Chain
We describe the behaviour of the groundhog by a Markov Chain that is a memoryless random process with Markov property.
Let xk be the position of the groundhog at day k. The stochastic transition function depends only on the hazard and on the current state :
xk+1 = f (xk , εk+1(ω))
Deﬁnition A (ﬁnite) Markov Chain is a tuple X , P :
X is the state space : a (ﬁnite) set of states. P is the state to state transition probability (given by a transition matrix): Px,x = P [xk+1 = x | xk = x ].
11/68

Lesson 5: MDP Markov Processes Markov Chain
Example 1 (ctd.)
Assume transition probabilities on the Example 1 such that: If the groundhog stays on the ﬁrst mountain, then it can remain on the ﬁrst mountain with probability 0.25; leave to go on the second mountain with probability 0.5; leave to go on the third mountain with probability 0.25. If the groundhog stays on the second mountain, then it can remain on the second mountain with probability 0.2; leave to go on the ﬁrst mountain with probability 0.4; leave to go on the third mountain with probability 0.4. If the groundhog stays on the third mountain, then it can remain on the third mountain with probability 0.3; leave to go on the ﬁrst mountain with probability 0.4 leave to go on the second mountain with probability 0.3.
12/68

Lesson 5: MDP Markov Processes Markov Chain
Example 1 (ctd.): Model

The state space is {M1, M2, M3}, where Mi is the mountain i.

The transition matrix is

The transition graph is

0.25 0.5 0.25 P =  0.4 0.2 0.4 
0.4 0.3 0.3

M1

0.25

0.5 0.4

M2

0.2 0.25 0.4

0.4 0.4

M3

0.3

13/68

Lesson 5: MDP Markov Processes Markov Reward Processes
Example 1 with reward
Consider again the ﬁrst example :
In state M1 we are very happy we receive a satisfaction of 10,
14/68

Lesson 5: MDP Markov Processes Markov Reward Processes
Example 1 with reward
Consider again the ﬁrst example :
In state M2 we are happy we receive a satisfaction of 1,
14/68

Lesson 5: MDP Markov Processes Markov Reward Processes
Example 1 with reward
Consider again the ﬁrst example :
In state M3 we are unhappy we receive a satisfaction of 0.1,
14/68

Lesson 5: MDP Markov Processes Markov Reward Processes
Example 1 with reward
Consider again the ﬁrst example :
Satisfaction is the reward of a state.
14/68

Lesson 5: MDP Markov Processes Markov Reward Processes
Markov Reward Process (MRP)
Let rk be the reward collected at day k. The reward is given by a function rk = E (R(xk+1, xk , εk+1))
Deﬁnition A Markov Reward process is a tuple X , P, R :
X is the state space : a set of states. P is the state to state transition probability: Px,x = P [xk+1 = x | xk = x ]. Rk is a reward function Rk (xk+1, xk , ε).
15/68

Lesson 5: MDP Markov Processes Markov Reward Processes
Collected rewards
We are interested by the return of a MRP: the sum of the rewards collected along a path : Deﬁne Vk (x) the sum of collected rewards from state x at day k :
i =N
Vk (x) = ri .
i =k
16/68

Lesson 5: MDP Markov Processes Markov Reward Processes
Collected rewards

We are interested by the return of a MRP: the sum of the rewards collected along a path :

Deﬁne Vk (x) the sum of collected rewards from state x at day k :

i =N
Vk (x) = ri .
i =k

Path: (M1, M2, M3, M3, M3, M3) (M1, M2, M3, M2, M1, M2) (M1, M2, M1, M1, M2, M1)

Return: (V1(M1) with N = 6) 10 + 1 + 0.1 + 0.1 + 0.1 + 0.1 = 11.4 10 + 1 + 0.1 + 1 + 10 + 1 = 23.1 10 + 1 + 10 + 10 + 1 + 10 = 42

16/68

Lesson 5: MDP Markov Processes Markov Reward Processes
Expected collected rewards
It is more interesting to use the expectation of the sum of the collected rewards. This takes better into account the stochastic feature of the system (a path occurs with a given probability). We evaluate the expected cost but we experience a trajectory cost.
17/68

Lesson 5: MDP Markov Processes Markov Reward Processes
Expected collected rewards

It is more interesting to use the expectation of the sum of the collected rewards.
This takes better into account the stochastic feature of the system (a path occurs with a given probability).
We evaluate the expected cost but we experience a trajectory cost.

Path: (M1, M2, M3, M3, M3, M3) (M1, M2, M3, M2, M1, M2) (M1, M2, M1, M1, M2, M1) Expected reward : 0.00996

Return 11.4 with probability 0.0054 0.5 × 0.4 × 0.3 × 0.3 × 0.3 = 0.0054 23.1 with probability 0.012 0.5 × 0.4 × 0.3 × 0.4 × 0.5 = 0.012 42 with probability 0.01 0.5 × 0.4 × 0.3 × 0.3 × 0.3 = 0.01
17/68

Lesson 5: MDP Markov Processes Markov Reward Processes
Objectives

But actually it exists more realistic objectives:

Discounted reward

Let θ ∈ [0, 1) be the discount factor. The ﬁnite discounted reward

is:

k =N

VNθ (x) = E

θk rk |x0 = x

k =0

Economic relevance: interest rate can be taken into account. Practical relevance: future rewards have diﬀerent weights according to θ. Mathematical relevance: convergence of the sum.

18/68

Lesson 5: MDP Markov Processes Markov Reward Processes
Objectives

But actually it exists more realistic objectives:

Average reward In an inﬁnite horizon problem, the average reward is deﬁned by:

1 k=N

ρ(x )

=

lim
N →∞

N

E

rk |x0 = x

k =0

Practical relevance: same cost in average for each day. Mathematical relevance: convergence.

18/68

Lesson 5: MDP Markov Processes Markov Decision Processes: an example
Example 1 with rewards and controls
Consider again the ﬁrst example :
You can put food on one mountain each day.
19/68

Lesson 5: MDP Markov Processes Markov Decision Processes: an example
Example 1 with rewards and controls
Consider again the ﬁrst example :
The food modiﬁes the behaviour of the groundhog.
19/68

Lesson 5: MDP Markov Processes Markov Decision Processes: an example
Example 1 with rewards and controls
Consider again the ﬁrst example :
But putting food on a mountain has a cost: M1 costs 5, M2 costs 1 and M3 costs 0.5.
19/68

Lesson 5: MDP Markov Processes Markov Decision Processes: an example
Example 1 with with rewards and controls :
a dynamic control problem
You have a set of possible actions in each state. When you perform an action the stochastic transition function is modiﬁed (also the evolution of the system). When you perform an action the reward is modiﬁed.
We denote by rk (xk , ak ) the reward at day k when action ak is triggered in state xk .
The transition function depends on state, action and hazard:
xk+1 = f (xk , ak , εk+1) .
The transition probability is now expressed with respect to the state and action:
P (xk+1 = y |xk = x, ak = a) = p(y |x, a)
20/68

Lesson 5: MDP Markov Processes Markov Decision Processes: an example
Markov Decision Process
Deﬁnition A Markov Decision Process is a tuple X , A, (P)a, R :
X is the state space : a set of states. A is the action space : a set of actions. (P)a is a collection of transition probabilities : (Pa)x,x = P [xk+1 = x | xk = x , ak = a]. Rk is a reward function Rk (xk+1, xk , ak , ε)
The Markov Decision Process (MDP) is a tool to solve : Dynamic control problems aka Optimal Control problems aka Stochastic multistage problems.
21/68

Lesson 5: MDP Markov Processes Markov Decision Processes: an example
Example 1: Temporal behaviour
It is important to express with precision the temporal decomposition of events and their eﬀects in a time slot.
It is important to detail carefully when rewards are received and when action is triggered.
Temporal decomposition of a day (method 1) 1 Breakfast 2 Decide where to place the food 3 Put the food on the chosen mountain 4 Go to sleep (cost due to the action) 5 Wake up 6 Observe the mountain and receive satisfaction (cost due to nature)
22/68

Lesson 5: MDP Markov Processes Markov Decision Processes: an example
Example 1: Temporal behaviour
It is important to express with precision the temporal decomposition of events and their eﬀects in a time slot.
Temporal decomposition of a day (method 2) 1 Wake up 2 Observe the mountain and receive satisfaction (cost due to nature) 3 Breakfast 4 Decide where to place the food 5 Put the food on the chosen mountain 6 Go to sleep (cost due to the action)
If we choose to let begin the slot at step 5 (wake), then satisfaction is not (directly) related with the action taken during the day.
22/68

Lesson 5: MDP Markov Processes Markov Decision Processes: an example
Example 1 expressed with MDP (ctd.)

State space : {1, 2, 3}, where i means mountain Mi. Action space : {0, 1, 2, 3}, where i means put the food on mountain Mi (and 0 means that nothing is done) Transition probabilities:

0.25 0.5 0.25

0.5 0.25 0.25

P0 =  0.4 0.2 0.4  , P1 = 0.6 0.2 0.2  ;

0.4 0.3 0.3

0.5 0.4 0.1

0.25 0.55 0.2

0.1 0.2 0.7

P2 =  0.2 0.4 0.4 , P3 = 0.0 0.1 0.9

0.3 0.4 0.3

001

23/68

Lesson 5: MDP Markov Processes Markov Decision Processes: an example
Example 1 expressed with MDP (rewards)
Express the rewards now for state 1:
r (1|1, 0) = 10 , r (2|1, 0) = 1 , r (3|1, 0) = 0.1 r (1|1, 1) = 5 , r (2|1, 1) = −4 , r (3|1, 1) = −4.9 r (1|1, 2) = 9 , r (2|1, 2) = 0 , r (3|1, 2) = −0.9 r (1|1, 3) = 9.5 , r (2|1, 3) = 0.5 , r (3|1, 3) = −0.4
The expected reward r (x, a) is
r (x, a) = R(y |x, a) · p(y |x, a)
y ∈X
and then we have :
r (1, 0) = 0.25 · r (1|1, 0) + 0.5 · r (2|1, 0) + 0.25 · r (3|1, 0) = 3.025 r (1, 1) = 0.25 · r (1|1, 1) + 0.5 · r (2|1, 1) + 0.25 · r (3|1, 1) = −1.975 r (1, 2) = 0.25 · r (1|1, 2) + 0.55 · r (2|1, 2) + 0.2 · r (3|1, 2) = 2.07 r (1, 3) = 0.1 · r (1|1, 3) + 0.2 · r (2|1, 3) + 0.7 · r (3|1, 3) = 0.77
24/68

Lesson 5: MDP Markov Processes Markov Decision Processes: an example
Example 1 expressed with MDP (rewards ctd.)
We can count the rewards for the other states in the same way, we get
r (2, 0) = 10 · 0.4 + 1 · 0.2 + 0.1 · 0.4 = 4.24 r (2, 1) = (10 − 5) · 0.6 + (1 − 5) · +(0.1 − 5) · 0.2 = 1.22 r (2, 2) = 9 · 0.2 − 0.9 · 0.4· = 1.45 r (2, 3) = 0.5 · 0.1 − 0.4 · 0.9 = −0.31
r (3, 0) = 4.33 r (3, 1) = 0.41 r (3, 2) = 2.43 r (3, 3) = −0.4
25/68

Lesson 5: MDP Markov Processes Markov Decision Processes: an example
Solving dynamic control problem

We want to optimise the discounted cost, we have

k =7

Vθ = E

θk r (xk, ak)

(1)

k =0

How optimise our problem ?

26/68

Lesson 5: MDP Markov Processes Markov Decision Processes: an example
Solving dynamic control problem

We want to optimise the discounted cost, we have

k =7

Vθ = E

θk r (xk, ak)

(1)

k =0

How optimise our problem ?

What can I do ?

26/68

Lesson 5: MDP Markov Processes Markov Decision Processes: an example
Solving dynamic control problem

We want to optimise the discounted cost, we have

k =7

Vθ = E

θk r (xk, ak)

(1)

k =0

How optimise our problem ?

What can I do ?

⇒ Choose the action to perform each day.

26/68

Lesson 5: MDP Markov Processes Markov Decision Processes: an example
Solving dynamic control problem

We want to optimise the discounted cost, we have

k =7

Vθ = E

θk r (xk, ak)

k =0

How optimise our problem ?

What can I do ?

⇒ Choose the action to perform each day.

What should I do ?

(1)
26/68

Lesson 5: MDP Markov Processes Markov Decision Processes: an example
Solving dynamic control problem

We want to optimise the discounted cost, we have

k =7

Vθ = E

θk r (xk, ak)

(1)

k =0

How optimise our problem ?

What can I do ?

⇒ Choose the action to perform each day.

What should I do ?

⇒ Determine the most proﬁtable action (for the objective)

26/68

Lesson 5: MDP Markov Processes Markov Decision Processes: an example
Decision tree
The set of possible paths including decisions and their possible consequences can be represented by a decision tree. A branch in the tree is a path in the system The costs of a trajectory in the system are the costs of a tree path.
An illustration We assume an example with 5 states (s1, s2, s3, s4, s5) and three actions (a1, a2, a3).
It can be represented by a decision tree that depends on the transition probabilities and whose edges are valued by the rewards.
27/68

Lesson 5: MDP Markov Processes Markov Decision Processes: an example
Decision tree associated with the illustration

a1

s2

s2

a1

a3

s3

a3

s3

a1

s1

a2

s1

a2

s1

a3

a2

s4

s4

a3

a3

a1

s5

s5

a3

28/68

Lesson 5: MDP Markov Processes Markov Decision Processes: an example
Computation of optimal action
Let us detail now (with the help of the previous decision tree) some diﬃculties with the determination of optimal actions:
1 An action has a long term eﬀect. Ex. 1 Once you are in state s3 you remain in it. Ex. 2 Once you played a1 in state s1 then you have no possibility to reach states (s1, s4, s5).
2 The beneﬁt of an action depends on the following actions and their rewards.
Ex. 3 Assume that r (s1, a1) = 3, r (s1, a2) = 2, r (s1, a3) = 1 and r (s2, a2) = r (s2, a3) = r (s3, a3) = 0. In s1 the action a1 will give a good immediate reward but no rewards in the future.
Ex. 4 Assume moreover that r (s4, ·) = 3.5 and r (s5, ·) = 4 Action a3 in s1 is better than action a2 since the rewards in s4 or s5 are larger.
29/68

Lesson 5: MDP Markov Processes Markov Decision Processes: an example
Computation of optimal action (ctd.)
So the question is how evaluate branches ? and how evaluate sub branches ?
Monte Carlo Simulations ? But how do we choose the action ?
Random choice ? The action that gives the best immediate reward ? We explore a part of the branch ? The Markov Decision Process can represent a dynamic control problem. is a tool to solve the computation of optimal action using dynamic programming.
30/68

Lesson 5: MDP Elements of Markov Decision Processes
Outline
3 Elements of Markov Decision Processes Model Objective function Decision Rule and Policy Value function
31/68

Lesson 5: MDP Elements of Markov Decision Processes Model
Mathematical model of MDP
Deﬁnition A Markov Decision Process is a tuple X , A, (P)a, R :
X is the state space : a set of states. A is the action space : a set of actions. (Pk )a is a collection of transition probabilities : (Pk )a(x, x ) = Pk [xk+1 = x | xk = x, ak = a]. Rk is a reward function r (xk , ak )
When transition probabilities (Pk )a are the same for any k and when the rewards are the same for any k then the MDP is stationary (or homogeneous). We only study here stationary MDP.
32/68

Lesson 5: MDP Elements of Markov Decision Processes Model
State Space
The state space is the (countable) set of states it represents the set of values in which our system evolves. It can be
the physical state of the system. e.g. the number of customers in a queue. e.g. the number of objects that are connected to me. the physical state of the system and an additional information. e.g. the mountain where the groundhog is and the presence of an eagle in the sky. e.g. the number of customers in a queue and an information about the transmission rate. Powell [1] [2] proposes a “deﬁnition” : State represents the “physical” state and all additional information required for the decision.
33/68

Lesson 5: MDP Elements of Markov Decision Processes Model
Action Space
The Action Space represents the set of actions available at the controller in a given state.
However this set can be diﬀerent depending on the state. Hence you can have two diﬀerent sets Ax and Ax for diﬀerent states x and x .
Usually you represent the action space by a set A that contains all possible actions (A = ∪x Ax ) considering that there are impossible actions in given states.
To model impossible action, two possibilities 1 You consider only the actions of Ax in x. 2 You consider all the actions in each state but you penalise impossible actions
34/68

Lesson 5: MDP Elements of Markov Decision Processes Model
Transition function
Your transition function “has” the Markov properties.

All the past is resumed in the present state.

Thus

xk+1 = f (xk , ak , εk+1) .

P (xk+1 = y | xk = x , ak = a, xk−1, ak−1, . . . , x0, a0) = P (xk+1 = y | xk = x, ak = a)
also P (xk+1 = y | xk = x, ak = a) = p(y |x, a) .
35/68

Lesson 5: MDP Elements of Markov Decision Processes Model
Rewards
We consider only rewards which depend on the state and the action : r (x, a). Reward can be bounded or unbounded.
36/68

Lesson 5: MDP Elements of Markov Decision Processes Model
Computer Representation

Matrices
How : Storing a set of transition probabilities matrices in an array.
When: a complete description of the system is storable and available. Advantages: Eﬃcient especially coupled with sparse matrices Disadvantages Large MDP can not be represented.

Hash tables

List and function

How : Storing transition probabilities in an hash table. Key is the tuple (xk , xk+1, ak ).

How : Probability transitions are described by a function and not all the states are studied.

When: Simple system in When: A compact form

which a complete

is desired.

description is available.

Advantages: Fast to program. Eﬃcient.

Advantages Adapted for very large systems.

Disadvantages Large and complex MDP can not be represented.

Disadvantages Require
many computations and
recalculations.
37/68

Lesson 5: MDP Elements of Markov Decision Processes Objective function
Objectives

The diﬀerent objectives that can be solved are :

Finite Horizon The gain on a ﬁnite horizon N is the expected sum of the reward on a path of length N:

N −1

VN = E

r (xi , ai )

i =0

where r (xi , ai ) is the reward at step i.

38/68

Lesson 5: MDP Elements of Markov Decision Processes Objective function
Objectives II

The diﬀerent objectives that can be solved are :

discounted ﬁnite Horizon The gain on a discounted ﬁnite horizon N is the expected sum of the weighted rewards on a path of length N:

N −1

VNθ = E

θi r (xi , ai )

i =0

where r (xi , ai ) is the reward at step i, where θ ∈ [0, 1) is the discount factor.

39/68

Lesson 5: MDP Elements of Markov Decision Processes Objective function
Objectives III

The diﬀerent objectives that can be solved are :

discounted inﬁnite Horizon The gain on a discounted inﬁnite horizon is the expected sum of the rewards on an inﬁnite path :

∞

Vθ = E

θi r (xi , ai )

i =0

where r (xi , ai ) is the reward at step i, where θ ∈ [0, 1) is the discount factor.

40/68

Lesson 5: MDP Elements of Markov Decision Processes Objective function
Objectives IV

The diﬀerent objectives that can be solved are :

total reward

The gain on a total reward is the expected sum of the rewards on

an inﬁnite path :
∞

V =E

r (xi , ai )

i =0

where r (xi , ai ) is the reward at step i.

The sum of the total rewards may not converge. This kind of models is often applied with a set of absorbing states.

41/68

Lesson 5: MDP Elements of Markov Decision Processes Objective function
Objectives V

The diﬀerent objectives that can be solved are :
average The gain on an average cost model is the limit of the Cesaro mean of the expected sum of the rewards on a path :

1

N −1

ρ

=

lim
N →∞

N

E

r (xi , ai )

i =0

where r (xi , ai ) is the reward at step i.

This kind of objective has a gain which depends on the intrinsic Markov Chain.
42/68

Lesson 5: MDP Elements of Markov Decision Processes Decision Rule and Policy
Decision rule

Deﬁnition We deﬁne a decision rule as the mapping that allows to select an action to perform.

Its classiﬁcation depends on the information set that is used.

We denote by πk the decision rule that is applied at step k. A decision πk can be :
History dependent Depends on

the complete past (states, actions)

State dependent Depends on

(or Markov)

the current state

Thus a Decision rule is a mapping π from the history to an action π : H → A from the state to an action π : S → A
43/68

Lesson 5: MDP Elements of Markov Decision Processes Decision Rule and Policy
Decision rule (ctd.)

Another classiﬁcation is made according to the manner the decision is returned

Deterministic The rule deﬁnes a single action

Random

The rule deﬁnes a probability on an action.

Thus a decision rule can deﬁne a single action π : H → a a probability with which the action is chosen π : H → P(a)

44/68

Lesson 5: MDP Elements of Markov Decision Processes Decision Rule and Policy
Decision rule (ctd.)
Thus we have four classes of decision rules. 1 Random History dependent, 2 Deterministic History dependent, 3 Random Markov, 4 Deterministic Markov,
Random history dependent is the more general.
Deterministic Markov is the more speciﬁc and is included in Random History dependent.
45/68

Lesson 5: MDP Elements of Markov Decision Processes Decision Rule and Policy
Policy
Usually the terms policy and decision rule are considered as synonym. However, I prefer make a distinction between them for the sake of clarity
Deﬁnition A Policy Π is a sequence of decision rules
Π = (π1, π2, . . . , πn, . . .) .
Deﬁnition A decision rule can be stationary : the same rule is applied in each slot
Π = (π, π, . . . , π) .
46/68

Lesson 5: MDP Elements of Markov Decision Processes Value function
Value function
How to evaluate the reward of a policy ?
For this we deﬁne value function. Deﬁnition A value function is deﬁned for a ﬁxed policy and a ﬁxed initial state x ∈ X and returns the total gain (for the criteria considered) collected following the policy Π from the initial state.
For a ﬁxed Π we have a function (or a vector) such that V Π : X → R.
Back to the groundhog The value function have a value for each of the state M1, M2, M3.
47/68

Lesson 5: MDP Elements of Markov Decision Processes Value function
Value function (ctd.)

There exists a diﬀerent value function expression for each of the objective.

Value Function for ﬁnite horizon problem
Let N be the horizon. The ﬁnite horizon value function is denoted by VNΠ(x) with

N −1

VNΠ(x ) = EΠ

r xkΠ, πk (xkΠ) |x0 = x .

k =0

Value Function for discounted cost The discounted cost value function is VθΠ such that ∀x ∈ X :

∞

VθΠ(x ) = EΠ

θk r xkΠ, π(xkΠ) |x0 = x .

k =0

48/68

Lesson 5: MDP Elements of Markov Decision Processes Value function
Back to example 1
I will deﬁne a (Markov deterministic stationary) policy such that the decision rule is
π(M1) = 0 π(M2) = 1 π(M3) = 2
I can compute the value function for this policy (the MDP is now a Markov Reward Process) either formally or by Monte Carlo.
But to ﬁnd the best policy, I have to compare diﬀerent reward processes !!!
49/68

Lesson 5: MDP Elements of Markov Decision Processes Value function
Decision tree associated with a decision rule

Take the model of the illustration before. We assume that π(s1) = a3, π(s2) = a1, π(s3) = a3, π(s4) = a2, π(s5) = a1.

a1

s2

s2

a1

a3

s3

a3

s3

a1

s1

a2

s1

a2

s1

a3

a2

s4

s4

a3

a3

a1

s5

s5

a3

50/68

Lesson 5: MDP Elements of Markov Decision Processes Value function
Main goal of MDP
Partial Order on value function We denote by V the space of functions from X → R (related with the vector space R|X |) The space V has a partial order :
∀ U, V ∈ V U ≤ V ⇔ U(x) ≤ V (x) , ∀x ∈ X .
Goal of a MDP The goal of a Markov Decision Process is to characterise, as well as to search and compute (if they exist): the optimal policy Π∗ ∈ ΠHR ( ΠHR is the set of history dependent random policies) such that
∀ Π ∈ ΠHR , ∀x ∈ X : V Π(x) ≤ V Π∗(x) ,
(equivalently said Π∗ ∈ arg maxΠ∈ΠHR V Π) and the optimal value V Π∗ .
51/68

Lesson 5: MDP Resolution of Markov Decision Processes
Outline
4 Resolution of Markov Decision Processes Bellman Equation Resolution Algorithms
52/68

Lesson 5: MDP Resolution of Markov Decision Processes
Equivalence of policy
If we should consider all the history dependent random policies, then the computation will suﬀer from curse of dimensional.
Fortunately, they are results that claim we can search in the Markov Deterministic policy set.
Theorem Let Π ∈ ΠHR be an history dependent random policy. Then for each initial state x it exists a Markov Random policy Π such that
VNΠ(x) = VNΠ (x) Finite horizon VθΠ(x) = VθΠ (x) Discounted cost V Π(x) = V Π (x) Total reward ρΠ(x) = ρΠ (x) average reward For each objective, the two value functions have the same value.
53/68

Lesson 5: MDP Resolution of Markov Decision Processes
Equivalence of policy
If we should consider all the history dependent random policies, then the computation will suﬀer from curse of dimensional. Fortunately, they are results that claim we can search in the Markov Deterministic policy set. Then the passing of the equivalence from Markov Random policy to Markov Deterministic policy is a case by case study (for each objective). Proofs and validity conditions can be found in [1].
53/68

Lesson 5: MDP Resolution of Markov Decision Processes Bellman Equation
Dynamic Programming and framework of proof
Bellman formulation of DP principle If the shortest path from town A to town B goes through town C then the shortest path from C to B is the portion between C and B of the shortest path between A and B.
54/68

Lesson 5: MDP Resolution of Markov Decision Processes Bellman Equation
Dynamic Programming and framework of proof
Bellman formulation of DP principle If the shortest path from town A to town B goes through town C then the shortest path from C to B is the portion between C and B of the shortest path between A and B.
Then to obtain the shortest path from A to B: 1 you consider all the successors of A 2 for each successor (say y ) you compute the length of the path from A to B by adding: -the length from A to y -the shortest path from y to B. 3 you take the successor such that the path from A to B is the smallest.
54/68

Lesson 5: MDP Resolution of Markov Decision Processes Bellman Equation
Dynamic Programming and framework of proof
Bellman formulation of DP principle If the shortest path from town A to town B goes through town C then the shortest path from C to B is the portion between C and B of the shortest path between A and B.
Framework of proof 1 Express the Bellman Equation. This means decompose your cost in two parts: the immediate cost due to action and the future cost due to the eﬀects of your action. 2 Prove its validity: This means to show that using the Bellman Equation will lead to the optimal policy. 3 Solve the equation numerically: This means apply well known algorithms.
54/68

Lesson 5: MDP Resolution of Markov Decision Processes Bellman Equation
Bellman Equation

Let us compute the Bellman equation for the ﬁnite horizon criterion.

N −1

VN∗ (x

)

=

max
a,...,a∈AN

E

r (xk , ak )|x0 = x ,

k =0

N −1

= max E r (x, a)| x0 = x + E r (xk , ak ) x0 = x ,
a,...,a∈AN k =1

= max r (x, a)
a,...,a∈AN

N −1

+E

r (xk , ak )1{x1=y ,a0=a} x0 = x ,

y ∈X k=1

55/68

Lesson 5: MDP Resolution of Markov Decision Processes Bellman Equation
Bellman Equation

Let us compute the Bellman equation for the ﬁnite horizon criterion.

VN∗ (x

)

=

max
a,...,a∈AN

r (x, a)

N −1
+ E r (xk , ak ) x1 = y , a0 = 0, x0 = x ×
y ∈X k=1

P x1 = y x0 = x, a0 = a

N −1

= max r (x, a) + p(y |x, a)E r (xk , ak ) x1 = y

a,...,a∈AN

y ∈X

k =1

55/68

Lesson 5: MDP Resolution of Markov Decision Processes Bellman Equation
Bellman Equation

Let us compute the Bellman equation for the ﬁnite horizon criterion.

VN∗ (x )

=

max
a∈A

r (x, a) +

p(y |x, a)×

y ∈X

N −1

max E r (xk , ak ) x1 = y
a,...,a∈AN−1 k =1

= max r (x, a) +
a∈A

p(y |x, a)VN∗−1(y ) .

y ∈X

55/68

Lesson 5: MDP Resolution of Markov Decision Processes Bellman Equation
Bellman Equation (ctd.)

The Bellman equation in ﬁnite horizon case is :

VN∗ (x )

=

max
a∈A

r (x, a) +

VN∗−1(y )p(y |x, a) .

(2)

y ∈X

Sketch of proof: To prove its validity we have several things to show.
1 Show that the value function VN that results from Eq. (2) is the optimal (i.e. VN∗ ).
2 Show that the value function VN−1 used is the optimal (i.e. VN∗−1)

56/68

Lesson 5: MDP Resolution of Markov Decision Processes Bellman Equation
Bellman Equation (proof of Bellman function)
Proof of Bellman Function in other ﬁelds The checking of the validity of the Bellman Equation is not restricted to MDP ! e.g. a graph course that presents the Bellman-Ford algorithm will check the conditions of convergence.
Many works have already done the job In a large number of MDP models the validity are proved and the assumptions that should be veriﬁed are detailed. The Puterman’s book presents some of them with detailed proofs. When you work with MDP you have to use the assumptions related to your model. e.g. bounded rewards or unbounded rewards models require diﬀerent sets of assumptions
57/68

Lesson 5: MDP Resolution of Markov Decision Processes Resolution Algorithms
Resolution algorithm
It exists three main ways to compute both optimal value function and optimal policy (both are computed together).
1 Value Iteration (VI) 2 Policy Iteration (PI) 3 Linear Programming Note that the computation of the decision rule is computed oﬀ-line and then applied by the controller.
You can compute the decision rule on-line but you are in the ﬁeld of on-line MDP.
58/68

Lesson 5: MDP Resolution of Markov Decision Processes Resolution Algorithms
Value Iteration for the ﬁnite horizon

Backward Algorithm: resolution of ﬁnite horizon criterion

Initialise V0 = rN for k = N − 1 to 0 do

for x ∈ X do

Vk∗ (s )

=

max
a∈A

rk (x, a) +

Pk (y |x, a)Vk∗+1(y )

y ∈X

πk (x) ∈ arg max rk (x, a) +
a∈A

Pk (y |x, a)Vk∗+1(y )

y ∈X

end for

end for return the two sequences Vk∗ and πk

59/68

Lesson 5: MDP Resolution of Markov Decision Processes Resolution Algorithms
Value Iteration for discounted inﬁnite horizon
value iteration algorithm: θ-discounted cost criterion Initialise V0 k ←0 repeat
for x ∈ X do Vk+1(x) = max r (x, a) + θ P(y |x, a)Vk (y )
a∈A y ∈S
k ←k+1 until Vk+1 − Vk ≤ for x ∈ X do π(x) ∈ arg max{r (x, a) + θ p(y |x, a)Vk+1(y )}
a∈A y ∈X
return Vk+1, π
60/68

Lesson 5: MDP Resolution of Markov Decision Processes Resolution Algorithms
Policy Iteration for inﬁnite horizon discounted cost
Policy iteration algorithm: θ-discounted cost criterion
Initialise π0 ∈ ΠMD k ←0 repeat
Computation phase): solve Vk (x) = r (x, πk (x)) + θ p(y |(x, πk ))Vk (y ), ∀x ∈ X
y ∈X
improvement phase): improve for x ∈ X do πk+1(x) ∈ arg max{r (x, a) + θ p(y |(s, a))Vk (y )}
a∈A y ∈X
k ←k+1 until πk = πk+1 return Vk , πk+1
61/68

Lesson 5: MDP Related models (extension of MDP)
Outline
5 Related models (extension of MDP) Approximated MDP Reinforcement Learning Models Stochastic Games
62/68

Lesson 5: MDP Related models (extension of MDP) Approximated MDP
Approximated MDP
Consider again the ﬁrst example : with some modiﬁcations
you observe a large number of positions on each of the mountains, the cardinal of your state space increases exponentially, thus the MDP can not be stored in memory.
You can not solve completely the MDP and you should approximate it.
You use approximate dynamic programming or factored MDP but only approximated values or partial exploration are obtained.
63/68

Lesson 5: MDP Related models (extension of MDP) Reinforcement Learning Models
Reinforcement Learning
Consider again the ﬁrst example :
Assumption: You can put food on one mountain each day.
64/68

Lesson 5: MDP Related models (extension of MDP) Reinforcement Learning Models
Reinforcement Learning
Consider again the ﬁrst example :
Assumption: You know neither transition probabilities nor rewards.
64/68

Lesson 5: MDP Related models (extension of MDP) Reinforcement Learning Models
Reinforcement Learning
Consider again the ﬁrst example :
You are in a Reinforcement learning case
64/68

Lesson 5: MDP Related models (extension of MDP) Reinforcement Learning Models
Reinforcement Learning
Consider again the ﬁrst example :
You will learn probabilities and rewards (and “value function”) to determine the best action.
64/68

Lesson 5: MDP Related models (extension of MDP) Stochastic Games
Stochastic games
Consider again the ﬁrst example :
You have a neighbour who can also put food.
65/68

Lesson 5: MDP Related models (extension of MDP) Stochastic Games
Stochastic games
Consider again the ﬁrst example :
But he has not the same rewards than you.
65/68

Lesson 5: MDP Related models (extension of MDP) Stochastic Games
Stochastic games
Consider again the ﬁrst example :
Since there are two people who act, then probabilities and rewards depend on two actions: p(y |x, a1, a2) and r1(x, a1, a2) and r2(x, a1, a2).
65/68

Lesson 5: MDP Related models (extension of MDP) Stochastic Games
Stochastic games
Consider again the ﬁrst example :
The model is now a stochastic game.
65/68

Lesson 5: MDP Related models (extension of MDP) Stochastic Games
Stochastic games
Consider again the ﬁrst example :
Same basis (due to Shapley) but computation of maxima is replaced by Nash Equilibrium equation.
65/68

Lesson 5: MDP Bibliography
Outline
6 Bibliography
66/68

Lesson 5: MDP Bibliography
Bibliography
References that present the main concepts of MDP F. Garcia. Processus D´ecisionnels de Markov Chapter 1 in Processus D´ecisionnels de Markov en Intelligence Artiﬁcielle Hermes, 2008. W.B. Powell Introduction to Markov Decision Processes Chapter 3 in Approximate Dynamic Programming, Solving the Curses of Dimensionality Wiley, 2011.
67/68

Lesson 5: MDP Bibliography
Bibliography
Complete book (with detailed proofs) M. Puterman. Markov Decision Processes Discrete Stochastic Dynamic Programming. Wiley, 2005.
Article to locate MDP in the stochastic optimization ﬁeld. W. B. Powell. Clearing the Jungle of Stochastic Optimization Informs TutORials, 2014.
68/68

