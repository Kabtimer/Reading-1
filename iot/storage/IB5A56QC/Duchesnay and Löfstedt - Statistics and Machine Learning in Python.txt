Statistics and Machine Learning in Python
Release 0.2
Edouard Duchesnay, Tommy L√∂fstedt
May 16, 2019

CONTENTS

1 Introduction

1

1.1 Python ecosystem for data-science . . . . . . . . . . . . . . . . . . . . . . . . . . 1

1.2 Introduction to Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . 5

1.3 Data analysis methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6

2 Python language

9

2.1 Basic operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9

2.2 Data types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

2.3 Execution control statements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

2.4 Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

2.5 List comprehensions, iterators, etc. . . . . . . . . . . . . . . . . . . . . . . . . . . 20

2.6 Regular expression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

2.7 System programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

2.8 Scripts and argument parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28

2.9 Networking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29

2.10 Modules and packages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29

2.11 Object Oriented Programming (OOP) . . . . . . . . . . . . . . . . . . . . . . . . 30

2.12 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

3 ScientiÔ¨Åc Python

33

3.1 Numpy: arrays and matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

3.2 Pandas: data manipulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

3.3 Matplotlib: data visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52

4 Statistics

67

4.1 Univariate statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67

4.2 Lab 1: Brain volumes study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103

4.3 Multivariate statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114

4.4 Time Series in python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126

5 Machine Learning

143

5.1 Dimension reduction and feature extraction . . . . . . . . . . . . . . . . . . . . . 143

5.2 Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158

5.3 Linear methods for regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166

5.4 Linear classiÔ¨Åcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180

5.5 Non linear learning algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197

5.6 Resampling Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201

6 Indices and tables

215

i

ii

CHAPTER
ONE
INTRODUCTION
1.1 Python ecosystem for data-science
1.1.1 Python language
‚Ä¢ Interpreted ‚Ä¢ Garbage collector (do not prevent from memory leak) ‚Ä¢ Dynamically-typed language (Java is statically typed)
1.1.2 Anaconda
Anaconda is a python distribution that ships most of python tools and libraries Installation
1. Download anaconda (Python 3.x) http://continuum.io/downloads 2. Install it, on Linux bash Anaconda3-2.4.1-Linux-x86_64.sh 3. Add anaconda path in your PATH variable in your .bashrc Ô¨Åle: export PATH="${HOME}/anaconda3/bin:$PATH" Managing with ‚Äò‚Äòconda‚Äò‚Äò Update conda package and environment manager to current version conda update conda Install additional packages. Those commands install qt back-end (Fix a temporary issue to run spyder) conda install pyqt conda install PyOpenGL conda update --all Install seaborn for graphics
1

Statistics and Machine Learning in Python, Release 0.2
conda install seaborn # install a specific version from anaconda chanel conda install -c anaconda pyqt=4.11.4
List installed packages
conda list
Search available packages
conda search pyqt conda search scikit-learn
Environments ‚Ä¢ A conda environment is a directory that contains a speciÔ¨Åc collection of conda packages that you have installed. ‚Ä¢ Control packages environment for a speciÔ¨Åc purpose: collaborating with someone else, delivering an application to your client, ‚Ä¢ Switch between environments
List of all environments :: conda info ‚Äìenvs
1. Create new environment 2. Activate 3. Install new package
conda create --name test # Or conda env create -f environment.yml source activate test conda info --envs conda list conda search -f numpy conda install numpy
Miniconda Anaconda without the collection of (>700) packages. With Miniconda you download only the packages you want with the conda command: conda install PACKAGENAME
1. Download anaconda (Python 3.x) https://conda.io/miniconda.html 2. Install it, on Linux
bash Miniconda3-latest-Linux-x86_64.sh
3. Add anaconda path in your PATH variable in your .bashrc Ô¨Åle:
export PATH=${HOME}/miniconda3/bin:$PATH
4. Install required packages

2

Chapter 1. Introduction

Statistics and Machine Learning in Python, Release 0.2

conda install -y scipy conda install -y pandas conda install -y matplotlib conda install -y statsmodels conda install -y scikit-learn conda install -y sqlite conda install -y spyder conda install -y jupyter

1.1.3 Commands
python: python interpreter. On the dos/unix command line execute wholes Ô¨Åle: python file.py
Interactive mode: python
Quite with CTL-D ipython: advanced interactive python interpreter: ipython
Quite with CTL-D pip alternative for packages management (update -U in user directory --user): pip install -U --user seaborn
For neuroimaging: pip install -U --user nibabel pip install -U --user nilearn
spyder: IDE (integrated development environment): ‚Ä¢ Syntax highlighting. ‚Ä¢ Code introspection for code completion (use TAB). ‚Ä¢ Support for multiple Python consoles (including IPython). ‚Ä¢ Explore and edit variables from a GUI. ‚Ä¢ Debugging. ‚Ä¢ Navigate in code (go to function deÔ¨Ånition) CTL.
3 or 4 panels:
text editor help/variable explorer ipython interpreter
Shortcuts: - F9 run line/selection

1.1. Python ecosystem for data-science

3

Statistics and Machine Learning in Python, Release 0.2

1.1.4 Libraries

scipy.org: https://www.scipy.org/docs.html
Numpy: Basic numerical operation. Matrix operation plus some basic solvers.:
import numpy as np X = np.array([[1, 2], [3, 4]]) #v = np.array([1, 2]).reshape((2, 1)) v = np.array([1, 2]) np.dot(X, v) # no broadcasting X * v # broadcasting np.dot(v, X) X - X.mean(axis=0)
Scipy: general scientiÔ¨Åc libraries with advanced solver:
import scipy import scipy.linalg scipy.linalg.svd(X, full_matrices=False)
Matplotlib: visualization:
import numpy as np import matplotlib.pyplot as plt #%matplotlib qt x = np.linspace(0, 10, 50) sinus = np.sin(x) plt.plot(x, sinus) plt.show()
Pandas: Manipulation of structured data (tables). input/output excel Ô¨Åles, etc.
Statsmodel: Advanced statistics
Scikit-learn: Machine learning

li-

Arrays

brary Num.

I/O

Numpy X

Scipy

Pan-

das

Stat-

mod-

els

Scikit-

learn

data, Structured comp, data, I/O
X

Solvers: Solvers: basic advanced

X

X

X

Stats: basic
X
X

Stats: advanced
X

Machine learning
X

4

Chapter 1. Introduction

Statistics and Machine Learning in Python, Release 0.2
1.2 Introduction to Machine Learning
1.2.1 Machine learning within data science

Machine learning covers two main types of data analysis:
1. Exploratory analysis: Unsupervised learning. Discover the structure within the data. E.g.: Experience (in years in a company) and salary are correlated.
2. Predictive analysis: Supervised learning. This is sometimes described as ‚Äúlearn from the past to predict the future‚Äù. Scenario: a company wants to detect potential future clients among a base of prospects. Retrospective data analysis: we go through the data constituted of previous prospected companies, with their characteristics (size, domain, localization, etc. . . ). Some of these companies became clients, others did not. The question is, can we possibly predict which of the new companies are more likely to become clients, based on their characteristics based on previous observations? In this example, the training data consists of a set of n training samples. Each sample, Ì†µÌ±•Ì†µÌ±ñ, is a vector of p input features (company characteristics) and a target feature (Ì†µÌ±¶Ì†µÌ±ñ ‚àà {Ì†µÌ±å Ì†µÌ±íÌ†µÌ±†, Ì†µÌ±Å Ì†µÌ±ú} (whether they became a client or not).

1.2.2 IT/computing science tools
‚Ä¢ High Performance Computing (HPC) ‚Ä¢ Data Ô¨Çow, data base, Ô¨Åle I/O, etc.

1.2. Introduction to Machine Learning

5

Statistics and Machine Learning in Python, Release 0.2

‚Ä¢ Python: the programming language. ‚Ä¢ Numpy: python library particularly useful for handling of raw numerical data (matrices,
mathematical operations). ‚Ä¢ Pandas: input/output, manipulation structured data (tables).
1.2.3 Statistics and applied mathematics
‚Ä¢ Linear model. ‚Ä¢ Non parametric statistics. ‚Ä¢ Linear algebra: matrix operations, inversion, eigenvalues.

1.3 Data analysis methodology

1. Formalize customer‚Äôs needs into a learning problem:
‚Ä¢ A target variable: supervised problem.
‚Äì Target is qualitative: classiÔ¨Åcation.
‚Äì Target is quantitative: regression.
‚Ä¢ No target variable: unsupervised problem
‚Äì Vizualisation of high-dimensional samples: PCA, manifolds learning, etc.
‚Äì Finding groups of samples (hidden structure): clustering.
2. Ask question about the datasets
‚Ä¢ Number of samples
‚Ä¢ Number of variables, types of each variable.
3. DeÔ¨Åne the sample
‚Ä¢ For prospective study formalize the experimental design: inclusion/exlusion criteria. The conditions that deÔ¨Åne the acquisition of the dataset.
‚Ä¢ For retrospective study formalize the experimental design: inclusion/exlusion criteria. The conditions that deÔ¨Åne the selection of the dataset.
4. In a document formalize (i) the project objectives; (ii) the required learning dataset (more speciÔ¨Åcally the input data and the target variables); (iii) The conditions that deÔ¨Åne the acquisition of the dataset. In this document, warn the customer that the learned algorithms may not work on new data acquired under different condition.
5. Read the learning dataset.
6. (i) Sanity check (basic descriptive statistics); (ii) data cleaning (impute missing data, recoding); Final Quality Control (QC) perform descriptive statistics and think ! (remove possible confounding variable, etc.).
7. Explore data (visualization, PCA) and perform basic univariate statistics for association between the target an input variables.
8. Perform more complex multivariate-machine learning.

6

Chapter 1. Introduction

Statistics and Machine Learning in Python, Release 0.2
9. Model validation using a left-out-sample strategy (cross-validation, etc.). 10. Apply on new data.

1.3. Data analysis methodology

7

Statistics and Machine Learning in Python, Release 0.2

8

Chapter 1. Introduction

CHAPTER
TWO
PYTHON LANGUAGE

Note: Click here to download the full example code
Source Kevin Markham https://github.com/justmarkham/python-reference Import libraries ‚Äî‚Äî‚Äî‚Äî‚Äî-
# generic import of math module import math math.sqrt(25)
# import a function from math import sqrt sqrt(25) # no longer have to reference the module
# import multiple functions at once from math import cos, floor
# import all functions in a module (generally discouraged) # from os import *
# define an alias import numpy as np
# show all functions in math module content = dir(math)

2.1 Basic operations

# Numbers 10 + 4 10 - 4 10 * 4 10 ** 4 10 / 4 10 / float(4) 5%4

# add (returns 14) # subtract (returns 6) # multiply (returns 40) # exponent (returns 10000) # divide (returns 2 because both types are int ) # divide (returns 2.5) # modulo (returns 1) - also known as the remainder

10 / 4 10 // 4

# true division (returns 2.5) # floor division (returns 2)

(continues on next page)

9

Statistics and Machine Learning in Python, Release 0.2
(continued from previous page)
# Boolean operations # comparisons (these return True) 5>3 5 >= 3 5 != 3 5 == 5
# boolean operations (these return True) 5 > 3 and 6 > 3 5 > 3 or 5 < 3 not False False or not False and True # evaluation order: not, and, or

2.2 Data types

# determine the type of an object

type(2)

# returns int

type(2.0)

# returns float

type( two ) # returns str

type(True)

# returns bool

type(None)

# returns NoneType

# check if an object is of a given type

isinstance(2.0, int)

# returns False

isinstance(2.0, (int, float)) # returns True

# convert an object to a given type float(2) int(2.9) str(2.9)

# zero, None, and empty containers are converted to False bool(0) bool(None) bool( ) # empty string bool([]) # empty list bool({}) # empty dictionary

# non-empty containers and non-zeros are converted to True bool(2) bool( two ) bool([2])

2.2.1 Lists
Different objects categorized along a certain ordered sequence, lists are ordered, iterable, mutable (adding or removing objects changes the list size), can contain multiple data types .. chunk-chap13-001

10

Chapter 2. Python language

Statistics and Machine Learning in Python, Release 0.2

# create an empty list (two ways) empty_list = [] empty_list = list()

# create a list simpsons = [ homer , marge , bart ]

# examine a list simpsons[0] # print element 0 ( homer ) len(simpsons) # returns the length (3)

# modify a list (does not return the list)

simpsons.append( lisa )

# append element to end

simpsons.extend([ itchy , scratchy ]) # append multiple elements to end

simpsons.insert(0, maggie )

# insert element at index 0 (shifts everything‚ê£

Àì‚Üíright)

simpsons.remove( bart )

# searches for first instance and removes it

simpsons.pop(0)

# removes element 0 and returns it

del simpsons[0]

# removes element 0 (does not return it)

simpsons[0] = krusty

# replace element 0

# concatenate lists (slower than extend method) neighbors = simpsons + [ ned , rod , todd ]

# find elements in a list simpsons.count( lisa ) simpsons.index( itchy )

# counts the number of instances # returns index of first instance

# list slicing [start:end:stride]

weekdays = [ mon , tues , wed , thurs , fri ]

weekdays[0]

# element 0

weekdays[0:3]

# elements 0, 1, 2

weekdays[:3]

# elements 0, 1, 2

weekdays[3:]

# elements 3, 4

weekdays[-1]

# last element (element 4)

weekdays[::2]

# every 2nd element (0, 2, 4)

weekdays[::-1]

# backwards (4, 3, 2, 1, 0)

# alternative method for returning the list backwards list(reversed(weekdays))

# sort a list in place (modifies but does not return the list)

simpsons.sort()

simpsons.sort(reverse=True) # sort in reverse

simpsons.sort(key=len)

# sort by a key

# return a sorted list (but does not modify the original list) sorted(simpsons) sorted(simpsons, reverse=True) sorted(simpsons, key=len)

# create a second reference to the same list

num = [1, 2, 3]

same_num = num

same_num[0] = 0

# modifies both num and same_num

# copy a list (three ways)

(continues on next page)

2.2. Data types

11

Statistics and Machine Learning in Python, Release 0.2

new_num = num.copy() new_num = num[:] new_num = list(num)

(continued from previous page)

# examine objects

id(num) == id(same_num) # returns True

id(num) == id(new_num) # returns False

num is same_num

# returns True

num is new_num

# returns False

num == same_num

# returns True

num == new_num

# returns True (their contents are equivalent)

# conatenate +, replicate * [1, 2, 3] + [4, 5, 6] ["a"] * 2 + ["b"] * 3

2.2.2 Tuples

Like lists, but their size cannot change: ordered, iterable, immutable, can contain multiple data types

# create a tuple digits = (0, 1, two ) digits = tuple([0, 1, two ]) zero = (0,)

# create a tuple directly # create a tuple from a list # trailing comma is required to indicate it s a tuple

# examine a tuple digits[2] len(digits) digits.count(0) digits.index(1)

# returns two # returns 3 # counts the number of instances of that value (1) # returns the index of the first instance of that value (1)

# elements of a tuple cannot be modified

# digits[2] = 2

# throws an error

# concatenate tuples digits = digits + (3, 4)

# create a single tuple with elements repeated (also works with lists)

(3, 4) * 2

# returns (3, 4, 3, 4)

# tuple unpacking bart = ( male , 10, simpson ) # create a tuple

2.2.3 Strings

A sequence of characters, they are iterable, immutable

# create a string s = str(42) s = I like you

# convert another data type into a string

(continues on next page)

12

Chapter 2. Python language

Statistics and Machine Learning in Python, Release 0.2

# examine a string

s[0]

# returns I

len(s)

# returns 10

(continued from previous page)

# string slicing like lists

s[:6]

# returns

s[7:]

# returns

s[-1]

# returns

I like you u

# basic string methods (does not modify the original string)

s.lower()

# returns i like you

s.upper()

# returns I LIKE YOU

s.startswith( I ) # returns True

s.endswith( you ) # returns True

s.isdigit()

# returns False (returns True if every character in the string is a‚ê£

Àì‚Üídigit)

s.find( like )

# returns index of first occurrence (2), but doesn t support regex

s.find( hate )

# returns -1 since not found

s.replace( like , love ) # replaces all instances of like with love

# split a string into a list of substrings separated by a delimiter

s.split( )

# returns [ I , like , you ]

s.split()

# same thing

s2 = a, an, the

s2.split( , )

# returns [ a , an , the ]

# join a list of strings into one string using a delimiter stooges = [ larry , curly , moe ]
.join(stooges) # returns larry curly moe

# concatenate strings

s3 = The meaning of life is

s4 = 42

s3 + + s4

# returns The meaning of life is 42

s3 + + str(42) # same thing

# remove whitespace from start and end of a string

s5 = ham and cheese

s5.strip()

# returns ham and cheese

# string substitutions: all of these return raining cats and dogs

raining %s and %s % ( cats , dogs )

# old way

raining {} and {} .format( cats , dogs )

# new way

raining {arg1} and {arg2} .format(arg1= cats ,arg2= dogs ) # named arguments

# string formatting

# more examples: http://mkaz.com/2012/10/10/python-string-format/

pi is {:.2f} .format(3.14159)

# returns pi is 3.14

2.2.4 Strings 2/2
Normal strings allow for escaped characters print( first line\nsecond line )

2.2. Data types

13

Statistics and Machine Learning in Python, Release 0.2

Out: first line second line
raw strings treat backslashes as literal characters print(r first line\nfirst line )
Out: first line\nfirst line
sequece of bytes are not strings, should be decoded before some operations s = b first line\nsecond line print(s) print(s.decode( utf-8 ).split())
Out: b first line\nsecond line [ first , line , second , line ]

2.2.5 Dictionaries

Dictionaries are structures which can contain multiple data types, and is ordered with key-value pairs: for each (unique) key, the dictionary outputs one value. Keys can be strings, numbers, or tuples, while the corresponding values can be any Python object. Dictionaries are: unordered, iterable, mutable
# create an empty dictionary (two ways) empty_dict = {} empty_dict = dict()

# create a dictionary (two ways) family = { dad : homer , mom : marge , size :6} family = dict(dad= homer , mom= marge , size=6)

# convert a list of tuples into a dictionary list_of_tuples = [( dad , homer ), ( mom , marge ), ( size , 6)] family = dict(list_of_tuples)

# examine a dictionary

family[ dad ]

# returns homer

len(family)

# returns 3

family.keys()

# returns list: [ dad , mom , size ]

family.values() # returns list: [ homer , marge , 6]

family.items()

# returns list of tuples:

# [( dad , homer ), ( mom , marge ), ( size , 6)]

mom in family # returns True

marge in family # returns False (only checks keys)

# modify a dictionary (does not return the dictionary)

(continues on next page)

14

Chapter 2. Python language

Statistics and Machine Learning in Python, Release 0.2

(continued from previous page)

family[ cat ] = snowball

# add a new entry

family[ cat ] = snowball ii

# edit an existing entry

del family[ cat ]

# delete an entry

family[ kids ] = [ bart , lisa ]

# value can be a list

family.pop( dad )

# removes an entry and returns the value ( homer )

family.update({ baby : maggie , grandpa : abe }) # add multiple entries

# accessing values more safely with get

family[ mom ]

# returns marge

family.get( mom )

# same thing

try:

family[ grandma ]

# throws an error

except KeyError as e:

print("Error", e)

family.get( grandma )

# returns None

family.get( grandma , not found ) # returns not found (the default)

# accessing a list element within a dictionary

family[ kids ][0]

# returns bart

family[ kids ].remove( lisa )

# removes lisa

# string substitution using a dictionary youngest child is %(baby)s % family # returns youngest child is maggie

Out: Error grandma

2.2.6 Sets

Like dictionaries, but with unique keys only (no corresponding values). They are: unordered, iterable, mutable, can contain multiple data types made up of unique elements (strings, numbers, or tuples)
# create an empty set empty_set = set()

# create a set

languages = { python , r , java }

# create a set directly

snakes = set([ cobra , viper , python ]) # create a set from a list

# examine a set len(languages) python in languages

# returns 3 # returns True

# set operations languages & snakes languages | snakes languages - snakes snakes - languages

# returns intersection: { python } # returns union: { cobra , r , java , viper , # returns set difference: { r , java } # returns set difference: { cobra , viper }

python }

# modify a set (does not return the set)

languages.add( sql )

# add a new element

(continues on next page)

2.2. Data types

15

Statistics and Machine Learning in Python, Release 0.2

(continued from previous page)

languages.add( r )

# try to add an existing element (ignored, no error)

languages.remove( java ) # remove an element

try:

languages.remove( c )

# try to remove a non-existing element (throws an error)

except KeyError as e:

print("Error", e)

languages.discard( c )

# removes an element if present, but ignored otherwise

languages.pop()

# removes and returns an arbitrary element

languages.clear()

# removes all elements

languages.update( go , spark ) # add multiple elements (can also pass a list or set)

# get a sorted list of unique elements from a list sorted(set([9, 0, 2, 1, 0])) # returns [0, 1, 2, 9]

Out: Error c

2.3 Execution control statements
2.3.1 Conditional statements
x=3 # if statement if x > 0:
print( positive )
# if/else statement if x > 0:
print( positive ) else:
print( zero or negative )
# if/elif/else statement if x > 0:
print( positive ) elif x == 0:
print( zero ) else:
print( negative )
# single-line if statement (sometimes discouraged) if x > 0: print( positive )
# single-line if/else statement (sometimes discouraged) # known as a ternary operator positive if x > 0 else zero or negative
Out:
positive positive
16

(continues on next page)
Chapter 2. Python language

Statistics and Machine Learning in Python, Release 0.2

positive positive

(continued from previous page)

2.3.2 Loops

Loops are a set of instructions which repeat until termination conditions are met. This can include iterating through all values in an object, go through a range of values, etc

# range returns a list of integers

range(0, 3) # returns [0, 1, 2]: includes first value but excludes second value

range(3)

# same thing: starting at zero is the default

range(0, 5, 2) # returns [0, 2, 4]: third argument specifies the stride

# for loop (not recommended) fruits = [ apple , banana , for i in range(len(fruits)):
print(fruits[i].upper())

cherry ]

# alternative for loop (recommended style) for fruit in fruits:
print(fruit.upper())

# use range when iterating over a large sequence to avoid actually creating the integer‚ê£ Àì‚Üílist in memory for i in range(10**6):
pass

# iterate through two things at once (using tuple unpacking) family = { dad : homer , mom : marge , size :6} for key, value in family.items():
print(key, value)

# use enumerate if you need to access the index value within the loop for index, fruit in enumerate(fruits):
print(index, fruit)

# for/else loop for fruit in fruits:
if fruit == banana : print("Found the banana!") break # exit the loop and skip the else block
else: # this block executes ONLY if the for loop completes without hitting print("Can t find the banana")

break

# while loop

count = 0

while count < 5:

print("This will print 5 times")

count += 1

# equivalent to count = count + 1

Out:

2.3. Execution control statements

17

Statistics and Machine Learning in Python, Release 0.2

APPLE BANANA CHERRY APPLE BANANA CHERRY dad homer mom marge size 6 0 apple 1 banana 2 cherry Can t find the banana Found the banana! This will print 5 times This will print 5 times This will print 5 times This will print 5 times This will print 5 times
2.3.3 Exceptions handling
dct = dict(a=[1, 2], b=[4, 5])
key = c try:
dct[key] except:
print("Key %s is missing. Add it with empty value" % key) dct[ c ] = []
print(dct)
Out:
Key c is missing. Add it with empty value { a : [1, 2], b : [4, 5], c : []}

2.4 Functions

Functions are sets of instructions launched when called upon, they can have multiple input values and a return value
# define a function with no arguments and no return values def print_text():
print( this is text )

# call the function print_text()

# define a function with one argument and no return values def print_this(x):

(continues on next page)

18

Chapter 2. Python language

Statistics and Machine Learning in Python, Release 0.2

print(x)

(continued from previous page)

# call the function

print_this(3)

# prints 3

n = print_this(3) # prints 3, but doesn t assign 3 to n

# because the function has no return statement

# define a function with one argument and one return value def square_this(x):
return x ** 2

# include an optional docstring to describe the effect of a function def square_this(x):
"""Return the square of a number.""" return x ** 2

# call the function square_this(3) var = square_this(3)

# prints 9 # assigns 9 to var, but does not print 9

# default arguments def power_this(x, power=2):
return x ** power

power_this(2) # 4 power_this(2, 3) # 8

# use pass as a placeholder if you haven t written the function body def stub():
pass

# return two values from a single function def min_max(nums):
return min(nums), max(nums)

# return values can be assigned to a single variable as a tuple

nums = [1, 2, 3]

min_max_num = min_max(nums)

# min_max_num = (1, 3)

# return values can be assigned into multiple variables using tuple unpacking min_num, max_num = min_max(nums) # min_num = 1, max_num = 3

Out:
this is text 3 3

2.4. Functions

19

Statistics and Machine Learning in Python, Release 0.2

2.5 List comprehensions, iterators, etc.

2.5.1 List comprehensions

Process which affects whole lists without iterating through loops. For more: http:// python-3-patterns-idioms-test.readthedocs.io/en/latest/Comprehensions.html
# for loop to create a list of cubes nums = [1, 2, 3, 4, 5] cubes = [] for num in nums:
cubes.append(num**3)

# equivalent list comprehension cubes = [num**3 for num in nums]

# [1, 8, 27, 64, 125]

# for loop to create a list of cubes of even numbers cubes_of_even = [] for num in nums:
if num % 2 == 0: cubes_of_even.append(num**3)

# equivalent list comprehension # syntax: [expression for variable in iterable if condition] cubes_of_even = [num**3 for num in nums if num % 2 == 0] # [8, 64]

# for loop to cube even numbers and square odd numbers cubes_and_squares = [] for num in nums:
if num % 2 == 0: cubes_and_squares.append(num**3)
else: cubes_and_squares.append(num**2)

# equivalent list comprehension (using a ternary expression) # syntax: [true_condition if condition else false_condition for variable in iterable] cubes_and_squares = [num**3 if num % 2 == 0 else num**2 for num in nums] # [1, 8, 9,‚ê£ Àì‚Üí64, 25]

# for loop to flatten a 2d-matrix matrix = [[1, 2], [3, 4]] items = [] for row in matrix:
for item in row: items.append(item)

# equivalent list comprehension items = [item for row in matrix
for item in row]

# [1, 2, 3, 4]

# set comprehension fruits = [ apple , banana , cherry ] unique_lengths = {len(fruit) for fruit in fruits}

# {5, 6}

# dictionary comprehension fruit_lengths = {fruit:len(fruit) for fruit in fruits} Àì‚Üí : 6, cherry : 6}

# { apple : 5, banana (continues on next page)

20

Chapter 2. Python language

Statistics and Machine Learning in Python, Release 0.2
(continued from previous page)

2.6 Regular expression

1. Compile Regular expression with a patetrn
import re
# 1. compile Regular expression with a patetrn regex = re.compile("^.+(sub-.+)_(ses-.+)_(mod-.+)")
2. Match compiled RE on string Capture the pattern anyprefixsub-<subj id>_ses-<session id>_<modality> strings = ["abcsub-033_ses-01_mod-mri", "defsub-044_ses-01_mod-mri", "ghisub-055_ses-02_ Àì‚Üímod-ctscan" ] print([regex.findall(s)[0] for s in strings])
Out: [( sub-033 , ses-01 , mod-mri ), ( sub-044 , ses-01 , mod-mri ), ( sub-055 , ses-02 , Àì‚Üí mod-ctscan )]
Match methods on compiled regular expression

Method/Attribute match(string) search(string) Ô¨Åndall(string) Ô¨Ånditer(string)

Purpose Determine if the RE matches at the beginning of the string. Scan through a string, looking for any location where this RE matches. Find all substrings where the RE matches, and returns them as a list. Find all substrings where the RE matches, and returns them as an iterator.

2. Replace compiled RE on string
regex = re.compile("(sub-[^_]+)") # match (sub-...)_ print([regex.sub("SUB-", s) for s in strings])

regex.sub("SUB-", "toto")

Out: [ abcSUB-_ses-01_mod-mri , defSUB-_ses-01_mod-mri , ghiSUB-_ses-02_mod-ctscan ]

Replace all non-alphanumeric characters in a string re.sub( [^0-9a-zA-Z]+ , , h^&ell .,|o w]{+orld )

2.6. Regular expression

21

Statistics and Machine Learning in Python, Release 0.2
2.7 System programming
2.7.1 Operating system interfaces (os)
import os
Current working directory # Get the current working directory cwd = os.getcwd() print(cwd) # Set the current working directory os.chdir(cwd)
Out: /home/edouard/git/pystatsml/python_lang
Temporary directory import tempfile tmpdir = tempfile.gettempdir()
Join paths mytmpdir = os.path.join(tmpdir, "foobar") # list containing the names of the entries in the directory given by path. os.listdir(tmpdir)
Create a directory if not os.path.exists(mytmpdir):
os.mkdir(mytmpdir) os.makedirs(os.path.join(tmpdir, "foobar", "plop", "toto"), exist_ok=True)

2.7.2 File input/output
filename = os.path.join(mytmpdir, "myfile.txt") print(filename)
# Write lines = ["Dans python tout est bon", "Enfin, presque"]
## write line by line fd = open(filename, "w") fd.write(lines[0] + "\n") fd.write(lines[1]+ "\n") fd.close()
22

(continues on next page)
Chapter 2. Python language

Statistics and Machine Learning in Python, Release 0.2

## use a context manager to automatically close your file with open(filename, w ) as f:
for line in lines: f.write(line + \n )

(continued from previous page)

# Read ## read one line at a time (entire file does not have to fit into memory) f = open(filename, "r") f.readline() # one string per line (including newlines) f.readline() # next line f.close()

## read one line at a time (entire file does not have to fit into memory) f = open(filename, r ) f.readline() # one string per line (including newlines) f.readline() # next line f.close()

## read the whole file at once, return a list of lines f = open(filename, r ) f.readlines() # one list, each line is one string f.close()

## use list comprehension to duplicate readlines without reading entire file at once f = open(filename, r ) [line for line in f] f.close()

## use a context manager to automatically close your file with open(filename, r ) as f:
lines = [line for line in f]

Out: /tmp/foobar/myfile.txt

2.7.3 Explore, list directories
Walk
import os
WD = os.path.join(os.environ["HOME"], "git", "pystatsml", "datasets")
for dirpath, dirnames, filenames in os.walk(WD): print(dirpath, dirnames, filenames)
Out:
/home/edouard/git/pystatsml/datasets [ brain_volumes ] [ multiTimeline.csv , Advertising. Àì‚Üícsv , salary_table.csv , birthwt.txt , default of credit card clients.xls , Àì‚Üí eurodist.csv , birthwt.csv , iris.csv , readme.rst , s_curve.csv ] /home/edouard/git/pystatsml/datasets/brain_volumes [] [ brain_volumes.xlsx , gm.csv , Àì‚Üí brain_volumes.csv , wm.csv , demo.csv , csf.csv ]

2.7. System programming

23

Statistics and Machine Learning in Python, Release 0.2
glob, basename and Ô¨Åle extension
import glob
filenames = glob.glob(os.path.join(os.environ["HOME"], "git", "pystatsml", "datasets", "*", "tissue-*.csv"))
# take basename then remove extension basenames = [os.path.splitext(os.path.basename(f))[0] for f in filenames] print(basenames)
Out: []
shutil - High-level Ô¨Åle operations
import shutil
src = os.path.join(tmpdir, "foobar", "myfile.txt") dst = os.path.join(tmpdir, "foobar", "plop", "myfile.txt") print("copy %s to %s" % (src, dst))
shutil.copy(src, dst)
print("File %s exists ?" % dst, os.path.exists(dst))
src = os.path.join(tmpdir, "foobar", "plop") dst = os.path.join(tmpdir, "plop2") print("copy tree %s under %s" % (src, dst))
try: shutil.copytree(src, dst)
shutil.rmtree(dst)
shutil.move(src, dst) except (FileExistsError, FileNotFoundError) as e:
pass
Out: copy /tmp/foobar/myfile.txt to /tmp/foobar/plop/myfile.txt File /tmp/foobar/plop/myfile.txt exists ? True copy tree /tmp/foobar/plop under /tmp/plop2
2.7.4 Command execution with subprocess
‚Ä¢ For more advanced use cases, the underlying Popen interface can be used directly. ‚Ä¢ Run the command described by args. ‚Ä¢ Wait for command to complete ‚Ä¢ return a CompletedProcess instance.

24

Chapter 2. Python language

Statistics and Machine Learning in Python, Release 0.2

‚Ä¢ Does not capture stdout or stderr by default. To do so, pass PIPE for the stdout and/or stderr arguments.
import subprocess
# doesn t capture output p = subprocess.run(["ls", "-l"]) print(p.returncode)
# Run through the shell. subprocess.run("ls -l", shell=True)
# Capture output out = subprocess.run(["ls", "-a", "/"], stdout=subprocess.PIPE, stderr=subprocess.STDOUT) # out.stdout is a sequence of bytes that should be decoded into a utf-8 string print(out.stdout.decode( utf-8 ).split("\n")[:5])
Out:
0 [ . , .. , bin , boot , cdrom ]

2.7.5 Multiprocessing and multithreading
Process A process is a name given to a program instance that has been loaded into memory and managed by the operating system. Process = address space + execution context (thread of control) Process address space (segments):
‚Ä¢ Code. ‚Ä¢ Data (static/global). ‚Ä¢ Heap (dynamic memory allocation). ‚Ä¢ Stack. Execution context: ‚Ä¢ Data registers. ‚Ä¢ Stack pointer (SP). ‚Ä¢ Program counter (PC). ‚Ä¢ Working Registers. OS Scheduling of processes: context switching (ie. save/load Execution context) Pros/cons ‚Ä¢ Context switching expensive. ‚Ä¢ (potentially) complex data sharing (not necessary true). ‚Ä¢ Cooperating processes - no need for memory protection (separate address
spaces).

2.7. System programming

25

Statistics and Machine Learning in Python, Release 0.2
‚Ä¢ Relevant for parrallel computation with memory allocation.
Threads
‚Ä¢ Threads share the same address space (Data registers): access to code, heap and (global) data.
‚Ä¢ Separate execution stack, PC and Working Registers.
Pros/cons
‚Ä¢ Faster context switching only SP, PC and Working Registers.
‚Ä¢ Can exploit Ô¨Åne-grain concurrency
‚Ä¢ Simple data sharing through the shared address space.
‚Ä¢ Precautions have to be taken or two threads will write to the same memory at the same time. This is what the global interpreter lock (GIL) is for.
‚Ä¢ Relevant for GUI, I/O (Network, disk) concurrent operation
In Python ‚Ä¢ The threading module uses threads. ‚Ä¢ The multiprocessing module uses processes.
Multithreading
import time import threading
def list_append(count, sign=1, out_list=None): if out_list is None: out_list = list() for i in range(count): out_list.append(sign * i) sum(out_list) # do some computation return out_list
size = 10000 # Number of numbers to add
out_list = list() # result is a simple list thread1 = threading.Thread(target=list_append, args=(size, 1, out_list, )) thread2 = threading.Thread(target=list_append, args=(size, -1, out_list, ))
startime = time.time() # Will execute both in parallel thread1.start() thread2.start() # Joins threads back to the parent process thread1.join() thread2.join() print("Threading ellapsed time ", time.time() - startime)
print(out_list[:10])
Out:

26

Chapter 2. Python language

Statistics and Machine Learning in Python, Release 0.2
Threading ellapsed time 1.685194492340088 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
Multiprocessing
import multiprocessing
# Sharing requires specific mecanism out_list1 = multiprocessing.Manager().list() p1 = multiprocessing.Process(target=list_append, args=(size, 1, None)) out_list2 = multiprocessing.Manager().list() p2 = multiprocessing.Process(target=list_append, args=(size, -1, None))
startime = time.time() p1.start() p2.start() p1.join() p2.join() print("Multiprocessing ellapsed time ", time.time() - startime)
# print(out_list[:10]) is not availlable
Out:
Multiprocessing ellapsed time 0.38483119010925293
Sharing object between process with Managers Managers provide a way to create data which can be shared between different processes, including sharing over a network between processes running on different machines. A manager object controls a server process which manages shared objects.
import multiprocessing import time
size = int(size / 100) # Number of numbers to add
# Sharing requires specific mecanism out_list = multiprocessing.Manager().list() p1 = multiprocessing.Process(target=list_append, args=(size, 1, out_list)) p2 = multiprocessing.Process(target=list_append, args=(size, -1, out_list))
startime = time.time()
p1.start() p2.start()
p1.join() p2.join()
print(out_list[:10])
print("Multiprocessing with shared object ellapsed time ", time.time() - startime)
Out:

2.7. System programming

27

Statistics and Machine Learning in Python, Release 0.2

[0, 0, 1, -1, 2, -2, 3, -3, 4, -4] Multiprocessing with shared object ellapsed time 1.1143779754638672

2.8 Scripts and argument parsing

Example, the word count script

import os import os.path import argparse import re import pandas as pd

if __name__ == "__main__": # parse command line options output = "word_count.csv" parser = argparse.ArgumentParser() parser.add_argument( -i , --input , help= list of input files. , nargs= + , type=str) parser.add_argument( -o , --output , help= output csv file (default %s) type=str, default=output) options = parser.parse_args()

% output,

if options.input is None : parser.print_help() raise SystemExit("Error: input files are missing")
else: filenames = [f for f in options.input if os.path.isfile(f)]

# Match words regex = re.compile("[a-zA-Z]+")

count = dict() for filename in filenames:
fd = open(filename, "r") for line in fd:
for word in regex.findall(line.lower()): if not word in count: count[word] = 1 else: count[word] += 1

fd = open(options.output, "w")

# Pandas df = pd.DataFrame([[k, count[k]] for k in count], columns=["word", "count"]) df.to_csv(options.output, index=False)

28

Chapter 2. Python language

2.9 Networking
# TODO

Statistics and Machine Learning in Python, Release 0.2

2.9.1 FTP

# Full FTP features with ftplib import ftplib ftp = ftplib.FTP("ftp.cea.fr") ftp.login() ftp.cwd( /pub/unati/people/educhesnay/pystatml ) ftp.retrlines( LIST )
fd = open(os.path.join(tmpdir, "README.md"), "wb") ftp.retrbinary( RETR README.md , fd.write) fd.close() ftp.quit()
# File download urllib import urllib.request ftp_url = ftp://ftp.cea.fr/pub/unati/people/educhesnay/pystatml/README.md urllib.request.urlretrieve(ftp_url, os.path.join(tmpdir, "README2.md"))

Out:

-rw-r--r-- 1 ftp

ftp

1282 Apr 15 22:33 README.md

-rw-r--r-- 1 ftp

ftp

6301451 Mar 14 00:29‚ê£

Àì‚ÜíStatisticsMachineLearningPythonDraft.pdf

2.9.2 HTTP
# TODO

2.9.3 Sockets
# TODO

2.9.4 xmlrpc
# TODO

2.10 Modules and packages

A module is a Python Ô¨Åle. A package is a directory which MUST contain a special Ô¨Åle called __init__.py
To import, extend variable PYTHONPATH:

2.9. Networking

29

Statistics and Machine Learning in Python, Release 0.2

export PYTHONPATH=path_to_parent_python_module:${PYTHONPATH}
Or
import sys sys.path.append("path_to_parent_python_module")
The __init__.py Ô¨Åle can be empty. But you can set which modules the package exports as the API, while keeping other modules internal, by overriding the __all__ variable, like so: parentmodule/__init__.py Ô¨Åle: from . import submodule1 from . import submodule2
from .submodule3 import function1 from .submodule3 import function2
__all__ = ["submodule1", "submodule2", "function1", "function2"]
User can import:
import parentmodule.submodule1 import parentmodule.function1
Python Unit Testing

2.11 Object Oriented Programming (OOP)

Sources ‚Ä¢ http://python-textbok.readthedocs.org/en/latest/Object_Oriented_Programming.html
Principles ‚Ä¢ Encapsulate data (attributes) and code (methods) into objects. ‚Ä¢ Class = template or blueprint that can be used to create objects. ‚Ä¢ An object is a speciÔ¨Åc instance of a class. ‚Ä¢ Inheritance: OOP allows classes to inherit commonly used state and behaviour from other classes. Reduce code duplication ‚Ä¢ Polymorphism: (usually obtained through polymorphism) calling code is agnostic as to whether an object belongs to a parent class or one of its descendants (abstraction, modularity). The same method called on 2 objects of 2 different classes will behave differently.

import math

class Shape2D: def area(self): raise NotImplementedError()

# __init__ is a special method called the constructor

(continues on next page)

30

Chapter 2. Python language

Statistics and Machine Learning in Python, Release 0.2

# Inheritance + Encapsulation class Square(Shape2D):
def __init__(self, width): self.width = width
def area(self): return self.width ** 2
class Disk(Shape2D): def __init__(self, radius): self.radius = radius
def area(self): return math.pi * self.radius ** 2
shapes = [Square(2), Disk(3)]
# Polymorphism print([s.area() for s in shapes])
s = Shape2D() try:
s.area() except NotImplementedError as e:
print("NotImplementedError")
Out:
[4, 28.274333882308138] NotImplementedError

(continued from previous page)

2.12 Exercises
2.12.1 Exercise 1: functions
Create a function that acts as a simple calulator If the operation is not speciÔ¨Åed, default to addition If the operation is misspeciÔ¨Åed, return an prompt message Ex: calc(4,5,"multiply") returns 20 Ex: calc(3,5) returns 8 Ex: calc(1, 2, "something") returns error message
2.12.2 Exercise 2: functions + list + loop
Given a list of numbers, return a list where all adjacent duplicate elements have been reduced to a single element. Ex: [1, 2, 2, 3, 2] returns [1, 2, 3, 2]. You may create a new list or modify the passed in list. Remove all duplicate values (adjacent or not) Ex: [1, 2, 2, 3, 2] returns [1, 2, 3]

2.12. Exercises

31

Statistics and Machine Learning in Python, Release 0.2
2.12.3 Exercise 3: File I/O
1. Copy/paste the BSD 4 clause license (https://en.wikipedia.org/wiki/BSD_licenses) into a text Ô¨Åle. Read, the Ô¨Åle and count the occurrences of each word within the Ô¨Åle. Store the words‚Äô occurrence number in a dictionary. 2. Write an executable python command count_words.py that parse a list of input Ô¨Åles provided after --input parameter. The dictionary of occurrence is save in a csv Ô¨Åle provides by --output. with default value word_count.csv. Use: - open - regular expression - argparse (https://docs. python.org/3/howto/argparse.html)
2.12.4 Exercise 4: OOP
1. Create a class Employee with 2 attributes provided in the constructor: name, years_of_service. With one method salary with is obtained by 1500 + 100 * years_of_service.
2. Create a subclass Manager which redeÔ¨Åne salary method 2500 + 120 * years_of_service. 3. Create a small dictionary-nosed database where the key is the employee‚Äôs name. Populate
the database with: samples = Employee(‚Äòlucy‚Äô, 3), Employee(‚Äòjohn‚Äô, 1), Manager(‚Äòjulie‚Äô, 10), Manager(‚Äòpaul‚Äô, 3) 4. Return a table of made name, salary rows, i.e. a list of list [[name, salary]] 5. Compute the average salary Total running time of the script: ( 0 minutes 4.315 seconds)

32

Chapter 2. Python language

CHAPTER
THREE
SCIENTIFIC PYTHON

Note: Click here to download the full example code

3.1 Numpy: arrays and matrices
NumPy is an extension to the Python programming language, adding support for large, multidimensional (numerical) arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays. Sources:
‚Ä¢ Kevin Markham: https://github.com/justmarkham
import numpy as np

3.1.1 Create arrays

Create ndarrays from lists. note: every element must be the same type (will be converted if possible)

data1 = [1, 2, 3, 4, 5] arr1 = np.array(data1) data2 = [range(1, 5), range(5, 9)] arr2 = np.array(data2) arr2.tolist()

# list # 1d array # list of lists # 2d array # convert array back to list

create special arrays
np.zeros(10) np.zeros((3, 6)) np.ones(10) np.linspace(0, 1, 5) np.logspace(0, 3, 4)

# 0 to 1 (inclusive) with 5 points # 10^0 to 10^3 (inclusive) with 4 points

arange is like range, except it returns an array (not a list)
int_array = np.arange(5) float_array = int_array.astype(float)

33

Statistics and Machine Learning in Python, Release 0.2

3.1.2 Examining arrays

arr1.dtype arr2.dtype arr2.ndim arr2.shape arr2.size len(arr2)

# float64 # int32 #2 # (2, 4) - axis 0 is rows, axis 1 is columns # 8 - total number of elements # 2 - size of first dimension (aka axis)

3.1.3 Reshaping
arr = np.arange(10, dtype=float).reshape((2, 5)) print(arr.shape) print(arr.reshape(5, 2))
Out:
(2, 5) [[0. 1.] [2. 3.] [4. 5.] [6. 7.] [8. 9.]]
Add an axis
a = np.array([0, 1]) a_col = a[:, np.newaxis] print(a_col) #or a_col = a[:, None]
Out:
[[0] [1]]
Transpose
print(a_col.T)
Out:
[[0 1]]
Flatten: always returns a Ô¨Çat copy of the orriginal array
arr_flt = arr.flatten() arr_flt[0] = 33 print(arr_flt) print(arr)
Out:

34

Chapter 3. ScientiÔ¨Åc Python

Statistics and Machine Learning in Python, Release 0.2

[33. 1. 2. 3. 4. 5. 6. 7. 8. 9.] [[0. 1. 2. 3. 4.] [5. 6. 7. 8. 9.]]
Ravel: returns a view of the original array whenever possible.
arr_flt = arr.ravel() arr_flt[0] = 33 print(arr_flt) print(arr)
Out:
[33. 1. 2. 3. 4. 5. 6. 7. 8. 9.] [[33. 1. 2. 3. 4.] [ 5. 6. 7. 8. 9.]]

3.1.4 Stack arrays
Stack Ô¨Çat arrays in columns
a = np.array([0, 1]) b = np.array([2, 3])
ab = np.stack((a, b)).T print(ab)
# or np.hstack((a[:, None], b[:, None]))
Out:
[[0 2] [1 3]]

3.1.5 Selection

Single item

arr = np.arange(10, dtype=float).reshape((2, 5))

arr[0] arr[0, 3] arr[0][3]

# 0th element (slices like a list) # row 0, column 3: returns 4 # alternative syntax

Slicing Syntax: start:stop:step with start (default 0) stop (default last) step (default 1)

3.1. Numpy: arrays and matrices

35

Statistics and Machine Learning in Python, Release 0.2

arr[0, :]

# row 0: returns 1d array ([1, 2, 3, 4])

arr[:, 0]

# column 0: returns 1d array ([1, 5])

arr[:, :2] # columns strictly before index 2 (2 first columns)

arr[:, 2:] # columns after index 2 included

arr2 = arr[:, 1:4] # columns between index 1 (included) and 4 (excluded)

print(arr2)

Out:
[[1. 2. 3.] [6. 7. 8.]]

Slicing returns a view (not a copy)
arr2[0, 0] = 33 print(arr2) print(arr)

Out:

[[33. 2. [ 6. 7. [[ 0. 33. [ 5. 6.

3.] 8.]] 2. 3. 7. 8.

4.] 9.]]

Row 0: reverse order
print(arr[0, ::-1])
# The rule of thumb here can be: in the context of lvalue indexing (i.e. the indices are‚ê£ Àì‚Üíplaced in the left hand side value of an assignment), no view or copy of the array is‚ê£ Àì‚Üícreated (because there is no need to). However, with regular values, the above rules‚ê£ Àì‚Üífor creating views does apply.

Out: [ 4. 3. 2. 33. 0.]

Fancy indexing: Integer or boolean array indexing

Fancy indexing returns a copy not a view.

Integer array indexing

arr2 = arr[:, [1,2,3]] print(arr2) arr2[0, 0] = 44 print(arr2) print(arr)

# return a copy

Out:
[[33. 2. 3.] [ 6. 7. 8.]]

36

(continues on next page)
Chapter 3. ScientiÔ¨Åc Python

Statistics and Machine Learning in Python, Release 0.2

[[44. 2. [ 6. 7. [[ 0. 33. [ 5. 6.

3.] 8.]] 2. 3. 7. 8.

4.] 9.]]

(continued from previous page)

Boolean arrays indexing
arr2 = arr[arr > 5] # return a copy
print(arr2) arr2[0] = 44 print(arr2) print(arr)

Out:
[33. 6. 7. 8. 9.] [44. 6. 7. 8. 9.] [[ 0. 33. 2. 3. 4.] [ 5. 6. 7. 8. 9.]]

However, In the context of lvalue indexing (left hand side value of an assignment) Fancy authorizes the modiÔ¨Åcation of the original array
arr[arr > 5] = 0 print(arr)

Out:
[[0. 0. 2. 3. 4.] [5. 0. 0. 0. 0.]]

Boolean arrays indexing continues

names = np.array([ Bob , Joe , Will , Bob ])

names == Bob

# returns a boolean array

names[names != Bob ]

# logical selection

(names == Bob ) | (names == Will ) # keywords "and/or" don t work with boolean arrays

names[names != Bob ] = Joe

# assign based on a logical selection

np.unique(names)

# set function

3.1.6 Vectorized operations

nums = np.arange(5)

nums * 10

# multiply each element by 10

nums = np.sqrt(nums)

# square root of each element

np.ceil(nums)

# also floor, rint (round to nearest int)

np.isnan(nums)

# checks for NaN

nums + np.arange(5)

# add element-wise

np.maximum(nums, np.array([1, -2, 3, -4, 5])) # compare element-wise

# Compute Euclidean distance between 2 vectors vec1 = np.random.randn(10)

(continues on next page)

3.1. Numpy: arrays and matrices

37

Statistics and Machine Learning in Python, Release 0.2

vec2 = np.random.randn(10) dist = np.sqrt(np.sum((vec1 - vec2) ** 2))

# math and stats

rnd = np.random.randn(4, 2) # random normals in 4x2 array

rnd.mean()

rnd.std()

rnd.argmin()

# index of minimum element

rnd.sum()

rnd.sum(axis=0)

# sum of columns

rnd.sum(axis=1)

# sum of rows

# methods for boolean arrays

(rnd > 0).sum()

# counts number of positive values

(rnd > 0).any()

# checks if any value is True

(rnd > 0).all()

# checks if all values are True

# random numbers

np.random.seed(12234)

# Set the seed

np.random.rand(2, 3)

# 2 x 3 matrix in [0, 1]

np.random.randn(10)

# random normals (mean 0, sd 1)

np.random.randint(0, 2, 10) # 10 randomly picked 0 or 1

(continued from previous page)

3.1.7 Broadcasting
Sources: https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html Implicit conversion to allow operations on arrays of different sizes. - The smaller array is stretched or ‚Äúbroadcasted‚Äù across the larger array so that they have compatible shapes. - Fast vectorized operation in C instead of Python. - No needless copies.
Rules
Starting with the trailing axis and working backward, Numpy compares arrays dimensions. ‚Ä¢ If two dimensions are equal then continues ‚Ä¢ If one of the operand has dimension 1 stretches it to match the largest one ‚Ä¢ When one of the shapes runs out of dimensions (because it has less dimensions than the other shape), Numpy will use 1 in the comparison process until the other shape‚Äôs dimensions run out as well.
a = np.array([[ 0, 0, 0], [10, 10, 10], [20, 20, 20], [30, 30, 30]])
b = np.array([0, 1, 2])
print(a + b)
Out:

38

Chapter 3. ScientiÔ¨Åc Python

Statistics and Machine Learning in Python, Release 0.2

Fig. 1: Source: http://www.scipy-lectures.org

3.1. Numpy: arrays and matrices

39

Statistics and Machine Learning in Python, Release 0.2

[[ 0 1 2] [10 11 12] [20 21 22] [30 31 32]]

Examples Shapes of operands A, B and result:

A

(2d array): 5 x 4

B

(1d array):

1

Result (2d array): 5 x 4

A

(2d array): 5 x 4

B

(1d array):

4

Result (2d array): 5 x 4

A

(3d array): 15 x 3 x 5

B

(3d array): 15 x 1 x 5

Result (3d array): 15 x 3 x 5

A

(3d array): 15 x 3 x 5

B

(2d array):

3x5

Result (3d array): 15 x 3 x 5

A

(3d array): 15 x 3 x 5

B

(2d array):

3x1

Result (3d array): 15 x 3 x 5

3.1.8 Exercises
Given the array: X = np.random.randn(4, 2) # random normals in 4x2 array
‚Ä¢ For each column Ô¨Ånd the row index of the minimum value. ‚Ä¢ Write a function standardize(X) that return an array whose columns are centered and
scaled (by std-dev). Total running time of the script: ( 0 minutes 0.037 seconds)
Note: Click here to download the full example code

3.2 Pandas: data manipulation
It is often said that 80% of data analysis is spent on the cleaning and small, but important, aspect of data manipulation and cleaning with Pandas. Sources:
‚Ä¢ Kevin Markham: https://github.com/justmarkham

40

Chapter 3. ScientiÔ¨Åc Python

Statistics and Machine Learning in Python, Release 0.2

‚Ä¢ Pandas doc: http://pandas.pydata.org/pandas-docs/stable/index.html
Data structures
‚Ä¢ Series is a one-dimensional labeled array capable of holding any data type (integers, strings, Ô¨Çoating point numbers, Python objects, etc.). The axis labels are collectively referred to as the index. The basic method to create a Series is to call pd.Series([1,3,5,np.nan,6,8])
‚Ä¢ DataFrame is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dict of Series objects. It stems from the R data.frame() object.
from __future__ import print_function
import pandas as pd import numpy as np import matplotlib.pyplot as plt

3.2.1 Create DataFrame

columns = [ name , age , gender , job ]
user1 = pd.DataFrame([[ alice , 19, "F", "student"], [ john , 26, "M", "student"]],
columns=columns)
user2 = pd.DataFrame([[ eric , 22, "M", "student"], [ paul , 58, "F", "manager"]],
columns=columns)
user3 = pd.DataFrame(dict(name=[ peter , julie ], age=[33, 44], gender=[ M , F ], job=[ engineer , scientist ]))
print(user3)

Out:

name age gender 0 peter 33 1 julie 44

job M engineer F scientist

3.2.2 Combining DataFrames
Concatenate DataFrame
user1.append(user2) users = pd.concat([user1, user2, user3]) print(users)
Out:

3.2. Pandas: data manipulation

41

Statistics and Machine Learning in Python, Release 0.2

name age gender 0 alice 19 1 john 26 0 eric 22 1 paul 58 0 peter 33 1 julie 44

job F student M student M student F manager M engineer F scientist

Join DataFrame

user4 = pd.DataFrame(dict(name=[ alice , john , eric , julie ], height=[165, 180, 175, 171]))
print(user4)

Out:
name height 0 alice 165 1 john 180 2 eric 175 3 julie 171

Use intersection of keys from both frames merge_inter = pd.merge(users, user4, on="name") print(merge_inter)

Out:

name age gender 0 alice 19 1 john 26 2 eric 22 3 julie 44

job height F student 165 M student 180 M student 175 F scientist 171

Use union of keys from both frames
users = pd.merge(users, user4, on="name", how= outer ) print(users)

Out:

name age gender 0 alice 19 1 john 26 2 eric 22 3 paul 58 4 peter 33 5 julie 44

job height F student 165.0 M student 180.0 M student 175.0 F manager NaN M engineer NaN F scientist 171.0

Reshaping by pivoting

‚ÄúUnpivots‚Äù a DataFrame from wide format to long (stacked) format,

42

Chapter 3. ScientiÔ¨Åc Python

Statistics and Machine Learning in Python, Release 0.2

staked = pd.melt(users, id_vars="name", var_name="variable", value_name="value") print(staked)

Out:

name variable

value

0 alice

age

19

1 john

age

26

2 eric

age

22

3 paul

age

58

4 peter

age

33

5 julie

age

44

6 alice gender

F

7 john gender

M

8 eric gender

M

9 paul gender

F

10 peter gender

M

11 julie gender

F

12 alice

job student

13 john

job student

14 eric

job student

15 paul

job manager

16 peter

job engineer

17 julie

job scientist

18 alice height

165

19 john height

180

20 eric height

175

21 paul height

NaN

22 peter height

NaN

23 julie height

171

‚Äúpivots‚Äù a DataFrame from long (stacked) format to wide format, print(staked.pivot(index= name , columns= variable , values= value ))

Out:

variable age gender height

name

alice 19

F 165

eric

22

M 175

john

26

M 180

julie 44

F 171

paul

58

F NaN

peter 33

M NaN

job
student student student scientist manager engineer

3.2.3 Summarizing

# examine the users data

users type(users) users.head() users.tail()

# print the first 30 and last 30 rows # DataFrame # print the first 5 rows # print the last 5 rows

3.2. Pandas: data manipulation

(continues on next page)
43

Statistics and Machine Learning in Python, Release 0.2

(continued from previous page)

users.index users.columns users.dtypes users.shape users.values users.info()

# "the index" (aka "the labels") # column names (which is "an index") # data types of each column # number of rows and columns # underlying numpy array # concise summary (includes memory usage as of pandas 0.15.0)

Out:

<class pandas.core.frame.DataFrame >

Int64Index: 6 entries, 0 to 5

Data columns (total 5 columns):

name

6 non-null object

age

6 non-null int64

gender 6 non-null object

job

6 non-null object

height 4 non-null float64

dtypes: float64(1), int64(1), object(3)

memory usage: 288.0+ bytes

3.2.4 Columns selection

users[ gender ] type(users[ gender ]) users.gender

# select one column # Series # select one column using the DataFrame

# select multiple columns users[[ age , gender ]] my_cols = [ age , gender ] users[my_cols] type(users[my_cols])

# select two columns # or, create a list... # ...and use that list to select columns # DataFrame

3.2.5 Rows selection (basic)
iloc is strictly integer position based
df = users.copy() df.iloc[0] # first row df.iloc[0, 0] # first item of first row df.iloc[0, 0] = 55
for i in range(users.shape[0]): row = df.iloc[i] row.age *= 100 # setting a copy, and not the original frame data.
print(df) # df is not modified
Out:

44

Chapter 3. ScientiÔ¨Åc Python

Statistics and Machine Learning in Python, Release 0.2

name age gender 0 55 19 1 john 26 2 eric 22 3 paul 58 4 peter 33 5 julie 44

job height F student 165.0 M student 180.0 M student 175.0 F manager NaN M engineer NaN F scientist 171.0

ix supports mixed integer and label based access.

df = users.copy()

df.ix[0]

# first row

df.ix[0, "age"] # first item of first row

df.ix[0, "age"] = 55

for i in range(df.shape[0]): df.ix[i, "age"] *= 10

print(df) # df is modified

Out:

name age gender 0 alice 550 1 john 260 2 eric 220 3 paul 580 4 peter 330 5 julie 440

job height F student 165.0 M student 180.0 M student 175.0 F manager NaN M engineer NaN F scientist 171.0

3.2.6 Rows selection (Ô¨Åltering)

simple logical Ô¨Åltering

users[users.age < 20] young_bool = users.age < 20 young = users[young_bool] users[users.age < 20].job print(young)

# only show users with age < 20 # or, create a Series of booleans...
# ...and use that Series to filter rows # select one column from the filtered results

Out:

name age gender

job height

0 alice 19

F student 165.0

Advanced logical Ô¨Åltering

users[users.age < 20][[ age , job ]]

# select multiple columns

users[(users.age > 20) & (users.gender == M )] # use multiple conditions

users[users.job.isin([ student , engineer ])] # filter specific values

3.2. Pandas: data manipulation

45

Statistics and Machine Learning in Python, Release 0.2

3.2.7 Sorting

df = users.copy()

df.age.sort_values()

# only works for a Series

df.sort_values(by= age )

# sort rows by a specific column

df.sort_values(by= age , ascending=False) # use descending order instead

df.sort_values(by=[ job , age ])

# sort by multiple columns

df.sort_values(by=[ job , age ], inplace=True) # modify df

print(df)

Out:

name age gender 4 peter 33 3 paul 58 5 julie 44 0 alice 19 2 eric 22 1 john 26

job height M engineer NaN F manager NaN F scientist 171.0 F student 165.0 M student 175.0 M student 180.0

3.2.8 Descriptive statistics

Summarize all numeric columns print(df.describe())

Out:

age count mean std min 25% 50% 75% max

height 6.000000 33.666667 14.895189 19.000000 23.000000 29.500000 41.250000 58.000000

4.000000 172.750000
6.344289 165.000000 169.500000 173.000000 176.250000 180.000000

Summarize all columns
print(df.describe(include= all )) print(df.describe(include=[ object ])) # limit to one (or more) types

Out:

name count unique top freq mean std

age gender

6 6.000000

6

NaN

john

NaN

1

NaN

NaN 33.666667

NaN 14.895189

job

height

6

6 4.000000

2

4

NaN

F student

NaN

3

3

NaN

NaN

NaN 172.750000

NaN

NaN 6.344289

(continues on next page)

46

Chapter 3. ScientiÔ¨Åc Python

Statistics and Machine Learning in Python, Release 0.2

min 25% 50% 75% max
count unique top freq

NaN 19.000000 NaN

NaN 23.000000 NaN

NaN 29.500000 NaN

NaN 41.250000 NaN

NaN 58.000000 NaN

name gender

job

6

6

6

6

2

4

john

F student

1

3

3

NaN 165.000000 NaN 169.500000 NaN 173.000000 NaN 176.250000 NaN 180.000000

(continued from previous page)

Statistics per group (groupby) print(df.groupby("job").mean()) print(df.groupby("job")["age"].mean()) print(df.groupby("job").describe(include= all ))

Out:

age

height

job

engineer 33.000000

NaN

manager 58.000000

NaN

scientist 44.000000 171.000000

student 22.333333 173.333333

job

engineer 33.000000

manager

58.000000

scientist 44.000000

student

22.333333

Name: age, dtype: float64

name

age

‚ê£

Àì‚Üí ... gender

height

count unique top freq mean std min 25% 50% 75% max count unique top‚ê£

Àì‚Üífreq ... 25% 50% 75% max count unique top freq

mean

std min ‚ê£

Àì‚Üí25% 50% 75% max

job

‚ê£

Àì‚Üí ...

engineer

1

1 peter 1 NaN NaN NaN NaN NaN NaN NaN 1.0 NaN NaN ‚ê£

Àì‚ÜíNaN ... NaN NaN NaN NaN 0.0 NaN NaN NaN

NaN

NaN NaN ‚ê£

Àì‚ÜíNaN NaN NaN NaN

manager

1

1 paul 1 NaN NaN NaN NaN NaN NaN NaN 1.0 NaN NaN ‚ê£

Àì‚ÜíNaN ... NaN NaN NaN NaN 0.0 NaN NaN NaN

NaN

NaN NaN ‚ê£

Àì‚ÜíNaN NaN NaN NaN

scientist 1

1 julie 1 NaN NaN NaN NaN NaN NaN NaN 1.0 NaN NaN ‚ê£

Àì‚ÜíNaN ... NaN NaN NaN NaN 1.0 NaN NaN NaN 171.000000

NaN 171.0 171.

Àì‚Üí0 171.0 171.0 171.0

student

3

3 john 1 NaN NaN NaN NaN NaN NaN NaN 3.0 NaN NaN ‚ê£

Àì‚ÜíNaN ... NaN NaN NaN NaN 3.0 NaN NaN NaN 173.333333 7.637626 165.0 170.

Àì‚Üí0 175.0 177.5 180.0

[4 rows x 44 columns]

Groupby in a loop

3.2. Pandas: data manipulation

47

Statistics and Machine Learning in Python, Release 0.2

for grp, data in df.groupby("job"): print(grp, data)

Out:

engineer name age gender

job height

4 peter 33

M engineer NaN

manager name age gender

job height

3 paul 58

F manager NaN

scientist name age gender

job height

5 julie 44

F scientist 171.0

student name age gender

job height

0 alice 19

F student 165.0

2 eric 22

M student 175.0

1 john 26

M student 180.0

3.2.9 Quality check

Remove duplicate data

df = users.append(df.iloc[0], ignore_index=True)

print(df.duplicated())

# Series of booleans

# (True if a row is identical to a previous row)

df.duplicated().sum()

# count of duplicates

df[df.duplicated()]

# only show duplicates

df.age.duplicated()

# check a single column for duplicates

df.duplicated([ age , gender ]).sum() # specify columns for finding duplicates

df = df.drop_duplicates()

# drop duplicate rows

Out:
0 False 1 False 2 False 3 False 4 False 5 False 6 True dtype: bool

Missing data

# Missing values are often just excluded df = users.copy()

df.describe(include= all )

# excludes missing values

# find missing values in a Series

df.height.isnull()

# True if NaN, False otherwise

df.height.notnull()

# False if NaN, True otherwise

df[df.height.notnull()]

# only show rows where age is not NaN

(continues on next page)

48

Chapter 3. ScientiÔ¨Åc Python

Statistics and Machine Learning in Python, Release 0.2

df.height.isnull().sum() # count the missing values

# find missing values in a DataFrame

df.isnull()

# DataFrame of booleans

df.isnull().sum()

# calculate the sum of each column

(continued from previous page)

Strategy 1: drop missing values

df.dropna() df.dropna(how= all )

# drop a row if ANY values are missing # drop a row only if ALL values are missing

Strategy 2: Ô¨Åll in missing values
df.height.mean() df = users.copy() df.ix[df.height.isnull(), "height"] = df["height"].mean()
print(df)

Out:

name age gender 0 alice 19 1 john 26 2 eric 22 3 paul 58 4 peter 33 5 julie 44

job height F student 165.00 M student 180.00 M student 175.00 F manager 172.75 M engineer 172.75 F scientist 171.00

3.2.10 Rename values
df = users.copy() print(df.columns) df.columns = [ age , genre , travail , nom , taille ]
df.travail = df.travail.map({ student : etudiant , manager : manager , engineer : ingenieur , scientist : scientific })
# assert df.travail.isnull().sum() == 0

df[ travail ].str.contains("etu|inge") Out: Index([ name , age , gender , job , height ], dtype= object )

3.2.11 Dealing with outliers
size = pd.Series(np.random.normal(loc=175, size=20, scale=10)) # Corrupt the first 3 measures size[:3] += 500

3.2. Pandas: data manipulation

49

Statistics and Machine Learning in Python, Release 0.2
Based on parametric statistics: use the mean
Assume random variable follows the normal distribution Exclude data outside 3 standarddeviations: - Probability that a sample lies within 1 sd: 68.27% - Probability that a sample lies within 3 sd: 99.73% (68.27 + 2 * 15.73) size_outlr_mean = size.copy() size_outlr_mean[((size - size.mean()).abs() > 3 * size.std())] = size.mean() print(size_outlr_mean.mean())
Out: 248.48963819938044
Based on non-parametric statistics: use the median
Median absolute deviation (MAD), based on the median, is a robust non-parametric statistics. https://en.wikipedia.org/wiki/Median_absolute_deviation mad = 1.4826 * np.median(np.abs(size - size.median())) size_outlr_mad = size.copy() size_outlr_mad[((size - size.median()).abs() > 3 * mad)] = size.median() print(size_outlr_mad.mean(), size_outlr_mad.median())
Out: 173.80000467192673 178.7023568870694
3.2.12 File I/O
csv
import tempfile, os.path tmpdir = tempfile.gettempdir() csv_filename = os.path.join(tmpdir, "users.csv") users.to_csv(csv_filename, index=False) other = pd.read_csv(csv_filename)
Read csv from url
url = https://raw.github.com/neurospin/pystatsml/master/datasets/salary_table.csv salary = pd.read_csv(url)
Excel

50

Chapter 3. ScientiÔ¨Åc Python

Statistics and Machine Learning in Python, Release 0.2

xls_filename = os.path.join(tmpdir, "users.xlsx") users.to_excel(xls_filename, sheet_name= users , index=False)
pd.read_excel(xls_filename, sheetname= users )
# Multiple sheets with pd.ExcelWriter(xls_filename) as writer:
users.to_excel(writer, sheet_name= users , index=False) df.to_excel(writer, sheet_name= salary , index=False)
pd.read_excel(xls_filename, sheetname= users ) pd.read_excel(xls_filename, sheetname= salary )

SQL (SQLite)

import pandas as pd import sqlite3
db_filename = os.path.join(tmpdir, "users.db")

Connect conn = sqlite3.connect(db_filename)

Creating tables with pandas
url = https://raw.github.com/neurospin/pystatsml/master/datasets/salary_table.csv salary = pd.read_csv(url)
salary.to_sql("salary", conn, if_exists="replace")

Push modiÔ¨Åcations
cur = conn.cursor() values = (100, 14000, 5, Bachelor , N ) cur.execute("insert into salary values (?, ?, ?, ?, ?)", values) conn.commit()

Reading results into a pandas DataFrame
salary_sql = pd.read_sql_query("select * from salary;", conn) print(salary_sql.head())
pd.read_sql_query("select * from salary;", conn).tail() pd.read_sql_query( select * from salary where salary>25000; , conn) pd.read_sql_query( select * from salary where experience=16; , conn) pd.read_sql_query( select * from salary where education="Master"; , conn)

Out:

index salary experience education management

0

0 13876

1 Bachelor

Y

1

1 11608

1

Ph.D

N

2

2 18701

1

Ph.D

Y

(continues on next page)

3.2. Pandas: data manipulation

51

Statistics and Machine Learning in Python, Release 0.2

3

3 11283

4

4 11767

1 Master

N

1

Ph.D

N

(continued from previous page)

3.2.13 Exercises

Data Frame

1. Read the iris dataset at ‚Äòhttps://github.com/neurospin/pystatsml/tree/master/datasets/ iris.csv‚Äô

2. Print column names

3. Get numerical columns
4. For each species compute the mean of numerical columns and store it in a stats table like:

species sepal_length sepal_width petal_length petal_width

0

setosa

5.006

3.428

1.462

0.246

1 versicolor

5.936

2.770

4.260

1.326

2 virginica

6.588

2.974

5.552

2.026

Missing data
Add some missing data to the previous table users:
df = users.copy() df.ix[[0, 2], "age"] = None df.ix[[1, 3], "gender"] = None
1. Write a function fillmissing_with_mean(df) that Ô¨Åll all missing value of numerical column with the mean of the current columns.
2. Save the original users and ‚Äúimputed‚Äù frame in a single excel Ô¨Åle ‚Äúusers.xlsx‚Äù with 2 sheets: original, imputed.
Total running time of the script: ( 0 minutes 1.826 seconds)

3.3 Matplotlib: data visualization
Sources - Nicolas P. Rougier: http://www.labri.fr/perso/nrougier/teaching/matplotlib - https: //www.kaggle.com/benhamner/d/uciml/iris/python-data-visualizations

3.3.1 Basic plots
import numpy as np import matplotlib.pyplot as plt # inline plot (for jupyter)
52

(continues on next page)
Chapter 3. ScientiÔ¨Åc Python

%matplotlib inline
x = np.linspace(0, 10, 50) sinus = np.sin(x)
plt.plot(x, sinus) plt.show()

Statistics and Machine Learning in Python, Release 0.2
(continued from previous page)

plt.plot(x, sinus, "o") plt.show() # use plt.plot to get color / marker abbreviations

3.3. Matplotlib: data visualization

53

Statistics and Machine Learning in Python, Release 0.2
# Rapid multiplot
cosinus = np.cos(x) plt.plot(x, sinus, "-b", x, sinus, "ob", x, cosinus, "-r", x, cosinus, "or") plt.xlabel( this is x! ) plt.ylabel( this is y! ) plt.title( My First Plot ) plt.show()

# Step by step plt.plot(x, sinus, label= sinus , color= blue , linestyle= -- , linewidth=2) plt.plot(x, cosinus, label= cosinus , color= red , linestyle= - , linewidth=2) plt.legend() plt.show()

54

Chapter 3. ScientiÔ¨Åc Python

Statistics and Machine Learning in Python, Release 0.2

3.3.2 Scatter (2D) plots
Load dataset
import pandas as pd try:
salary = pd.read_csv("../datasets/salary_table.csv") except:
url = https://raw.github.com/neurospin/pystatsml/master/datasets/salary_table.csv salary = pd.read_csv(url)
df = salary
Simple scatter with colors
colors = colors_edu = { Bachelor : r , Master : g , Ph.D : blue } plt.scatter(df[ experience ], df[ salary ], c=df[ education ].apply(lambda x: colors[x]),‚ê£ Àì‚Üís=100)
<matplotlib.collections.PathCollection at 0x7fa627ddb438>

3.3. Matplotlib: data visualization

55

Statistics and Machine Learning in Python, Release 0.2

Scatter plot with colors and symbols
## Figure size plt.figure(figsize=(6,5))
## Define colors / sumbols manually symbols_manag = dict(Y= * , N= . ) colors_edu = { Bachelor : r , Master : g , Ph.D : blue }
## group by education x management => 6 groups for values, d in salary.groupby([ education , management ]):
edu, manager = values plt.scatter(d[ experience ], d[ salary ], marker=symbols_manag[manager], color=colors_ Àì‚Üíedu[edu],
s=150, label=manager+"/"+edu)
## Set labels plt.xlabel( Experience ) plt.ylabel( Salary ) plt.legend(loc=4) # lower right plt.show()

56

Chapter 3. ScientiÔ¨Åc Python

Statistics and Machine Learning in Python, Release 0.2

3.3.3 Saving Figures
### bitmap format plt.plot(x, sinus) plt.savefig("sinus.png") plt.close()
# Prefer vectorial format (SVG: Scalable Vector Graphics) can be edited with # Inkscape, Adobe Illustrator, Blender, etc. plt.plot(x, sinus) plt.savefig("sinus.svg") plt.close()
# Or pdf plt.plot(x, sinus) plt.savefig("sinus.pdf") plt.close()
3.3.4 Seaborn
Sources: - http://stanford.edu/~mwaskom/software/seaborn - https://elitedatascience.com/ python-seaborn-tutorial
If needed, install using: pip install -U --user seaborn

3.3. Matplotlib: data visualization

57

Statistics and Machine Learning in Python, Release 0.2
Boxplot Box plots are non-parametric: they display variation in samples of a statistical population without making any assumptions of the underlying statistical distribution.

Fig. 2: title import seaborn as sns sns.boxplot(x="education", y="salary", hue="management", data=salary) <matplotlib.axes._subplots.AxesSubplot at 0x7fa62372cb70>

58

Chapter 3. ScientiÔ¨Åc Python

Statistics and Machine Learning in Python, Release 0.2

sns.boxplot(x="management", y="salary", hue="education", data=salary) sns.stripplot(x="management", y="salary", hue="education", data=salary, jitter=True,‚ê£ Àì‚Üídodge=True, linewidth=1)# Jitter and split options separate datapoints according to‚ê£ Àì‚Üígroup"
<matplotlib.axes._subplots.AxesSubplot at 0x7fa623680780>

### Density plot with one Ô¨Ågure containing multiple axis One Ô¨Ågure can contain several axis, whose contain the graphic elements

3.3. Matplotlib: data visualization

59

Statistics and Machine Learning in Python, Release 0.2
# Set up the matplotlib figure: 3 x 1 axis
f, axes = plt.subplots(3, 1, figsize=(9, 9), sharex=True)
i=0 for edu, d in salary.groupby([ education ]):
sns.distplot(d.salary[d.management == "Y"], color="b", bins=10, label="Manager",‚ê£ Àì‚Üíax=axes[i])
sns.distplot(d.salary[d.management == "N"], color="r", bins=10, label="Employee",‚ê£ Àì‚Üíax=axes[i])
axes[i].set_title(edu) axes[i].set_ylabel( Density ) i += 1 ax = plt.legend()

60

Chapter 3. ScientiÔ¨Åc Python

Statistics and Machine Learning in Python, Release 0.2 Violin plot (distribution) ax = sns.violinplot(x="salary", data=salary)

Tune bandwidth ax = sns.violinplot(x="salary", data=salary, bw=.15)

ax = sns.violinplot(x="management", y="salary", hue="education", data=salary)

3.3. Matplotlib: data visualization

61

Statistics and Machine Learning in Python, Release 0.2

Tips dataset One waiter recorded information about each tip he received over a period of a few months working in one restaurant. He collected several variables:
import seaborn as sns #sns.set(style="whitegrid") tips = sns.load_dataset("tips") print(tips.head())
ax = sns.violinplot(x=tips["total_bill"])

total_bill tip sex smoker day time size

0

16.99 1.01 Female No Sun Dinner 2

1

10.34 1.66 Male No Sun Dinner 3

2

21.01 3.50 Male No Sun Dinner 3

3

23.68 3.31 Male No Sun Dinner 2

4

24.59 3.61 Female No Sun Dinner 4

62

Chapter 3. ScientiÔ¨Åc Python

Statistics and Machine Learning in Python, Release 0.2

Group by day ax = sns.violinplot(x="day", y="total_bill", data=tips, palette="muted")

Group by day and color by time (lunch vs dinner)
ax = sns.violinplot(x="day", y="total_bill", hue="time", data=tips, palette="muted",‚ê£ Àì‚Üísplit=True)

3.3. Matplotlib: data visualization

63

Statistics and Machine Learning in Python, Release 0.2
Pairwise scatter plots g = sns.PairGrid(salary, hue="management") g.map_diag(plt.hist) g.map_offdiag(plt.scatter) ax = g.add_legend()

64

Chapter 3. ScientiÔ¨Åc Python

Statistics and Machine Learning in Python, Release 0.2

3.3.5 Time series
import seaborn as sns sns.set(style="darkgrid")
# Load an example dataset with long-form data fmri = sns.load_dataset("fmri")
# Plot the responses for different events and regions

ax = sns.pointplot(x="timepoint", y="signal",

hue="region", style="event",

data=fmri)

# version 0.9

# sns.lineplot(x="timepoint", y="signal",

#

hue="region", style="event",

#

data=fmri)

3.3. Matplotlib: data visualization

65

Statistics and Machine Learning in Python, Release 0.2

66

Chapter 3. ScientiÔ¨Åc Python

CHAPTER
FOUR
STATISTICS

4.1 Univariate statistics
Basics univariate statistics are required to explore dataset: ‚Ä¢ Discover associations between a variable of interest and potential predictors. It is strongly recommended to start with simple univariate methods before moving to complex multivariate predictors. ‚Ä¢ Assess the prediction performances of machine learning predictors. ‚Ä¢ Most of the univariate statistics are based on the linear model which is one of the main model in machine learning.
4.1.1 Estimators of the main statistical measures
Mean
Properties of the expected value operator E(¬∑) of a random variable Ì†µÌ±ã

Ì†µÌ∞∏(Ì†µÌ±ã + Ì†µÌ±ê) = Ì†µÌ∞∏(Ì†µÌ±ã) + Ì†µÌ±ê Ì†µÌ∞∏(Ì†µÌ±ã + Ì†µÌ±å ) = Ì†µÌ∞∏(Ì†µÌ±ã) + Ì†µÌ∞∏(Ì†µÌ±å )
Ì†µÌ∞∏(Ì†µÌ±éÌ†µÌ±ã) = Ì†µÌ±éÌ†µÌ∞∏(Ì†µÌ±ã)

The estimator Ì†µ¬ØÌ±• on a sample of size Ì†µÌ±õ: Ì†µÌ±• = Ì†µÌ±•1, ..., Ì†µÌ±•Ì†µÌ±õ is given by

1 ‚àëÔ∏Å

Ì†µ¬ØÌ±• = Ì†µÌ±õ

Ì†µÌ±•Ì†µÌ±ñ

Ì†µÌ±ñ

Ì†µ¬ØÌ±• is itself a random variable with properties:

‚Ä¢ Ì†µÌ∞∏(Ì†µ¬ØÌ±•) = Ì†µ¬ØÌ±•,

‚Ä¢

Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µ¬ØÌ±•) =

Ì†µÌ±â

Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã Ì†µÌ±õ

)

.

Variance

Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) = Ì†µÌ∞∏((Ì†µÌ±ã ‚àí Ì†µÌ∞∏(Ì†µÌ±ã))2) = Ì†µÌ∞∏(Ì†µÌ±ã2) ‚àí (Ì†µÌ∞∏(Ì†µÌ±ã))2

(4.1) (4.2) (4.3)

67

Statistics and Machine Learning in Python, Release 0.2

The estimator is

Ì†µÌºéÌ†µ2Ì±•

=

Ì†µÌ±õ

1 ‚àí

1

‚àëÔ∏Å (Ì†µÌ±•Ì†µÌ±ñ

‚àí

Ì†µ¬ØÌ±•)2

Ì†µÌ±ñ

Note here the subtracted 1 degree of freedom (df) in the divisor. In standard statistical practice, Ì†µÌ±ëÌ†µÌ±ì = 1 provides an unbiased estimator of the variance of a hypothetical inÔ¨Ånite population. With Ì†µÌ±ëÌ†µÌ±ì = 0 it instead provides a maximum likelihood estimate of the variance for normally distributed variables.

Standard deviation
‚àöÔ∏Ä Ì†µÌ±ÜÌ†µÌ±°Ì†µÌ±ë(Ì†µÌ±ã) = Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) The estimator is simply Ì†µÌºéÌ†µÌ±• = ‚àöÔ∏ÄÌ†µÌºéÌ†µ2Ì±•.

Covariance

Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã, Ì†µÌ±å ) = Ì†µÌ∞∏((Ì†µÌ±ã ‚àí Ì†µÌ∞∏(Ì†µÌ±ã))(Ì†µÌ±å ‚àí Ì†µÌ∞∏(Ì†µÌ±å ))) = Ì†µÌ∞∏(Ì†µÌ±ãÌ†µÌ±å ) ‚àí Ì†µÌ∞∏(Ì†µÌ±ã)Ì†µÌ∞∏(Ì†µÌ±å ).

Properties:

Cov(Ì†µÌ±ã, Ì†µÌ±ã) = Var(Ì†µÌ±ã) Cov(Ì†µÌ±ã, Ì†µÌ±å ) = Cov(Ì†µÌ±å, Ì†µÌ±ã) Cov(Ì†µÌ±êÌ†µÌ±ã, Ì†µÌ±å ) = Ì†µÌ±ê Cov(Ì†µÌ±ã, Ì†µÌ±å ) Cov(Ì†µÌ±ã + Ì†µÌ±ê, Ì†µÌ±å ) = Cov(Ì†µÌ±ã, Ì†µÌ±å )

The estimator with Ì†µÌ±ëÌ†µÌ±ì = 1 is

1 ‚àëÔ∏Å Ì†µÌºéÌ†µÌ±•Ì†µÌ±¶ = Ì†µÌ±õ ‚àí 1 (Ì†µÌ±•Ì†µÌ±ñ ‚àí Ì†µ¬ØÌ±•)(Ì†µÌ±¶Ì†µÌ±ñ ‚àí Ì†µ¬ØÌ±¶).
Ì†µÌ±ñ

Correlation The estimator is

Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã, Ì†µÌ±å ) Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±ü(Ì†µÌ±ã, Ì†µÌ±å ) =
Ì†µÌ±ÜÌ†µÌ±°Ì†µÌ±ë(Ì†µÌ±ã)Ì†µÌ±ÜÌ†µÌ±°Ì†µÌ±ë(Ì†µÌ±å )

Ì†µÌºåÌ†µÌ±•Ì†µÌ±¶

=

Ì†µÌºéÌ†µÌ±•Ì†µÌ±¶ . Ì†µÌºéÌ†µÌ±•Ì†µÌºéÌ†µÌ±¶

Standard Error (SE)
The standard error (SE) is the standard deviation (of the sampling distribution) of a statistic: Ì†µÌ±Ü Ì†µÌ±°Ì†µÌ±ë(Ì†µÌ±ã )
Ì†µÌ±ÜÌ†µÌ∞∏(Ì†µÌ±ã) = ‚àö . Ì†µÌ±õ
It is most commonly considered for the mean with the estimator $

Ì†µÌ±ÜÌ†µÌ∞∏(Ì†µ¬ØÌ±•) = Ì†µÌºéÌ†µ¬ØÌ±• = ‚àöÌ†µÌºéÌ†µÌ±• . Ì†µÌ±õ

(4.4) (4.5)

68

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2
Exercises
‚Ä¢ Generate 2 random samples: Ì†µÌ±• ‚àº Ì†µÌ±Å (1.78, 0.1) and Ì†µÌ±¶ ‚àº Ì†µÌ±Å (1.66, 0.1), both of size 10. ‚Ä¢ Compute Ì†µ¬ØÌ±•, Ì†µÌºéÌ†µÌ±•, Ì†µÌºéÌ†µÌ±•Ì†µÌ±¶ (xbar, xvar, xycov) using only the np.sum() operation. Explore
the np. module to Ô¨Ånd out which numpy functions performs the same computations and compare them (using assert) with your previous results.
4.1.2 Main distributions
Normal distribution
The normal distribution, noted Ì†µÌ≤© (Ì†µÌºá, Ì†µÌºé) with parameters: Ì†µÌºá mean (location) and Ì†µÌºé > 0 std-dev. Estimators: Ì†µ¬ØÌ±• and Ì†µÌºéÌ†µÌ±•. The normal distribution, noted Ì†µÌ±öÌ†µÌ±éÌ†µÌ±°‚ÑéÌ†µÌ±êÌ†µÌ±éÌ†µÌ±ôÌ†µÌ±Å , is useful because of the central limit theorem (CLT) which states that: given certain conditions, the arithmetic mean of a sufÔ¨Åciently large number of iterates of independent random variables, each with a well-deÔ¨Åned expected value and well-deÔ¨Åned variance, will be approximately normally distributed, regardless of the underlying distribution.
import numpy as np import matplotlib.pyplot as plt from scipy.stats import norm %matplotlib inline
mu = 0 # mean variance = 2 #variance sigma = np.sqrt(variance) #standard deviation\n", x = np.linspace(mu-3*variance,mu+3*variance, 100) plt.plot(x, norm.pdf(x, mu, sigma))
[<matplotlib.lines.Line2D at 0x7f6f2a4dae48>]

4.1. Univariate statistics

69

Statistics and Machine Learning in Python, Release 0.2

The Chi-Square distribution

The chi-square or Ì†µÌºí2Ì†µÌ±õ distribution with Ì†µÌ±õ degrees of freedom (df) is the distribution of a sum of the squares of Ì†µÌ±õ independent standard normal random variables Ì†µÌ≤© (0, 1). Let Ì†µÌ±ã ‚àº Ì†µÌ≤© (Ì†µÌºá, Ì†µÌºé2), then, Ì†µÌ±ç = (Ì†µÌ±ã ‚àí Ì†µÌºá)/Ì†µÌºé ‚àº Ì†µÌ≤© (0, 1), then:

‚Ä¢ The squared standard Ì†µÌ±ç2 ‚àº Ì†µÌºí21 (one df).

‚Ä¢

The distribution of sum of squares of Ì†µÌ±õ normal random variables:

‚àëÔ∏ÄÌ†µÌ±õ
Ì†µÌ±ñ

Ì†µÌ±çÌ†µ2Ì±ñ

‚àº Ì†µÌºí2Ì†µÌ±õ

The sum of two Ì†µÌºí2 RV with Ì†µÌ±ù and Ì†µÌ±û df is a Ì†µÌºí2 RV with Ì†µÌ±ù + Ì†µÌ±û df. This is useful when sum-

ming/subtracting sum of squares.

The Ì†µÌºí2-distribution is used to model errors measured as sum of squares or the distribution of the sample variance.

The Fisher‚Äôs F-distribution
The Ì†µÌ∞π -distribution, Ì†µÌ∞πÌ†µÌ±õ,Ì†µÌ±ù, with Ì†µÌ±õ and Ì†µÌ±ù degrees of freedom is the ratio of two independent Ì†µÌºí2 variables. Let Ì†µÌ±ã ‚àº Ì†µÌºí2Ì†µÌ±õ and Ì†µÌ±å ‚àº Ì†µÌºí2Ì†µÌ±ù then:
Ì†µÌ±ã/Ì†µÌ±õ Ì†µÌ∞πÌ†µÌ±õ,Ì†µÌ±ù = Ì†µÌ±å /Ì†µÌ±ù
The Ì†µÌ∞π -distribution plays a central role in hypothesis testing answering the question: Are two variances equals?, is the ratio or two errors signiÔ¨Åcantly large ?.
import numpy as np from scipy.stats import f import matplotlib.pyplot as plt %matplotlib inline
fvalues = np.linspace(.1, 5, 100)
# pdf(x, df1, df2): Probability density function at x of F. plt.plot(fvalues, f.pdf(fvalues, 1, 30), b- , label="F(1, 30)") plt.plot(fvalues, f.pdf(fvalues, 5, 30), r- , label="F(5, 30)") plt.legend()
# cdf(x, df1, df2): Cumulative distribution function of F. # ie. proba_at_f_inf_3 = f.cdf(3, 1, 30) # P(F(1,30) < 3)
# ppf(q, df1, df2): Percent point function (inverse of cdf) at q of F. f_at_proba_inf_95 = f.ppf(.95, 1, 30) # q such P(F(1,30) < .95) assert f.cdf(f_at_proba_inf_95, 1, 30) == .95
# sf(x, df1, df2): Survival function (1 - cdf) at x of F. proba_at_f_sup_3 = f.sf(3, 1, 30) # P(F(1,30) > 3) assert proba_at_f_inf_3 + proba_at_f_sup_3 == 1
# p-value: P(F(1, 30)) < 0.05 low_proba_fvalues = fvalues[fvalues > f_at_proba_inf_95] plt.fill_between(low_proba_fvalues, 0, f.pdf(low_proba_fvalues, 1, 30),
alpha=.8, label="P < 0.05") plt.show()

70

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

The Student‚Äôs Ì†µÌ±°-distribution
Let Ì†µÌ±Ä ‚àº Ì†µÌ≤© (0, 1) and Ì†µÌ±â ‚àº Ì†µÌºí2Ì†µÌ±õ. The Ì†µÌ±°-distribution, Ì†µÌ±áÌ†µÌ±õ, with Ì†µÌ±õ degrees of freedom is the ratio: Ì†µÌ±Ä
Ì†µÌ±áÌ†µÌ±õ = ‚àöÔ∏ÄÌ†µÌ±â /Ì†µÌ±õ
The distribution of the difference between an estimated parameter and its true (or assumed) value divided by the standard deviation of the estimated parameter (standard error) follow a Ì†µÌ±°-distribution. Is this parameters different from a given value?

4.1.3 Hypothesis Testing

Examples ‚Ä¢ Test a proportion: Biased coin ? 200 heads have been found over 300 Ô¨Çips, is it coins biased ? ‚Ä¢ Test the association between two variables. ‚Äì Exemple height and sex: In a sample of 25 individuals (15 females, 10 males), is female height is different from male height ? ‚Äì Exemple age and arterial hypertension: In a sample of 25 individuals is age height correlated with arterial hypertension ?
Steps 1. Model the data 2. Fit: estimate the model parameters (frequency, mean, correlation, regression coeÔ¨Åcient) 3. Compute a test statistic from model the parameters. 4. Formulate the null hypothesis: What would be the (distribution of the) test statistic if the observations are the result of pure chance.

4.1. Univariate statistics

71

Statistics and Machine Learning in Python, Release 0.2

5. Compute the probability (Ì†µÌ±ù-value) to obtain a larger value for the test statistic by chance (under the null hypothesis).

Flip coin: SimpliÔ¨Åed example
Biased coin ? 2 heads have been found over 3 Ô¨Çips, is it coins biased ? 1. Model the data: number of heads follow a Binomial disctribution. 2. Compute model parameters: N=3, P = the frequency of number of heads over the number of Ô¨Çip: 2/3. 3. Compute a test statistic, same as frequency. 4. Under the null hypothesis the distribution of the number of tail is:

1 2 3 count #heads

0

H

1

H

1

H1

HH

2

H

H2

HH2

HHH3

8 possibles conÔ¨Ågurations, probabilities of differents values for Ì†µÌ±ù are: Ì†µÌ±• measure the number of success.
‚Ä¢ Ì†µÌ±É (Ì†µÌ±• = 0) = 1/8
‚Ä¢ Ì†µÌ±É (Ì†µÌ±• = 1) = 3/8
‚Ä¢ Ì†µÌ±É (Ì†µÌ±• = 2) = 3/8
‚Ä¢ Ì†µÌ±É (Ì†µÌ±• = 3) = 1/8
plt.bar([0, 1, 2, 3], [1/8, 3/8, 3/8, 1/8], width=0.9) _ = plt.xticks([0, 1, 2, 3], [0, 1, 2, 3]) plt.xlabel("Distribution of the number of head over 3 flip under the null hypothesis")

Text(0.5, 0, Distribution of the number of head over 3 flip under the null hypothesis )

72

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

3. Compute the probability (Ì†µÌ±ù-value) to observe a value larger or equal that 2 under the null hypothesis ? This probability is the Ì†µÌ±ù-value:
Ì†µÌ±É (Ì†µÌ±• ‚â• 2|Ì†µÌ∞ª0) = Ì†µÌ±É (Ì†µÌ±• = 2) + Ì†µÌ±É (Ì†µÌ±• = 3) = 3/8 + 1/8 = 4/8 = 1/2

Flip coin: Real Example

Biased coin ? 60 heads have been found over 100 Ô¨Çips, is it coins biased ?

1. Model the data: number of heads follow a Binomial disctribution.

2. Compute model parameters: N=100, P=60/100.

3. Compute a test statistic, same as frequency.

4. Compute a test statistic: 60/100.

5. Under the null hypothesis the distribution of the number of tail (Ì†µÌ±ò) follow the binomial distribution of parameters N=100, P=0.5:

Ì†µÌ±É Ì†µÌ±ü(Ì†µÌ±ã

=

Ì†µÌ±ò|Ì†µÌ∞ª0)

=

Ì†µÌ±É Ì†µÌ±ü(Ì†µÌ±ã

=

Ì†µÌ±ò|Ì†µÌ±õ

=

100,

Ì†µÌ±ù

=

0.5)

=

(Ô∏Ç100)Ô∏Ç0.5Ì†µÌ±ò(1 Ì†µÌ±ò

‚àí

0.5)(100‚àíÌ†µÌ±ò).

Ì†µÌ±É (Ì†µÌ±ã

=

Ì†µÌ±ò

‚â•

60|Ì†µÌ∞ª0)

=

100
‚àëÔ∏Å

(Ô∏Ç100)Ô∏Ç0.5Ì†µÌ±ò(1 ‚àí 0.5)(100‚àíÌ†µÌ±ò) Ì†µÌ±ò

Ì†µÌ±ò=60

=

1

‚àí

60
‚àëÔ∏Å

(Ô∏Ç100)Ô∏Ç0.5Ì†µÌ±ò(1

‚àí

0.5)(100‚àíÌ†µÌ±ò),

the

cumulative

distribution

function.

Ì†µÌ±ò

Ì†µÌ±ò=1

Use tabulated binomial distribution

4.1. Univariate statistics

73

Statistics and Machine Learning in Python, Release 0.2
import scipy.stats import matplotlib.pyplot as plt
#tobs = 2.39687663116 # assume the t-value succes = np.linspace(30, 70, 41) plt.plot(succes, scipy.stats.binom.pmf(succes, 100, 0.5), b- , label="Binomial(100, 0.5) Àì‚Üí") upper_succes_tvalues = succes[succes > 60] plt.fill_between(upper_succes_tvalues, 0, scipy.stats.binom.pmf(upper_succes_tvalues, 100, Àì‚Üí 0.5), alpha=.8, label="p-value") _ = plt.legend()
pval = 1 - scipy.stats.binom.cdf(60, 100, 0.5) print(pval)
0.01760010010885238

Random sampling of the Binomial distribution under the null hypothesis
sccess_h0 = scipy.stats.binom.rvs(100, 0.5, size=10000, random_state=4)
#sccess_h0 = np.array([) for i in range(5000)]) import seaborn as sns _ = sns.distplot(sccess_h0, hist=False)
pval_rnd = np.sum(sccess_h0 >= 60) / (len(sccess_h0) + 1) print("P-value using monte-carlo sampling of the Binomial distribution under H0=", pval_ Àì‚Üírnd)
P-value using monte-carlo sampling of the Binomial distribution under H0= 0. Àì‚Üí025897410258974102

74

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

One sample Ì†µÌ±°-test
The one-sample Ì†µÌ±°-test is used to determine whether a sample comes from a population with a speciÔ¨Åc mean. For example you want to test if the average height of a population is 1.75 Ì†µÌ±ö. 1 Model the data Assume that height is normally distributed: Ì†µÌ±ã ‚àº Ì†µÌ≤© (Ì†µÌºá, Ì†µÌºé), ie:

heightÌ†µÌ±ñ = average height over the population + errorÌ†µÌ±ñ Ì†µÌ±•Ì†µÌ±ñ = Ì†µ¬ØÌ±• + Ì†µÌºÄÌ†µÌ±ñ

(4.6) (4.7)

The Ì†µÌºÄÌ†µÌ±ñ are called the residuals
2 Fit: estimate the model parameters
Ì†µ¬ØÌ±•, Ì†µÌ±†Ì†µÌ±• are the estimators of Ì†µÌºá, Ì†µÌºé.
3 Compute a test statistic
In testing the null hypothesis that the population mean is equal to a speciÔ¨Åed value Ì†µÌºá0 = 1.75, one uses the statistic:
Ì†µÌ±° = Ì†µ¬ØÌ±• ‚àí‚àöÌ†µÌºá0 Ì†µÌ±†Ì†µÌ±•/ Ì†µÌ±õ
Remarks: Although the parent population does not need to be normally distributed, the distribution of the population of sample means, Ì†µÌ±•, is assumed to be normal. By the central limit theorem, if the sampling of the parent population is independent then the sample means will be approximately normal.
4 Compute the probability of the test statistic under the null hypotheis. This require to have the distribution of the t statistic under Ì†µÌ∞ª0.

4.1. Univariate statistics

75

Statistics and Machine Learning in Python, Release 0.2
Example
Given the following samples, we will test whether its true mean is 1.75. Warning, when computing the std or the variance, set ddof=1. The default value, ddof=0, leads to the biased estimator of the variance.
import numpy as np
x= [ 1.83, 1.83, 1.73, 1.82, 1.83, 1.73, 1.99, 1.85, 1.68, 1.87]
xbar = np.mean(x) # sample mean mu0 = 1.75 # hypothesized value s = np.std(x, ddof=1) # sample standard deviation n = len(x) # sample size
tobs = (xbar - mu0) / (s / np.sqrt(n)) print(tobs)
2.3968766311585883
The :math:‚Äòp‚Äò-value is the probability to observe a value Ì†µÌ±° more extreme than the observed one Ì†µÌ±°Ì†µÌ±úÌ†µÌ±èÌ†µÌ±† under the null hypothesis Ì†µÌ∞ª0: Ì†µÌ±É (Ì†µÌ±° > Ì†µÌ±°Ì†µÌ±úÌ†µÌ±èÌ†µÌ±†|Ì†µÌ∞ª0)
import scipy.stats as stats import matplotlib.pyplot as plt
#tobs = 2.39687663116 # assume the t-value tvalues = np.linspace(-10, 10, 100) plt.plot(tvalues, stats.t.pdf(tvalues, n-1), b- , label="T(n-1)") upper_tval_tvalues = tvalues[tvalues > tobs] plt.fill_between(upper_tval_tvalues, 0, stats.t.pdf(upper_tval_tvalues, n-1), alpha=.8,‚ê£ Àì‚Üílabel="p-value") _ = plt.legend()

76

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2
4.1.4 Testing pairwise associations
Univariate statistical analysis: explore association betweens pairs of variables. ‚Ä¢ In statistics, a categorical variable or factor is a variable that can take on one of a limited, and usually Ô¨Åxed, number of possible values, thus assigning each individual to a particular group or ‚Äúcategory‚Äù. The levels are the possibles values of the variable. Number of levels = 2: binomial; Number of levels > 2: multinomial. There is no intrinsic ordering to the categories. For example, gender is a categorical variable having two categories (male and female) and there is no intrinsic ordering to the categories. For example, Sex (Female, Male), Hair color (blonde, brown, etc.). ‚Ä¢ An ordinal variable is a categorical variable with a clear ordering of the levels. For example: drinks per day (none, small, medium and high). ‚Ä¢ A continuous or quantitative variable Ì†µÌ±• ‚àà R is one that can take any value in a range of possible values, possibly inÔ¨Ånite. E.g.: salary, experience in years, weight.
What statistical test should I use? See: http://www.ats.ucla.edu/stat/mult_pkg/whatstat/

Fig. 1: Statistical tests ### Pearson correlation test: test association between two quantitative variables

4.1. Univariate statistics

77

Statistics and Machine Learning in Python, Release 0.2

Test the correlation coefÔ¨Åcient of two quantitative variables. The test calculates a Pearson correlation coefÔ¨Åcient and the Ì†µÌ±ù-value for testing non-correlation.

Let Ì†µÌ±• and Ì†µÌ±¶ two quantitative variables, where Ì†µÌ±õ samples were obeserved. The linear correlation coeÔ¨Åcient is deÔ¨Åned as :

Ì†µÌ±ü

=

‚àëÔ∏ÄÌ†µÌ†µÌ±ñÌ±õ=1(Ì†µÌ±•Ì†µÌ±ñ ‚àí Ì†µ¬ØÌ±•)(Ì†µÌ±¶Ì†µÌ±ñ ‚àí Ì†µ¬ØÌ±¶)

‚àöÔ∏Ä‚àëÔ∏ÄÌ†µÌ±õ
Ì†µÌ±ñ=1

(Ì†µÌ±•Ì†µÌ±ñ

‚àí

Ì†µ¬ØÌ±•)2‚àöÔ∏Ä‚àëÔ∏ÄÌ†µÌ†µÌ±ñÌ±õ=1(Ì†µÌ±¶Ì†µÌ±ñ

. ‚àí Ì†µ¬ØÌ±¶)2

Under

Ì†µÌ∞ª0,

the

test

statistic

Ì†µÌ±°

=

‚àö Ì†µÌ±õ

‚àí

2

‚àö Ì†µÌ±ü 1‚àíÌ†µÌ±ü2

follow

Student

distribution

with

Ì†µÌ±õ ‚àí 2

degrees

of

freedom.

import numpy as np
import scipy.stats as stats n = 50 x = np.random.normal(size=n) y = 2 * x + np.random.normal(size=n)

# Compute with scipy cor, pval = stats.pearsonr(x, y)

Two sample (Student) Ì†µÌ±°-test: compare two means

Fig. 2: Two-sample model
The two-sample Ì†µÌ±°-test (Snedecor and Cochran, 1989) is used to determine if two population means are equal. There are several variations on this test. If data are paired (e.g. 2 measures, before and after treatment for each individual) use the one-sample Ì†µÌ±°-test of the difference. The variances of the two samples may be assumed to be equal (a.k.a. homoscedasticity) or unequal (a.k.a. heteroscedasticity).

1. Model the data
Assume that the two random variables are normally distributed: Ì†µÌ±¶1 ‚àº Ì†µÌ≤© (Ì†µÌºá1, Ì†µÌºé1), Ì†µÌ±¶2 ‚àº Ì†µÌ≤© (Ì†µÌºá2, Ì†µÌºé2).

2. Fit: estimate the model parameters Estimate means and variances: Ì†µ¬ØÌ±¶1, Ì†µÌ±†2Ì†µÌ±¶1, Ì†µ¬ØÌ±¶2, Ì†µÌ±†2Ì†µÌ±¶2.

78

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

3. Ì†µÌ±°-test The general principle is

difference of means Ì†µÌ±° =
its standard error = Ì†µ¬ØÌ±¶1 ‚àí Ì†µ¬ØÌ±¶2
Ì†µÌ±†Ì†µ¬ØÌ±¶1‚àíÌ†µ¬ØÌ±¶2
Since Ì†µÌ±¶1 and Ì†µÌ±¶2 are independant:

(4.8) (4.9)

Ì†µÌ±†2Ì†µ¬ØÌ±¶1‚àíÌ†µ¬ØÌ±¶2

=

Ì†µÌ±†2Ì†µ¬ØÌ±¶1

+ Ì†µÌ±†2Ì†µ¬ØÌ±¶2

=

Ì†µÌ±†2Ì†µÌ±¶1 Ì†µÌ±õ1

+

Ì†µÌ±†2Ì†µÌ±¶2 Ì†µÌ±õ2

thus

‚àöÔ∏É

Ì†µÌ±†Ì†µ¬ØÌ±¶1‚àíÌ†µ¬ØÌ±¶2 =

Ì†µÌ±†2Ì†µÌ±¶1 + Ì†µÌ±†2Ì†µÌ±¶2 Ì†µÌ±õ1 Ì†µÌ±õ2

(4.10) (4.11) (4.12)

Equal or unequal sample sizes, unequal variances (Welch‚Äôs Ì†µÌ±°-test)

Welch‚Äôs Ì†µÌ±°-test deÔ¨Ånes the Ì†µÌ±° statistic as

Ì†µÌ±° = Ì†µ¬ØÌ±¶1 ‚àí Ì†µ¬ØÌ±¶2 .

‚àöÔ∏Å Ì†µÌ±†2Ì†µÌ±¶1
Ì†µÌ±õ1

+

Ì†µÌ±†2Ì†µÌ±¶2 Ì†µÌ±õ2

To compute the Ì†µÌ±ù-value one needs the degrees of freedom associated with this variance estimate. It is approximated using the Welch‚ÄìSatterthwaite equation:

(Ô∏Ç Ì†µÌ±†2Ì†µÌ±¶1
Ì†µÌ±õ1

+

Ì†µÌ±†2Ì†µÌ±¶2 )Ô∏Ç2
Ì†µÌ±õ2

Ì†µÌºà

‚âà

Ì†µÌ±†4Ì†µÌ±¶1 Ì†µÌ±õ21(Ì†µÌ±õ1‚àí1)

+

Ì†µÌ±†4Ì†µÌ±¶2 Ì†µÌ±õ22(Ì†µÌ±õ2‚àí1)

.

Equal or unequal sample sizes, equal variances

If we assume equal variance (ie, Ì†µÌ±†2Ì†µÌ±¶1 = Ì†µÌ±†2Ì†µÌ±¶1 = Ì†µÌ±†2), where Ì†µÌ±†2 is an estimator of the common variance of the two samples:

Ì†µÌ±†2 = Ì†µÌ±†2Ì†µÌ±¶1 (Ì†µÌ±õ1 ‚àí 1) + Ì†µÌ±†2Ì†µÌ±¶2 (Ì†µÌ±õ2 ‚àí 1) Ì†µÌ±õ1 + Ì†µÌ±õ2 ‚àí 2

=

‚àëÔ∏ÄÌ†µÌ±õ1
Ì†µÌ±ñ

(Ì†µÌ±¶1Ì†µÌ±ñ

‚àí

Ì†µ¬ØÌ±¶1)2

+

‚àëÔ∏ÄÌ†µÌ±õ2
Ì†µÌ±ó

(Ì†µÌ±¶2Ì†µÌ±ó

‚àí

Ì†µ¬ØÌ±¶2)2

(Ì†µÌ±õ1 ‚àí 1) + (Ì†µÌ±õ2 ‚àí 1)

then

‚àöÔ∏É

Ì†µÌ±†2 Ì†µÌ±†2 ‚àöÔ∏Ç 1 1

Ì†µÌ±†Ì†µ¬ØÌ±¶1‚àíÌ†µ¬ØÌ±¶2 =

+ = Ì†µÌ±† +

Ì†µÌ±õ1 Ì†µÌ±õ2

Ì†µÌ±õ1 Ì†µÌ±õ2

4.1. Univariate statistics

(4.13) (4.14)
79

Statistics and Machine Learning in Python, Release 0.2

Therefore, the Ì†µÌ±° statistic, that is used to test whether the means are different is:

Ì†µÌ±° =

Ì†µ¬ØÌ±¶1 ‚àí Ì†µ¬ØÌ±¶2 ‚àöÔ∏Å

,

Ì†µÌ±† ¬∑

1 Ì†µÌ±õ1

+

1 Ì†µÌ±õ2

Equal sample sizes, equal variances

If we simplify the problem assuming equal samples of size Ì†µÌ±õ1 = Ì†µÌ±õ2 = Ì†µÌ±õ we get

Ì†µÌ±°

=

Ì†µ¬ØÌ±¶1

‚àí ‚àö

Ì†µ¬ØÌ±¶2

¬∑

‚àö Ì†µÌ±õ

Ì†µÌ±† 2

‚àö

‚âà effect size ¬∑ Ì†µÌ±õ

difference of means

‚àö

‚âà

¬∑ Ì†µÌ±õ

standard deviation of the noise

(4.15) (4.16) (4.17)

Example

Given the following two samples, test whether their means are equal using the standard t-test, assuming equal variance.
import scipy.stats as stats
height = np.array([ 1.83, 1.83, 1.73, 1.82, 1.83, 1.73, 1.99, 1.85, 1.68, 1.87, 1.66, 1.71, 1.73, 1.64, 1.70, 1.60, 1.79, 1.73, 1.62, 1.77])
grp = np.array(["M"] * 10 + ["F"] * 10)
# Compute with scipy print(stats.ttest_ind(height[grp == "M"], height[grp == "F"], equal_var=True))
Ttest_indResult(statistic=3.5511519888466885, pvalue=0.00228208937112721)

ANOVA Ì†µÌ∞π -test (quantitative ~ categorial (>2 levels))
Analysis of variance (ANOVA) provides a statistical test of whether or not the means of several groups are equal, and therefore generalizes the Ì†µÌ±°-test to more than two groups. ANOVAs are useful for comparing (testing) three or more means (groups or variables) for statistical signiÔ¨Åcance. It is conceptually similar to multiple two-sample Ì†µÌ±°-tests, but is less conservative.
Here we will consider one-way ANOVA with one independent variable, ie one-way anova.
Wikipedia:
‚Ä¢ Test if any group is on average superior, or inferior, to the others versus the null hypothesis that all four strategies yield the same mean response
‚Ä¢ Detect any of several possible differences.
‚Ä¢ The advantage of the ANOVA Ì†µÌ∞π -test is that we do not need to pre-specify which strategies are to be compared, and we do not need to adjust for making multiple comparisons.

80

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

‚Ä¢ The disadvantage of the ANOVA Ì†µÌ∞π -test is that if we reject the null hypothesis, we do not know which strategies can be said to be signiÔ¨Åcantly different from the others.
1. Model the data
A company has applied three marketing strategies to three samples of customers in order increase their business volume. The marketing is asking whether the strategies led to different increases of business volume. Let Ì†µÌ±¶1, Ì†µÌ±¶2 and Ì†µÌ±¶3 be the three samples of business volume increase. Here we assume that the three populations were sampled from three random variables that are normally distributed. I.e., Ì†µÌ±å1 ‚àº Ì†µÌ±Å (Ì†µÌºá1, Ì†µÌºé1), Ì†µÌ±å2 ‚àº Ì†µÌ±Å (Ì†µÌºá2, Ì†µÌºé2) and Ì†µÌ±å3 ‚àº Ì†µÌ±Å (Ì†µÌºá3, Ì†µÌºé3).
2. Fit: estimate the model parameters
Estimate means and variances: Ì†µ¬ØÌ±¶Ì†µÌ±ñ, Ì†µÌºéÌ†µÌ±ñ, ‚àÄÌ†µÌ±ñ ‚àà {1, 2, 3}.
3. Ì†µÌ∞π -test
The formula for the one-way ANOVA F-test statistic is

Explained variance Ì†µÌ∞π =
Unexplained variance

=

Between-group variability Within-group variability

=

Ì†µÌ±†2Ì†µÌ∞µ Ì†µÌ±†2Ì†µÌ±ä

.

(4.18) (4.19)

The ‚Äúexplained variance‚Äù, or ‚Äúbetween-group variability‚Äù is

Ì†µÌ±†2Ì†µÌ∞µ = ‚àëÔ∏Å Ì†µÌ±õÌ†µÌ±ñ(Ì†µ¬ØÌ±¶Ì†µÌ±ñ¬∑ ‚àí Ì†µ¬ØÌ±¶)2/(Ì†µÌ∞æ ‚àí 1),
Ì†µÌ±ñ

where Ì†µ¬ØÌ±¶Ì†µÌ±ñ¬∑ denotes the sample mean in the Ì†µÌ±ñth group, Ì†µÌ±õÌ†µÌ±ñ is the number of observations in the Ì†µÌ±ñth group, Ì†µ¬ØÌ±¶ denotes the overall mean of the data, and Ì†µÌ∞æ denotes the number of groups.

The ‚Äúunexplained variance‚Äù, or ‚Äúwithin-group variability‚Äù is

Ì†µÌ±†2Ì†µÌ±ä

=

‚àëÔ∏Å (Ì†µÌ±¶Ì†µÌ±ñÌ†µÌ±ó

‚àí

Ì†µ¬ØÌ±¶Ì†µÌ±ñ¬∑)2/(Ì†µÌ±Å

‚àí Ì†µÌ∞æ),

Ì†µÌ±ñÌ†µÌ±ó

where Ì†µÌ±¶Ì†µÌ±ñÌ†µÌ±ó is the Ì†µÌ±óth observation in the Ì†µÌ±ñth out of Ì†µÌ∞æ groups and Ì†µÌ±Å is the overall sample size. This Ì†µÌ∞π -statistic follows the Ì†µÌ∞π -distribution with Ì†µÌ∞æ ‚àí 1 and Ì†µÌ±Å ‚àí Ì†µÌ∞æ degrees of freedom under the null hypothesis. The statistic will be large if the between-group variability is large relative to the within-group variability, which is unlikely to happen if the population means of the groups all have the same value.
Note that when there are only two groups for the one-way ANOVA F-test, Ì†µÌ∞π = Ì†µÌ±°2 where Ì†µÌ±° is the Student‚Äôs Ì†µÌ±° statistic.

4.1. Univariate statistics

81

Statistics and Machine Learning in Python, Release 0.2

Chi-square, Ì†µÌºí2 (categorial ~ categorial)
Computes the chi-square, Ì†µÌºí2, statistic and Ì†µÌ±ù-value for the hypothesis test of independence of frequencies in the observed contingency table (cross-table). The observed frequencies are tested against an expected contingency table obtained by computing expected frequencies based on the marginal sums under the assumption of independence.
Example: 20 participants: 10 exposed to some chemical product and 10 non exposed (exposed = 1 or 0). Among the 20 participants 10 had cancer 10 not (cancer = 1 or 0). Ì†µÌºí2 tests the association between those two variables.
import numpy as np import pandas as pd import scipy.stats as stats
# Dataset: # 15 samples: # 10 first exposed exposed = np.array([1] * 10 + [0] * 10) # 8 first with cancer, 10 without, the last two with. cancer = np.array([1] * 8 + [0] * 10 + [1] * 2)
crosstab = pd.crosstab(exposed, cancer, rownames=[ exposed ], colnames=[ cancer ])
print("Observed table:") print("---------------") print(crosstab)
chi2, pval, dof, expected = stats.chi2_contingency(crosstab) print("Statistics:") print("-----------") print("Chi2 = %f, pval = %f" % (chi2, pval)) print("Expected table:") print("---------------") print(expected)

Observed table:

---------------

cancer 0 1

exposed

0

82

1

28

Statistics:

-----------

Chi2 = 5.000000, pval = 0.025347

Expected table:

---------------

[[5. 5.]

[5. 5.]]

Computing expected cross-table
# Compute expected cross-table based on proportion exposed_marg = crosstab.sum(axis=0) exposed_freq = exposed_marg / exposed_marg.sum()

(continues on next page)

82

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

cancer_marg = crosstab.sum(axis=1) cancer_freq = cancer_marg / cancer_marg.sum()
print( Exposed frequency? Yes: %.2f % exposed_freq[0], No: %.2f % exposed_freq[1])
print( Cancer frequency? Yes: %.2f % cancer_freq[0], No: %.2f % cancer_freq[1])
print( Expected frequencies: ) print(np.outer(exposed_freq, cancer_freq))
print( Expected cross-table (frequencies * N): ) print(np.outer(exposed_freq, cancer_freq) * len(exposed))
Exposed frequency? Yes: 0.50 No: 0.50 Cancer frequency? Yes: 0.50 No: 0.50 Expected frequencies: [[0.25 0.25] [0.25 0.25]] Expected cross-table (frequencies * N): [[5. 5.] [5. 5.]]

(continued from previous page)

4.1.5 Non-parametric test of pairwise associations
### Spearman rank-order correlation (quantitative ~ quantitative)
The Spearman correlation is a non-parametric measure of the monotonicity of the relationship between two datasets.
When to use it? Observe the data distribution: - presence of outliers - the distribution of the residuals is not Gaussian.
Like other correlation coefÔ¨Åcients, this one varies between -1 and +1 with 0 implying no correlation. Correlations of -1 or +1 imply an exact monotonic relationship. Positive correlations imply that as Ì†µÌ±• increases, so does Ì†µÌ±¶. Negative correlations imply that as Ì†µÌ±• increases, Ì†µÌ±¶ decreases.
import numpy as np import scipy.stats as stats import matplotlib.pyplot as plt
x = np.array([44.4, 45.9, 41.9, 53.3, 44.7, 44.1, 50.7, 45.2, 46, 47, 48, 60.1]) y = np.array([2.6, 3.1, 2.5, 5.0, 3.6, 4.0, 5.2, 2.8, 4, 4.1, 4.5, 3.8])
plt.plot(x, y, "bo")
# Non-Parametric Spearman cor, pval = stats.spearmanr(x, y) print("Non-Parametric Spearman cor test, cor: %.4f, pval: %.4f" % (cor, pval))
# "Parametric Pearson cor test cor, pval = stats.pearsonr(x, y) print("Parametric Pearson cor test: cor: %.4f, pval: %.4f" % (cor, pval))

4.1. Univariate statistics

83

Statistics and Machine Learning in Python, Release 0.2
Non-Parametric Spearman cor test, cor: 0.7110, pval: 0.0095 Parametric Pearson cor test: cor: 0.5263, pval: 0.0788

Wilcoxon signed-rank test (quantitative ~ cte)

Source: https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test
The Wilcoxon signed-rank test is a non-parametric statistical hypothesis test used when comparing two related samples, matched samples, or repeated measurements on a single sample to assess whether their population mean ranks differ (i.e. it is a paired difference test). It is equivalent to one-sample test of the difference of paired samples.
It can be used as an alternative to the paired Student‚Äôs Ì†µÌ±°-test, Ì†µÌ±°-test for matched pairs, or the Ì†µÌ±°test for dependent samples when the population cannot be assumed to be normally distributed.
When to use it? Observe the data distribution: - presence of outliers - the distribution of the residuals is not Gaussian
It has a lower sensitivity compared to Ì†µÌ±°-test. May be problematic to use when the sample size is small.
Null hypothesis Ì†µÌ∞ª0: difference between the pairs follows a symmetric distribution around zero.
import scipy.stats as stats n = 20 # Buisness Volume time 0 bv0 = np.random.normal(loc=3, scale=.1, size=n) # Buisness Volume time 1 bv1 = bv0 + 0.1 + np.random.normal(loc=0, scale=.1, size=n)

# create an outlier bv1[0] -= 10

# Paired t-test

(continues on next page)

84

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

print(stats.ttest_rel(bv0, bv1))
# Wilcoxon print(stats.wilcoxon(bv0, bv1))

(continued from previous page)

Ttest_relResult(statistic=0.7821450892478711, pvalue=0.4437681541620575) WilcoxonResult(statistic=35.0, pvalue=0.008967599455194583)

Mann‚ÄìWhitney Ì†µÌ±à test (quantitative ~ categorial (2 levels))
In statistics, the Mann‚ÄìWhitney Ì†µÌ±à test (also called the Mann‚ÄìWhitney‚ÄìWilcoxon, Wilcoxon rank-sum test or Wilcoxon‚ÄìMann‚ÄìWhitney test) is a nonparametric test of the null hypothesis that two samples come from the same population against an alternative hypothesis, especially that a particular population tends to have larger values than the other.
It can be applied on unknown distributions contrary to e.g. a Ì†µÌ±°-test that has to be applied only on normal distributions, and it is nearly as efÔ¨Åcient as the Ì†µÌ±°-test on normal distributions.
import scipy.stats as stats n = 20 # Buismess Volume group 0 bv0 = np.random.normal(loc=1, scale=.1, size=n)
# Buismess Volume group 1 bv1 = np.random.normal(loc=1.2, scale=.1, size=n)
# create an outlier bv1[0] -= 10
# Two-samples t-test print(stats.ttest_ind(bv0, bv1))
# Wilcoxon print(stats.mannwhitneyu(bv0, bv1))
Ttest_indResult(statistic=0.6725314683035514, pvalue=0.505314623871812) MannwhitneyuResult(statistic=67.0, pvalue=0.0001690974050146689)

4.1.6 Linear model

Given Ì†µÌ±õ random samples (Ì†µÌ±¶Ì†µÌ±ñ, Ì†µÌ±•1Ì†µÌ±ñ, . . . , Ì†µÌ±•Ì†µÌ±ùÌ†µÌ±ñ), Ì†µÌ±ñ = 1, . . . , Ì†µÌ±õ, the linear regression models the relation between the observations Ì†µÌ±¶Ì†µÌ±ñ and the independent variables Ì†µÌ±•Ì†µÌ†µÌ±ñÌ±ù is formulated as
Ì†µÌ±¶Ì†µÌ±ñ = Ì†µÌªΩ0 + Ì†µÌªΩ1Ì†µÌ±•1Ì†µÌ±ñ + ¬∑ ¬∑ ¬∑ + Ì†µÌªΩÌ†µÌ±ùÌ†µÌ±•Ì†µÌ±ùÌ†µÌ±ñ + Ì†µÌºÄÌ†µÌ±ñ Ì†µÌ±ñ = 1, . . . , Ì†µÌ±õ
‚Ä¢ The Ì†µÌªΩ‚Äôs are the model parameters, ie, the regression coeÔ¨Åcients. ‚Ä¢ Ì†µÌªΩ0 is the intercept or the bias. ‚Ä¢ Ì†µÌºÄÌ†µÌ±ñ are the residuals. ‚Ä¢ An independent variable (IV). It is a variable that stands alone and isn‚Äôt changed by
the other variables you are trying to measure. For example, someone‚Äôs age might be an

4.1. Univariate statistics

85

Statistics and Machine Learning in Python, Release 0.2

Fig. 3: Linear model
independent variable. Other factors (such as what they eat, how much they go to school, how much television they watch) aren‚Äôt going to change a person‚Äôs age. In fact, when you are looking for some kind of relationship between variables you are trying to see if the independent variable causes some kind of change in the other variables, or dependent variables. In Machine Learning, these variables are also called the predictors.
‚Ä¢ A dependent variable. It is something that depends on other factors. For example, a test score could be a dependent variable because it could change depending on several factors such as how much you studied, how much sleep you got the night before you took the test, or even how hungry you were when you took it. Usually when you are looking for a relationship between two things you are trying to Ô¨Ånd out what makes the dependent variable change the way it does. In Machine Learning this variable is called a target variable.
Simple regression: test association between two quantitative variables
Using the dataset ‚Äúsalary‚Äù, explore the association between the dependant variable (e.g. Salary) and the independent variable (e.g.: Experience is quantitative).
import pandas as pd import matplotlib.pyplot as plt %matplotlib inline
url = https://raw.github.com/neurospin/pystatsml/master/datasets/salary_table.csv salary = pd.read_csv(url)

1. Model the data

Model the data on some hypothesis e.g.: salary is a linear function of the experience.

more generally

salaryÌ†µÌ±ñ = Ì†µÌªΩ experienceÌ†µÌ±ñ + Ì†µÌªΩ0 + Ì†µÌºñÌ†µÌ±ñ,

Ì†µÌ±¶Ì†µÌ±ñ = Ì†µÌªΩ Ì†µÌ±•Ì†µÌ±ñ + Ì†µÌªΩ0 + Ì†µÌºñÌ†µÌ±ñ

‚Ä¢ Ì†µÌªΩ: the slope or coefÔ¨Åcient or parameter of the model, ‚Ä¢ Ì†µÌªΩ0: the intercept or bias is the second parameter of the model,

86

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

‚Ä¢ Ì†µÌºñÌ†µÌ±ñ: is the Ì†µÌ±ñth error, or residual with Ì†µÌºñ ‚àº Ì†µÌ≤© (0, Ì†µÌºé2). The simple regression is equivalent to the Pearson correlation.

2. Fit: estimate the model parameters

The goal it so estimate Ì†µÌªΩ, Ì†µÌªΩ0 and Ì†µÌºé2.

Minimizes the mean squared error (MSE) or the Sum squared error (SSE). The so-called

Ordinary

Least

Squares

(OLS)

Ô¨Ånds

Ì†µÌªΩ,

Ì†µÌªΩ0

that

minimizes

the

Ì†µÌ±ÜÌ†µÌ±ÜÌ†µÌ∞∏

=

‚àëÔ∏Ä
Ì†µÌ±ñ

Ì†µÌºñ2Ì†µÌ±ñ

Ì†µÌ±ÜÌ†µÌ±ÜÌ†µÌ∞∏

=

‚àëÔ∏Å (Ì†µÌ±¶Ì†µÌ±ñ

‚àí

Ì†µÌªΩ

Ì†µÌ±•Ì†µÌ±ñ

‚àí

Ì†µÌªΩ0)2

Ì†µÌ±ñ

Recall from calculus that an extreme point can be found by computing where the derivative is zero, i.e. to Ô¨Ånd the intercept, we perform the steps:

Ì†µÌºïÌ†µÌ±ÜÌ†µÌ±ÜÌ†µÌ∞∏ ‚àëÔ∏Å

Ì†µÌºïÌ†µÌªΩ0

=

(Ì†µÌ±¶Ì†µÌ±ñ ‚àí Ì†µÌªΩ Ì†µÌ±•Ì†µÌ±ñ ‚àí Ì†µÌªΩ0) = 0
Ì†µÌ±ñ

‚àëÔ∏Å

‚àëÔ∏Å

Ì†µÌ±¶Ì†µÌ±ñ = Ì†µÌªΩ Ì†µÌ±•Ì†µÌ±ñ + Ì†µÌ±õ Ì†µÌªΩ0

Ì†µÌ±ñ

Ì†µÌ±ñ

Ì†µÌ±õ Ì†µ¬ØÌ±¶ = Ì†µÌ±õ Ì†µÌªΩ Ì†µ¬ØÌ±• + Ì†µÌ±õ Ì†µÌªΩ0

Ì†µÌªΩ0 = Ì†µ¬ØÌ±¶ ‚àí Ì†µÌªΩ Ì†µ¬ØÌ±•

To Ô¨Ånd the regression coefÔ¨Åcient, we perform the steps:

Ì†µÌºïÌ†µÌ±ÜÌ†µÌ±ÜÌ†µÌ∞∏ ‚àëÔ∏Å

= Ì†µÌºïÌ†µÌªΩ

Ì†µÌ±•Ì†µÌ±ñ(Ì†µÌ±¶Ì†µÌ±ñ ‚àí Ì†µÌªΩ Ì†µÌ±•Ì†µÌ±ñ ‚àí Ì†µÌªΩ0) = 0

Ì†µÌ±ñ

Plug in Ì†µÌªΩ0:

‚àëÔ∏Å Ì†µÌ±•Ì†µÌ±ñ(Ì†µÌ±¶Ì†µÌ±ñ ‚àí Ì†µÌªΩ Ì†µÌ±•Ì†µÌ±ñ ‚àí Ì†µ¬ØÌ±¶ + Ì†µÌªΩÌ†µ¬ØÌ±•) = 0

Ì†µÌ±ñ

‚àëÔ∏Å

‚àëÔ∏Å

‚àëÔ∏Å

Ì†µÌ±•Ì†µÌ±ñÌ†µÌ±¶Ì†µÌ±ñ ‚àí Ì†µ¬ØÌ±¶ Ì†µÌ±•Ì†µÌ±ñ = Ì†µÌªΩ (Ì†µÌ±•Ì†µÌ±ñ ‚àí Ì†µ¬ØÌ±•)

Ì†µÌ±ñ

Ì†µÌ±ñ

Ì†µÌ±ñ

Divide both sides by Ì†µÌ±õ:

1 ‚àëÔ∏Å

1 ‚àëÔ∏Å

Ì†µÌ±õ

Ì†µÌ±•Ì†µÌ±ñÌ†µÌ±¶Ì†µÌ±ñ

‚àí

Ì†µ¬ØÌ±¶Ì†µ¬ØÌ±•

=

Ì†µÌªΩ Ì†µÌ±õ

(Ì†µÌ±•Ì†µÌ±ñ ‚àí Ì†µ¬ØÌ±•)

Ì†µÌ±ñ

Ì†µÌ±ñ

Ì†µÌªΩ

=

1 Ì†µÌ±õ

‚àëÔ∏Ä
Ì†µÌ±ñ

Ì†µÌ±•Ì†µÌ±ñÌ†µÌ±¶Ì†µÌ±ñ

‚àí

Ì†µ¬ØÌ±¶Ì†µ¬ØÌ±•

1 Ì†µÌ±õ

‚àëÔ∏ÄÌ†µÌ±ñ(Ì†µÌ±•Ì†µÌ±ñ

‚àí

Ì†µ¬ØÌ±•)

=

Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±•, Ì†µÌ±¶) .
Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±•)

from scipy import stats
import numpy as np y, x = salary.salary, salary.experience beta, beta0, r_value, p_value, std_err = stats.linregress(x,y) print("y = %f x + %f, r: %f, r-squared: %f,\np-value: %f, std_err: %f"
% (beta, beta0, r_value, r_value**2, p_value, std_err))

print("Regression line with the scatterplot") yhat = beta * x + beta0 # regression line

(continues on next page)

4.1. Univariate statistics

87

Statistics and Machine Learning in Python, Release 0.2

plt.plot(x, yhat, r- , x, y, o ) plt.xlabel( Experience (years) ) plt.ylabel( Salary ) plt.show()
print("Using seaborn") import seaborn as sns sns.regplot(x="experience", y="salary", data=salary);

(continued from previous page)

y = 491.486913 x + 13584.043803, r: 0.538886, r-squared: 0.290398, p-value: 0.000112, std_err: 115.823381 Regression line with the scatterplot

Using seaborn 88

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

3. Ì†µÌ∞π -Test
3.1 Goodness of Ô¨Åt
The goodness of Ô¨Åt of a statistical model describes how well it Ô¨Åts a set of observations. Measures of goodness of Ô¨Åt typically summarize the discrepancy between observed values and the values expected under the model in question. We will consider the explained variance also known as the coefÔ¨Åcient of determination, denoted Ì†µÌ±Ö2 pronounced R-squared. The total sum of squares, Ì†µÌ±ÜÌ†µÌ±Ütot is the sum of the sum of squares explained by the regression, Ì†µÌ±ÜÌ†µÌ±Üreg, plus the sum of squares of residuals unexplained by the regression, Ì†µÌ±ÜÌ†µÌ±Üres, also called the SSE, i.e. such that
Ì†µÌ±ÜÌ†µÌ±Ütot = Ì†µÌ±ÜÌ†µÌ±Üreg + Ì†µÌ±ÜÌ†µÌ±Üres

Fig. 4: title

4.1. Univariate statistics

89

Statistics and Machine Learning in Python, Release 0.2

The mean of Ì†µÌ±¶ is

1 ‚àëÔ∏Å

Ì†µ¬ØÌ±¶ = Ì†µÌ±õ

Ì†µÌ±¶Ì†µÌ±ñ.

Ì†µÌ±ñ

The total sum of squares is the total squared sum of deviations from the mean of Ì†µÌ±¶, i.e.

Ì†µÌ±ÜÌ†µÌ±Ütot

=

‚àëÔ∏Å (Ì†µÌ±¶Ì†µÌ±ñ

‚àí

Ì†µ¬ØÌ±¶)2

Ì†µÌ±ñ

The regression sum of squares, also called the explained sum of squares:

Ì†µÌ±ÜÌ†µÌ±Üreg

=

‚àëÔ∏Å (Ì†µÀÜÌ±¶Ì†µÌ±ñ

‚àí

Ì†µ¬ØÌ±¶)2,

Ì†µÌ±ñ

where Ì†µÀÜÌ±¶Ì†µÌ±ñ = Ì†µÌªΩÌ†µÌ±•Ì†µÌ±ñ + Ì†µÌªΩ0 is the estimated value of salary Ì†µÀÜÌ±¶Ì†µÌ±ñ given a value of experience Ì†µÌ±•Ì†µÌ±ñ. The sum of squares of the residuals, also called the residual sum of squares (RSS) is:

Ì†µÌ±ÜÌ†µÌ±Üres

=

‚àëÔ∏Å (Ì†µÌ±¶Ì†µÌ±ñ

‚àí

Ì†µÀÜÌ±¶Ì†µÌ±ñ)2.

Ì†µÌ±ñ

Ì†µÌ±Ö2 is the explained sum of squares of errors. It is the variance explain by the regression divided by the total variance, i.e.

Ì†µÌ±Ö2 = explained SS = Ì†µÌ±ÜÌ†µÌ±Üreg = 1 ‚àí Ì†µÌ±ÜÌ†µÌ±ÜÌ†µÌ±üÌ†µÌ±íÌ†µÌ±† .

total SS

Ì†µÌ±ÜÌ†µÌ±ÜÌ†µÌ±°Ì†µÌ±úÌ†µÌ±°

Ì†µÌ±ÜÌ†µÌ±ÜÌ†µÌ±°Ì†µÌ±úÌ†µÌ±°

3.2 Test

Let Ì†µÀÜÌºé2 = Ì†µÌ±ÜÌ†µÌ±Üres/(Ì†µÌ±õ ‚àí 2) be an estimator of the variance of Ì†µÌºñ. The 2 in the denominator stems from the 2 estimated parameters: intercept and coefÔ¨Åcient.

‚Ä¢

Unexplained variance:

Ì†µÌ±ÜÌ†µÌ±Üres Ì†µ^Ìºé2

‚àº Ì†µÌºí2Ì†µÌ±õ‚àí2

‚Ä¢

Explained variance:

Ì†µÌ±ÜÌ†µÌ±Üreg Ì†µ^Ìºé2

‚àº Ì†µÌºí21. The single degree of freedom comes from the difference

between

Ì†µÌ±ÜÌ†µÌ±Ütot Ì†µ^Ìºé2

(‚àº

Ì†µÌºí2Ì†µÌ±õ‚àí1)

and

Ì†µÌ±ÜÌ†µÌ±Üres Ì†µ^Ìºé2

(‚àº

Ì†µÌºí2Ì†µÌ±õ‚àí2),

i.e.

(Ì†µÌ±õ

‚àí

1)

‚àí

(Ì†µÌ±õ

‚àí

2)

degree

of

freedom.

The Fisher statistics of the ratio of two variances:

Explained variance

Ì†µÌ∞π =

=

Ì†µÌ±ÜÌ†µÌ±Üreg/1

‚àº Ì†µÌ∞π (1, Ì†µÌ±õ ‚àí 2)

Unexplained variance Ì†µÌ±ÜÌ†µÌ±Üres/(Ì†µÌ±õ ‚àí 2)

Using the Ì†µÌ∞π -distribution, compute the probability of observing a value greater than Ì†µÌ∞π under Ì†µÌ∞ª0, i.e.: Ì†µÌ±É (Ì†µÌ±• > Ì†µÌ∞π |Ì†µÌ∞ª0), i.e. the survival function (1 ‚àí Cumulative Distribution Function) at Ì†µÌ±• of the given Ì†µÌ∞π -distribution.

Multiple regression Theory
Muliple Linear Regression is the most basic supervised learning algorithm. Given: a set of training data {Ì†µÌ±•1, ..., Ì†µÌ±•Ì†µÌ±Å } with corresponding targets {Ì†µÌ±¶1, ..., Ì†µÌ±¶Ì†µÌ±Å }.

90

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

In linear regression, we assume that the model that generates the data involves only a linear combination of the input variables, i.e.

Ì†µÌ±¶(Ì†µÌ±•Ì†µÌ±ñ, Ì†µÌªΩ) = Ì†µÌªΩ0 + Ì†µÌªΩ1Ì†µÌ±•1Ì†µÌ±ñ + ... + Ì†µÌªΩÌ†µÌ±É Ì†µÌ±•Ì†µÌ†µÌ±ñÌ±É ,

or, simpliÔ¨Åed

Ì†µÌ±É ‚àí1
Ì†µÌ±¶(Ì†µÌ±•Ì†µÌ±ñ, Ì†µÌªΩ) = Ì†µÌªΩ0 + ‚àëÔ∏Å Ì†µÌªΩÌ†µÌ±óÌ†µÌ±•Ì†µÌ†µÌ±ñÌ±ó.
Ì†µÌ±ó=1

Extending each sample with an intercept, Ì†µÌ±•Ì†µÌ±ñ := [1, Ì†µÌ±•Ì†µÌ±ñ] ‚àà Ì†µÌ±ÖÌ†µÌ±É +1 allows us to use a more general notation based on linear algebra and write it as a simple dot product:

Ì†µÌ±¶(Ì†µÌ±•Ì†µÌ±ñ, Ì†µÌªΩ) = Ì†µÌ±•Ì†µÌ†µÌ±ñÌ±á Ì†µÌªΩ,
where Ì†µÌªΩ ‚àà Ì†µÌ±ÖÌ†µÌ±É +1 is a vector of weights that deÔ¨Åne the Ì†µÌ±É + 1 parameters of the model. From now we have Ì†µÌ±É regressors + the intercept. Minimize the Mean Squared Error MSE loss:

Ì†µÌ±Ä Ì†µÌ±ÜÌ†µÌ∞∏(Ì†µÌªΩ)

=

1 Ì†µÌ±Å

Ì†µÌ±Å
‚àëÔ∏Å (Ì†µÌ±¶Ì†µÌ±ñ

‚àí

Ì†µÌ±¶(Ì†µÌ±•Ì†µÌ±ñ, Ì†µÌªΩ))2

=

1 Ì†µÌ±Å

Ì†µÌ±Å
‚àëÔ∏Å (Ì†µÌ±¶Ì†µÌ±ñ

‚àí

Ì†µÌ±•Ì†µÌ†µÌ±ñÌ±á Ì†µÌªΩ)2

Ì†µÌ±ñ=1

Ì†µÌ±ñ=1

Let Ì†µÌ±ã = [Ì†µÌ±•Ì†µ0Ì±á , ..., Ì†µÌ±•Ì†µÌ†µÌ±áÌ±Å ] be a Ì†µÌ±Å √ó Ì†µÌ±É + 1 matrix of Ì†µÌ±Å samples of Ì†µÌ±É input features with one column of one and let be Ì†µÌ±¶ = [Ì†µÌ±¶1, ..., Ì†µÌ±¶Ì†µÌ±Å ] be a vector of the Ì†µÌ±Å targets. Then, using linear algebra, the
mean squared error (MSE) loss can be rewritten:

Ì†µÌ±Ä Ì†µÌ±ÜÌ†µÌ∞∏(Ì†µÌªΩ)

=

1 Ì†µÌ±Å

||Ì†µÌ±¶

‚àí

Ì†µÌ±ã Ì†µÌªΩ ||22 .

The Ì†µÌªΩ that minimises the MSE can be found by:

‚àáÌ†µÌªΩ

(Ô∏Ç

1 Ì†µÌ±Å

||Ì†µÌ±¶

‚àí

Ì†µÌ±ã

Ì†µÌªΩ

||22

)Ô∏Ç

=

0

1 Ì†µÌ±Å

‚àáÌ†µÌªΩ (Ì†µÌ±¶

‚àí

Ì†µÌ±ã Ì†µÌªΩ )Ì†µÌ±á

(Ì†µÌ±¶

‚àí

Ì†µÌ±ã Ì†µÌªΩ )

=

0

1 Ì†µÌ±Å

‚àáÌ†µÌªΩ

(Ì†µÌ±¶Ì†µÌ±á

Ì†µÌ±¶

‚àí

2Ì†µÌªΩÌ†µÌ±á

Ì†µÌ±ã

Ì†µÌ±á

Ì†µÌ±¶

+

Ì†µÌªΩÌ†µÌ±ã

Ì†µÌ±á

Ì†µÌ±ã

Ì†µÌªΩ)

=

0

‚àí2Ì†µÌ±ãÌ†µÌ±á Ì†µÌ±¶ + 2Ì†µÌ±ãÌ†µÌ±á Ì†µÌ±ãÌ†µÌªΩ = 0

Ì†µÌ±ãÌ†µÌ±á Ì†µÌ±ãÌ†µÌªΩ = Ì†µÌ±ãÌ†µÌ±á Ì†µÌ±¶

Ì†µÌªΩ = (Ì†µÌ±ãÌ†µÌ±á Ì†µÌ±ã)‚àí1Ì†µÌ±ãÌ†µÌ±á Ì†µÌ±¶,

where (Ì†µÌ±ãÌ†µÌ±á Ì†µÌ±ã)‚àí1Ì†µÌ±ãÌ†µÌ±á is a pseudo inverse of Ì†µÌ±ã.

(4.20)
(4.21)
(4.22) (4.23) (4.24) (4.25)

Fit with numpy

import numpy as np from scipy import linalg np.random.seed(seed=42) # make the example reproducible
4.1. Univariate statistics

(continues on next page)
91

Statistics and Machine Learning in Python, Release 0.2

# Dataset N, P = 50, 4 X = np.random.normal(size= N * P).reshape((N, P)) ## Our model needs an intercept so we add a column of 1s: X[:, 0] = 1 print(X[:5, :])
betastar = np.array([10, 1., .5, 0.1]) e = np.random.normal(size=N) y = np.dot(X, betastar) + e
# Estimate the parameters Xpinv = linalg.pinv2(X) betahat = np.dot(Xpinv, y) print("Estimated beta:\n", betahat)

[[ 1.

-0.1382643 0.64768854 1.52302986]

[ 1.

-0.23413696 1.57921282 0.76743473]

[ 1.

0.54256004 -0.46341769 -0.46572975]

[ 1.

-1.91328024 -1.72491783 -0.56228753]

[ 1.

0.31424733 -0.90802408 -1.4123037 ]]

Estimated beta:

[10.14742501 0.57938106 0.51654653 0.17862194]

(continued from previous page)

4.1.7 Linear model with statsmodels
Sources: http://statsmodels.sourceforge.net/devel/examples/

Multiple regression Interface with Numpy

import statsmodels.api as sm
## Fit and summary: model = sm.OLS(y, X).fit() print(model.summary())
# prediction of new values ypred = model.predict(X)
# residuals + prediction == true values assert np.all(ypred + model.resid == y)

OLS Regression Results

==============================================================================

Dep. Variable:

y R-squared:

0.363

Model:

OLS Adj. R-squared:

0.322

Method:

Least Squares F-statistic:

8.748

Date:

Thu, 16 May 2019 Prob (F-statistic):

0.000106

(continues on next page)

92

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

(continued from previous page)

Time:

20:15:04 Log-Likelihood:

-71.271

No. Observations:

50 AIC:

150.5

Df Residuals:

46 BIC:

158.2

Df Model:

3

Covariance Type:

nonrobust

==============================================================================

coef std err

t

P>|t|

[0.025

0.975]

------------------------------------------------------------------------------

const

10.1474

0.150 67.520

0.000

9.845

10.450

x1

0.5794

0.160

3.623

0.001

0.258

0.901

x2

0.5165

0.151

3.425

0.001

0.213

0.820

x3

0.1786

0.144

1.240

0.221

-0.111

0.469

==============================================================================

Omnibus:

2.493 Durbin-Watson:

2.369

Prob(Omnibus):

0.288 Jarque-Bera (JB):

1.544

Skew:

0.330 Prob(JB):

0.462

Kurtosis:

3.554 Cond. No.

1.27

==============================================================================

Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly‚ê£ Àì‚Üíspecified.

Interface with Pandas

Use R language syntax for data.frame. For an additive model: Ì†µÌ±¶Ì†µÌ±ñ = Ì†µÌªΩ0 + Ì†µÌ±•1Ì†µÌ±ñ Ì†µÌªΩ1 + Ì†µÌ±•2Ì†µÌ±ñ Ì†µÌªΩ2 + Ì†µÌºñÌ†µÌ±ñ ‚â° y ~ x1 + x2.

import statsmodels.formula.api as smfrmla

df = pd.DataFrame(np.column_stack([X, y]), columns=[ inter , print(df.columns, df.shape) # Build a model excluding the intercept, it is implicit model = smfrmla.ols("y~x1 + x2 + x3", df).fit() print(model.summary())

x1 , x2 ,

x3 ,

y ])

Index([ inter , x1 , x2 , x3 , y ], dtype= object ) (50, 5)

OLS Regression Results

==============================================================================

Dep. Variable:

y R-squared:

0.363

Model:

OLS Adj. R-squared:

0.322

Method:

Least Squares F-statistic:

8.748

Date:

Thu, 16 May 2019 Prob (F-statistic):

0.000106

Time:

20:15:04 Log-Likelihood:

-71.271

No. Observations:

50 AIC:

150.5

Df Residuals:

46 BIC:

158.2

Df Model:

3

Covariance Type:

nonrobust

==============================================================================

coef std err

t

P>|t|

[0.025

0.975]

------------------------------------------------------------------------------

Intercept 10.1474

0.150 67.520

0.000

9.845

10.450

x1

0.5794

0.160

3.623

0.001

0.258

0.901

(continues on next page)

4.1. Univariate statistics

93

Statistics and Machine Learning in Python, Release 0.2

(continued from previous page)

x2

0.5165

0.151

3.425

0.001

0.213

0.820

x3

0.1786

0.144

1.240

0.221

-0.111

0.469

==============================================================================

Omnibus:

2.493 Durbin-Watson:

2.369

Prob(Omnibus):

0.288 Jarque-Bera (JB):

1.544

Skew:

0.330 Prob(JB):

0.462

Kurtosis:

3.554 Cond. No.

1.27

==============================================================================

Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly‚ê£ Àì‚Üíspecified.

Multiple regression with categorical independent variables or factors: Analysis of covariance (ANCOVA)
Analysis of covariance (ANCOVA) is a linear model that blends ANOVA and linear regression. ANCOVA evaluates whether population means of a dependent variable (DV) are equal across levels of a categorical independent variable (IV) often called a treatment, while statistically controlling for the effects of other quantitative or continuous variables that are not of primary interest, known as covariates (CV).
import pandas as pd import matplotlib.pyplot as plt %matplotlib inline
try: salary = pd.read_csv("../datasets/salary_table.csv")
except: url = https://raw.github.com/neurospin/pystatsml/master/datasets/salary_table.csv salary = pd.read_csv(url)

One-way AN(C)OVA

‚Ä¢ ANOVA: one categorical independent variable, i.e. one factor.
‚Ä¢ ANCOVA: ANOVA with some covariates.
import statsmodels.formula.api as smfrmla
oneway = smfrmla.ols( salary ~ management + experience , salary).fit() print(oneway.summary()) aov = sm.stats.anova_lm(oneway, typ=2) # Type 2 ANOVA DataFrame print(aov)

OLS Regression Results

==============================================================================

Dep. Variable:

salary R-squared:

0.865

Model:

OLS Adj. R-squared:

0.859

Method:

Least Squares F-statistic:

138.2

Date:

Thu, 16 May 2019 Prob (F-statistic):

1.90e-19

(continues on next page)

94

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

(continued from previous page)

Time:

20:15:04 Log-Likelihood:

-407.76

No. Observations:

46 AIC:

821.5

Df Residuals:

43 BIC:

827.0

Df Model:

2

Covariance Type:

nonrobust

===================================================================================

coef std err

t

P>|t|

[0.025

0.975]

-----------------------------------------------------------------------------------

Intercept

1.021e+04 525.999 19.411

0.000 9149.578 1.13e+04

management[T.Y] 7145.0151 527.320 13.550

0.000 6081.572 8208.458

experience

527.1081 51.106 10.314

0.000 424.042 630.174

==============================================================================

Omnibus:

11.437 Durbin-Watson:

2.193

Prob(Omnibus):

0.003 Jarque-Bera (JB):

11.260

Skew:

-1.131 Prob(JB):

0.00359

Kurtosis:

3.872 Cond. No.

22.4

==============================================================================

Warnings:

[1] Standard Errors assume that the covariance matrix of the errors is correctly‚ê£

Àì‚Üíspecified.

sum_sq df

F

PR(>F)

management 5.755739e+08 1.0 183.593466 4.054116e-17

experience 3.334992e+08 1.0 106.377768 3.349662e-13

Residual 1.348070e+08 43.0

NaN

NaN

Two-way AN(C)OVA

Ancova with two categorical independent variables, i.e. two factors.
import statsmodels.formula.api as smfrmla
twoway = smfrmla.ols( salary ~ education + management + experience , salary).fit() print(twoway.summary()) aov = sm.stats.anova_lm(twoway, typ=2) # Type 2 ANOVA DataFrame print(aov)

OLS Regression Results

==============================================================================

Dep. Variable:

salary R-squared:

0.957

Model:

OLS Adj. R-squared:

0.953

Method:

Least Squares F-statistic:

226.8

Date:

Thu, 16 May 2019 Prob (F-statistic):

2.23e-27

Time:

20:15:04 Log-Likelihood:

-381.63

No. Observations:

46 AIC:

773.3

Df Residuals:

41 BIC:

782.4

Df Model:

4

Covariance Type:

nonrobust

=======================================================================================

coef std err

t

P>|t|

[0.025

0.975]

---------------------------------------------------------------------------------------

Intercept

8035.5976 386.689 20.781

0.000 7254.663 8816.532

education[T.Master] 3144.0352 361.968

8.686

0.000 2413.025 3875.045

(continues on next page)

4.1. Univariate statistics

95

Statistics and Machine Learning in Python, Release 0.2

(continued from previous page)

education[T.Ph.D] 2996.2103 411.753

7.277

0.000 2164.659 3827.762

management[T.Y]

6883.5310 313.919 21.928

0.000 6249.559 7517.503

experience

546.1840 30.519 17.896

0.000 484.549 607.819

==============================================================================

Omnibus:

2.293 Durbin-Watson:

2.237

Prob(Omnibus):

0.318 Jarque-Bera (JB):

1.362

Skew:

-0.077 Prob(JB):

0.506

Kurtosis:

2.171 Cond. No.

33.5

==============================================================================

Warnings:

[1] Standard Errors assume that the covariance matrix of the errors is correctly‚ê£

Àì‚Üíspecified.

sum_sq df

F

PR(>F)

education 9.152624e+07 2.0 43.351589 7.672450e-11

management 5.075724e+08 1.0 480.825394 2.901444e-24

experience 3.380979e+08 1.0 320.281524 5.546313e-21

Residual 4.328072e+07 41.0

NaN

NaN

Comparing two nested models
oneway is nested within twoway. Comparing two nested models tells us if the additional predictors (i.e. education) of the full model signiÔ¨Åcantly decrease the residuals. Such comparison can be done using an Ì†µÌ∞π -test on residuals: print(twoway.compare_f_test(oneway)) # return F, pval, df
(43.35158945918107, 7.672449570495418e-11, 2.0)

Factor coding

See http://statsmodels.sourceforge.net/devel/contrasts.html
By default Pandas use ‚Äúdummy coding‚Äù. Explore:
print(twoway.model.data.param_names) print(twoway.model.data.exog[:10, :])

[ Intercept , education[T.Master] , [[1. 0. 0. 1. 1.] [1. 0. 1. 0. 1.] [1. 0. 1. 1. 1.] [1. 1. 0. 0. 1.] [1. 0. 1. 0. 1.] [1. 1. 0. 1. 2.] [1. 1. 0. 0. 2.] [1. 0. 0. 0. 2.] [1. 0. 1. 0. 2.] [1. 1. 0. 0. 3.]]

education[T.Ph.D] ,

management[T.Y] ,

experience ]

96

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

Contrasts and post-hoc tests

# t-test of the specific contribution of experience: ttest_exp = twoway.t_test([0, 0, 0, 0, 1]) ttest_exp.pvalue, ttest_exp.tvalue print(ttest_exp)
# Alternatively, you can specify the hypothesis tests using a string twoway.t_test( experience )
# Post-hoc is salary of Master different salary of Ph.D? # ie. t-test salary of Master = salary of Ph.D. print(twoway.t_test( education[T.Master] = education[T.Ph.D] ))

Test for Constraints

==============================================================================

coef std err

t

P>|t|

[0.025

0.975]

------------------------------------------------------------------------------

c0

546.1840 30.519 17.896

0.000 484.549 607.819

==============================================================================

Test for Constraints

==============================================================================

coef std err

t

P>|t|

[0.025

0.975]

------------------------------------------------------------------------------

c0

147.8249 387.659

0.381

0.705 -635.069 930.719

==============================================================================

4.1.8 Multiple comparisons

import numpy as np np.random.seed(seed=42) # make example reproducible

# Dataset n_samples, n_features = 100, 1000 n_info = int(n_features/10) # number of features with information n1, n2 = int(n_samples/2), n_samples - int(n_samples/2) snr = .5 Y = np.random.randn(n_samples, n_features) grp = np.array(["g1"] * n1 + ["g2"] * n2)

# Add some group effect for Pinfo features Y[grp=="g1", :n_info] += snr

# import scipy.stats as stats
import matplotlib.pyplot as plt tvals, pvals = np.full(n_features, np.NAN), np.full(n_features, np.NAN) for j in range(n_features):
tvals[j], pvals[j] = stats.ttest_ind(Y[grp=="g1", j], Y[grp=="g2", j], equal_var=True)

fig, axis = plt.subplots(3, 1)#, sharex= col )

axis[0].plot(range(n_features), tvals, o )

(continues on next page)

4.1. Univariate statistics

97

Statistics and Machine Learning in Python, Release 0.2

axis[0].set_ylabel("t-value")

(continued from previous page)

axis[1].plot(range(n_features), pvals, o ) axis[1].axhline(y=0.05, color= red , linewidth=3, label="p-value=0.05") #axis[1].axhline(y=0.05, label="toto", color= red ) axis[1].set_ylabel("p-value") axis[1].legend()

axis[2].hist([pvals[n_info:], pvals[:n_info]], stacked=True, bins=100, label=["Negatives", "Positives"])
axis[2].set_xlabel("p-value histogram") axis[2].set_ylabel("density") axis[2].legend()

plt.tight_layout()

Note that under the null hypothesis the distribution of the p-values is uniform.

Statistical measures:

‚Ä¢ True Positive (TP) equivalent to a hit. The test correctly concludes the presence of an effect.

‚Ä¢ True Negative (TN). The test correctly concludes the absence of an effect.

‚Ä¢ False Positive (FP) equivalent to a false alarm, Type I error. The test improperly concludes the presence of an effect. Thresholding at Ì†µÌ±ù-value < 0.05 leads to 47 FP.

‚Ä¢ False Negative (FN) equivalent to a miss, Type II error. The test improperly concludes the absence of an effect.

P, N = n_info, n_features - n_info # Positives, Negatives TP = np.sum(pvals[:n_info ] < 0.05) # True Positives

(continues on next page)

98

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2
(continued from previous page) FP = np.sum(pvals[n_info: ] < 0.05) # False Positives print("No correction, FP: %i (expected: %.2f), TP: %i" % (FP, N * 0.05, TP))
No correction, FP: 47 (expected: 45.00), TP: 71
Bonferroni correction for multiple comparisons
The Bonferroni correction is based on the idea that if an experimenter is testing Ì†µÌ±É hypotheses, then one way of maintaining the familywise error rate (FWER) is to test each individual hypothesis at a statistical signiÔ¨Åcance level of 1/Ì†µÌ±É times the desired maximum overall level. So, if the desired signiÔ¨Åcance level for the whole family of tests is Ì†µÌªº (usually 0.05), then the Bonferroni correction would test each individual hypothesis at a signiÔ¨Åcance level of Ì†µÌªº/Ì†µÌ±É . For example, if a trial is testing Ì†µÌ±É = 8 hypotheses with a desired Ì†µÌªº = 0.05, then the Bonferroni correction would test each individual hypothesis at Ì†µÌªº = 0.05/8 = 0.00625.
import statsmodels.sandbox.stats.multicomp as multicomp _, pvals_fwer, _, _ = multicomp.multipletests(pvals, alpha=0.05,
method= bonferroni ) TP = np.sum(pvals_fwer[:n_info ] < 0.05) # True Positives FP = np.sum(pvals_fwer[n_info: ] < 0.05) # False Positives print("FWER correction, FP: %i, TP: %i" % (FP, TP))
FWER correction, FP: 0, TP: 6
The False discovery rate (FDR) correction for multiple comparisons
FDR-controlling procedures are designed to control the expected proportion of rejected null hypotheses that were incorrect rejections (‚Äúfalse discoveries‚Äù). FDR-controlling procedures provide less stringent control of Type I errors compared to the familywise error rate (FWER) controlling procedures (such as the Bonferroni correction), which control the probability of at least one Type I error. Thus, FDR-controlling procedures have greater power, at the cost of increased rates of Type I errors.
import statsmodels.sandbox.stats.multicomp as multicomp _, pvals_fdr, _, _ = multicomp.multipletests(pvals, alpha=0.05,
method= fdr_bh ) TP = np.sum(pvals_fdr[:n_info ] < 0.05) # True Positives FP = np.sum(pvals_fdr[n_info: ] < 0.05) # False Positives
print("FDR correction, FP: %i, TP: %i" % (FP, TP))
FDR correction, FP: 3, TP: 20
4.1.9 Exercises

4.1. Univariate statistics

99

Statistics and Machine Learning in Python, Release 0.2
Simple linear regression and correlation (application)
Load the dataset: birthwt Risk Factors Associated with Low Infant Birth Weight at ftp://ftp. cea.fr/pub/unati/people/educhesnay/pystatml/datasets/birthwt.csv
1. Test the association of mother‚Äôs age and birth weight using the correlation test and linear regeression.
2. Test the association of mother‚Äôs weight and birth weight using the correlation test and linear regeression.
3. Produce two scatter plot of: (i) age by birth weight; (ii) mother‚Äôs weight by birth weight. Conclusion ?
Simple linear regression (maths)
Considering the salary and the experience of the salary table. https://raw.github.com/neurospin/pystatsml/master/datasets/salary_table.csv Compute:
‚Ä¢ Estimate the model paramters Ì†µÌªΩ, Ì†µÌªΩ0 using scipy stats.linregress(x,y) ‚Ä¢ Compute the predicted values Ì†µÀÜÌ±¶ Compute: ‚Ä¢ Ì†µ¬ØÌ±¶: y_mu ‚Ä¢ Ì†µÌ±ÜÌ†µÌ±Ütot: ss_tot ‚Ä¢ Ì†µÌ±ÜÌ†µÌ±Üreg: ss_reg ‚Ä¢ Ì†µÌ±ÜÌ†µÌ±Üres: ss_res ‚Ä¢ Check partition of variance formula based on sum of squares by using assert np.
allclose(val1, val2, atol=1e-05) ‚Ä¢ Compute Ì†µÌ±Ö2 and compare it with the r_value above ‚Ä¢ Compute the Ì†µÌ∞π score ‚Ä¢ Compute the Ì†µÌ±ù-value: ‚Ä¢ Plot the Ì†µÌ∞π (1, Ì†µÌ±õ) distribution for 100 Ì†µÌ±ì values within [10, 25]. Draw Ì†µÌ±É (Ì†µÌ∞π (1, Ì†µÌ±õ) > Ì†µÌ∞π ),
i.e. color the surface deÔ¨Åned by the Ì†µÌ±• values larger than Ì†µÌ∞π below the Ì†µÌ∞π (1, Ì†µÌ±õ). ‚Ä¢ Ì†µÌ±É (Ì†µÌ∞π (1, Ì†µÌ±õ) > Ì†µÌ∞π ) is the Ì†µÌ±ù-value, compute it.
Multiple regression
Considering the simulated data used below: 1. What are the dimensions of pinv(Ì†µÌ±ã)? 2. Compute the MSE between the predicted values and the true values.

100

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

import numpy as np from scipy import linalg np.random.seed(seed=42) # make the example reproducible
# Dataset N, P = 50, 4 X = np.random.normal(size= N * P).reshape((N, P)) ## Our model needs an intercept so we add a column of 1s: X[:, 0] = 1 print(X[:5, :])
betastar = np.array([10, 1., .5, 0.1]) e = np.random.normal(size=N) y = np.dot(X, betastar) + e
# Estimate the parameters Xpinv = linalg.pinv2(X) betahat = np.dot(Xpinv, y) print("Estimated beta:\n", betahat)

[[ 1.

-0.1382643 0.64768854 1.52302986]

[ 1.

-0.23413696 1.57921282 0.76743473]

[ 1.

0.54256004 -0.46341769 -0.46572975]

[ 1.

-1.91328024 -1.72491783 -0.56228753]

[ 1.

0.31424733 -0.90802408 -1.4123037 ]]

Estimated beta:

[10.14742501 0.57938106 0.51654653 0.17862194]

Two sample t-test (maths)
Given the following two sample, test whether their means are equals.
height = np.array([ 1.83, 1.83, 1.73, 1.82, 1.83, 1.73,1.99, 1.85, 1.68, 1.87, 1.66, 1.71, 1.73, 1.64, 1.70, 1.60, 1.79, 1.73, 1.62, 1.77])
grp = np.array(["M"] * 10 + ["F"] * 10)
‚Ä¢ Compute the means/std-dev per groups.
‚Ä¢ Compute the Ì†µÌ±°-value (standard two sample t-test with equal variances).
‚Ä¢ Compute the Ì†µÌ±ù-value. ‚Ä¢ The Ì†µÌ±ù-value is one-sided: a two-sided test would test P(T > tval) and P(T < -tval).
What would the two sided Ì†µÌ±ù-value be? ‚Ä¢ Compare the two-sided Ì†µÌ±ù-value with the one obtained by stats.ttest_ind using assert
np.allclose(arr1, arr2).

Two sample t-test (application)
Risk Factors Associated with Low Infant Birth Weight: https://raw.github.com/neurospin/pystatsml/master/datasets/birthwt.csv

4.1. Univariate statistics

101

Statistics and Machine Learning in Python, Release 0.2

1. Explore the data 2. Recode smoke factor 3. Compute the means/std-dev per groups. 4. Plot birth weight by smoking (box plot, violin plot or histogram) 5. Test the effect of smoking on birth weight

Two sample t-test and random permutations
Generate 100 samples following the model:
Ì†µÌ±¶ = Ì†µÌ±î + Ì†µÌºÄ
Where the noise Ì†µÌºÄ ‚àº Ì†µÌ±Å (1, 1) and Ì†µÌ±î ‚àà {0, 1} is a group indicator variable with 50 ones and 50 zeros.
‚Ä¢ Write a function tstat(y, g) that compute the two samples t-test of y splited in two groups deÔ¨Åned by g.
‚Ä¢ Sample the t-statistic distribution under the null hypothesis using random permutations. ‚Ä¢ Assess the p-value.

Univariate associations (developpement)
Write a function univar_stat(df, target, variables) that computes the parametric statistics and Ì†µÌ±ù-values between the target variable (provided as as string) and all variables (provided as a list of string) of the pandas DataFrame df. The target is a quantitative variable but variables may be quantitative or qualitative. The function returns a DataFrame with four columns: variable, test, value, p_value.
Apply it to the salary dataset available at https://raw.github.com/neurospin/pystatsml/master/ datasets/salary_table.csv, with target being S: salaries for IT staff in a corporation.

Multiple comparisons
This exercise has 2 goals: apply you knowledge of statistics using vectorized numpy operations. Given the dataset provided for multiple comparisons, compute the two-sample Ì†µÌ±°-test (assuming equal variance) for each (column) feature of the Y array given the two groups deÔ¨Åned by grp variable. You should return two vectors of size n_features: one for the Ì†µÌ±°-values and one for the Ì†µÌ±ù-values.

ANOVA

Perform an ANOVA dataset described bellow
‚Ä¢ Compute between and within variances ‚Ä¢ Compute Ì†µÌ∞π -value: fval ‚Ä¢ Compare the Ì†µÌ±ù-value with the one obtained by stats.f_oneway using assert np.
allclose(arr1, arr2)

102

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

# dataset

mu_k = np.array([1, 2, 3]) # means of 3 samples

sd_k = np.array([1, 1, 1]) # sd of 3 samples

n_k = np.array([10, 20, 30]) # sizes of 3 samples

grp = [0, 1, 2]

# group labels

n = np.sum(n_k)

label = np.hstack([[k] * n_k[k] for k in [0, 1, 2]])

y = np.zeros(n) for k in grp:
y[label == k] = np.random.normal(mu_k[k], sd_k[k], n_k[k])

# Compute with scipy fval, pval = stats.f_oneway(y[label == 0], y[label == 1], y[label == 2])

Note: Click here to download the full example code

4.2 Lab 1: Brain volumes study
The study provides the brain volumes of grey matter (gm), white matter (wm) and cerebrospinal Ô¨Çuid) (csf) of 808 anatomical MRI scans. Manipulate data ‚Äî‚Äî‚Äî‚Äî‚Äî
Set the working directory within a directory called ‚Äúbrainvol‚Äù
Create 2 subdirectories: data that will contain downloaded data and reports for results of the analysis.
import os import os.path import pandas as pd import tempfile import urllib.request
WD = os.path.join(tempfile.gettempdir(), "brainvol") os.makedirs(WD, exist_ok=True) #os.chdir(WD)
# use cookiecutter file organization # https://drivendata.github.io/cookiecutter-data-science/ os.makedirs(os.path.join(WD, "data"), exist_ok=True) #os.makedirs("reports", exist_ok=True)
Fetch data
‚Ä¢ Demographic data demo.csv (columns: participant_id, site, group, age, sex) and tissue volume data: group is Control or Patient. site is the recruiting site.
‚Ä¢ Gray matter volume gm.csv (columns: participant_id, session, gm_vol)
‚Ä¢ White matter volume wm.csv (columns: participant_id, session, wm_vol)
‚Ä¢ Cerebrospinal Fluid csf.csv (columns: participant_id, session, csf_vol)

4.2. Lab 1: Brain volumes study

103

Statistics and Machine Learning in Python, Release 0.2

base_url = https://raw.github.com/neurospin/pystatsml/master/datasets/brain_volumes/%s data = dict() for file in ["demo.csv", "gm.csv", "wm.csv", "csf.csv"]:
urllib.request.urlretrieve(base_url % file, os.path.join(WD, "data", file))
demo = pd.read_csv(os.path.join(WD, "data", "demo.csv")) gm = pd.read_csv(os.path.join(WD, "data", "gm.csv")) wm = pd.read_csv(os.path.join(WD, "data", "wm.csv")) csf = pd.read_csv(os.path.join(WD, "data", "csf.csv"))
print("tables can be merge using shared columns") print(gm.head())
Out:
tables can be merge using shared columns participant_id session gm_vol
0 sub-S1-0002 ses-01 0.672506 1 sub-S1-0002 ses-02 0.678772 2 sub-S1-0002 ses-03 0.665592 3 sub-S1-0004 ses-01 0.890714 4 sub-S1-0004 ses-02 0.881127
Merge tables according to participant_id
brain_vol = pd.merge(pd.merge(pd.merge(demo, gm), wm), csf) assert brain_vol.shape == (808, 9)
Drop rows with missing values
brain_vol = brain_vol.dropna() assert brain_vol.shape == (766, 9)
Compute Total Intra-cranial volume tiv_vol = gm_vol + csf_vol + wm_vol.
brain_vol["tiv_vol"] = brain_vol["gm_vol"] + brain_vol["wm_vol"] + brain_vol["csf_vol"]
Compute tissue fractions gm_f = gm_vol / tiv_vol, wm_f = wm_vol / tiv_vol.
brain_vol["gm_f"] = brain_vol["gm_vol"] / brain_vol["tiv_vol"] brain_vol["wm_f"] = brain_vol["wm_vol"] / brain_vol["tiv_vol"]
Save in a excel Ô¨Åle brain_vol.xlsx
brain_vol.to_excel(os.path.join(WD, "data", "brain_vol.xlsx"), sheet_name= data , index=False)

4.2.1 Descriptive Statistics
Load excel Ô¨Åle brain_vol.xlsx import os import pandas as pd import seaborn as sns
104

(continues on next page)
Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

import statsmodels.formula.api as smfrmla import statsmodels.api as sm

(continued from previous page)

brain_vol = pd.read_excel(os.path.join(WD, "data", "brain_vol.xlsx"), sheet_name= data )
# Round float at 2 decimals when printing pd.options.display.float_format = {:,.2f} .format

Descriptive statistics Most of participants have several MRI sessions (column session) Select on rows from session one ‚Äúses-01‚Äù
brain_vol1 = brain_vol[brain_vol.session == "ses-01"] # Check that there are no duplicates assert len(brain_vol1.participant_id.unique()) == len(brain_vol1.participant_id)

Global descriptives statistics of numerical variables
desc_glob_num = brain_vol1.describe() print(desc_glob_num)

Out:

age gm_vol wm_vol csf_vol count 244.00 244.00 244.00 mean 34.54 0.71 0.44 std 12.09 0.08 0.07 min 18.00 0.48 0.05 25% 25.00 0.66 0.40 50% 31.00 0.70 0.43 75% 44.00 0.77 0.48 max 61.00 1.03 0.62

tiv_vol 244.00 0.31 0.08 0.12 0.25 0.30 0.37 0.63

gm_f wm_f 244.00 244.00 244.00
1.46 0.49 0.30 0.17 0.04 0.03 0.83 0.37 0.06 1.34 0.46 0.28 1.45 0.49 0.30 1.57 0.52 0.31 2.06 0.60 0.36

Global Descriptive statistics of categorical variable
desc_glob_cat = brain_vol1[["site", "group", "sex"]].describe(include= all ) print(desc_glob_cat)
print("Get count by level") desc_glob_cat = pd.DataFrame({col: brain_vol1[col].value_counts().to_dict()
for col in ["site", "group", "sex"]}) print(desc_glob_cat)

Out:

site group sex

count 244

244 244

unique 7

22

top

S7 Patient M

freq 65

157 155

Get count by level

site group sex

Control nan 87.00 nan

F

nan nan 89.00

M

nan nan 155.00

Patient nan 157.00 nan

(continues on next page)

4.2. Lab 1: Brain volumes study

105

Statistics and Machine Learning in Python, Release 0.2

S1

13.00 nan nan

S3

29.00 nan nan

S4

15.00 nan nan

S5

62.00 nan nan

S6

1.00 nan nan

S7

65.00 nan nan

S8

59.00 nan nan

(continued from previous page)

Remove the single participant from site 6
brain_vol = brain_vol[brain_vol.site != "S6"] brain_vol1 = brain_vol[brain_vol.session == "ses-01"] desc_glob_cat = pd.DataFrame({col: brain_vol1[col].value_counts().to_dict()
for col in ["site", "group", "sex"]}) print(desc_glob_cat)

Out:

site group sex

Control nan 86.00 nan

F

nan nan 88.00

M

nan nan 155.00

Patient nan 157.00 nan

S1

13.00 nan nan

S3

29.00 nan nan

S4

15.00 nan nan

S5

62.00 nan nan

S7

65.00 nan nan

S8

59.00 nan nan

Descriptives statistics of numerical variables per clinical status
desc_group_num = brain_vol1[["group", gm_vol ]].groupby("group").describe() print(desc_group_num)

Out:
gm_vol count mean std min 25% 50% 75% max
group Control 86.00 0.72 0.09 0.48 0.66 0.71 0.78 1.03 Patient 157.00 0.70 0.08 0.53 0.65 0.70 0.76 0.90

4.2.2 Statistics

Objectives:
1. Site effect of gray matter atrophy
2. Test the association between the age and gray matter atrophy in the control and patient population independently.
3. Test for differences of atrophy between the patients and the controls
4. Test for interaction between age and clinical status, ie: is the brain atrophy process in patient population faster than in the control population.

106

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2
5. The effect of the medication in the patient population.
import statsmodels.api as sm import statsmodels.formula.api as smfrmla import scipy.stats import seaborn as sns
1 Site effect on Grey Matter atrophy The model is Oneway Anova gm_f ~ site The ANOVA test has important assumptions that must be satisÔ¨Åed in order for the associated p-value to be valid.
‚Ä¢ The samples are independent. ‚Ä¢ Each sample is from a normally distributed population. ‚Ä¢ The population standard deviations of the groups are all equal. This property is known as
homoscedasticity. Plot sns.violinplot("site", "gm_f", data=brain_vol1)

Stats with scipy
fstat, pval = scipy.stats.f_oneway(*[brain_vol1.gm_f[brain_vol1.site == s] for s in brain_vol1.site.unique()])
print("Oneway Anova gm_f ~ site F=%.2f, p-value=%E" % (fstat, pval))
Out:

4.2. Lab 1: Brain volumes study

107

Statistics and Machine Learning in Python, Release 0.2

Oneway Anova gm_f ~ site F=14.82, p-value=1.188136E-12

Stats with statsmodels
anova = smfrmla.ols("gm_f ~ site", data=brain_vol1).fit() # print(anova.summary()) print("Site explains %.2f%% of the grey matter fraction variance" %
(anova.rsquared * 100))
print(sm.stats.anova_lm(anova, typ=2))

Out:

Site explains 23.82% of the grey matter fraction variance

sum_sq df F PR(>F)

site

0.11 5.00 14.82 0.00

Residual 0.35 237.00 nan nan

2. Test the association between the age and gray matter atrophy in the control and patient population independently. Plot
sns.lmplot("age", "gm_f", hue="group", data=brain_vol1)
brain_vol1_ctl = brain_vol1[brain_vol1.group == "Control"] brain_vol1_pat = brain_vol1[brain_vol1.group == "Patient"]

108

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

Stats with scipy
print("--- In control population ---") beta, beta0, r_value, p_value, std_err = \
scipy.stats.linregress(x=brain_vol1_ctl.age, y=brain_vol1_ctl.gm_f)
print("gm_f = %f * age + %f" % (beta, beta0)) print("Corr: %f, r-squared: %f, p-value: %f, std_err: %f"\
% (r_value, r_value**2, p_value, std_err))
print("--- In patient population ---") beta, beta0, r_value, p_value, std_err = \
scipy.stats.linregress(x=brain_vol1_pat.age, y=brain_vol1_pat.gm_f)
print("gm_f = %f * age + %f" % (beta, beta0)) print("Corr: %f, r-squared: %f, p-value: %f, std_err: %f"\
% (r_value, r_value**2, p_value, std_err))
print("Decrease seems faster in patient than in control population")
Out:
--- In control population --gm_f = -0.001181 * age + 0.529829 Corr: -0.325122, r-squared: 0.105704, p-value: 0.002255, std_err: 0.000375 --- In patient population ---
(continues on next page)

4.2. Lab 1: Brain volumes study

109

Statistics and Machine Learning in Python, Release 0.2

(continued from previous page)
gm_f = -0.001899 * age + 0.556886 Corr: -0.528765, r-squared: 0.279592, p-value: 0.000000, std_err: 0.000245 Decrease seems faster in patient than in control population

Stats with statsmodels
print("--- In control population ---") lr = smfrmla.ols("gm_f ~ age", data=brain_vol1_ctl).fit() print(lr.summary()) print("Age explains %.2f%% of the grey matter fraction variance" %
(lr.rsquared * 100))
print("--- In patient population ---") lr = smfrmla.ols("gm_f ~ age", data=brain_vol1_pat).fit() print(lr.summary()) print("Age explains %.2f%% of the grey matter fraction variance" %
(lr.rsquared * 100))

Out:

--- In control population ---

OLS Regression Results

==============================================================================

Dep. Variable:

gm_f R-squared:

0.106

Model:

OLS Adj. R-squared:

0.095

Method:

Least Squares F-statistic:

9.929

Date:

jeu., 16 mai 2019 Prob (F-statistic):

0.00226

Time:

20:18:24 Log-Likelihood:

159.34

No. Observations:

86 AIC:

-314.7

Df Residuals:

84 BIC:

-309.8

Df Model:

1

Covariance Type:

nonrobust

==============================================================================

coef std err

t

P>|t|

[0.025

0.975]

------------------------------------------------------------------------------

Intercept

0.5298

0.013 40.350

0.000

0.504

0.556

age

-0.0012

0.000 -3.151

0.002

-0.002

-0.000

==============================================================================

Omnibus:

0.946 Durbin-Watson:

1.628

Prob(Omnibus):

0.623 Jarque-Bera (JB):

0.782

Skew:

0.233 Prob(JB):

0.676

Kurtosis:

2.962 Cond. No.

111.

==============================================================================

Warnings:

[1] Standard Errors assume that the covariance matrix of the errors is correctly‚ê£

Àì‚Üíspecified.

Age explains 10.57% of the grey matter fraction variance

--- In patient population ---

OLS Regression Results

==============================================================================

Dep. Variable:

gm_f R-squared:

0.280

Model:

OLS Adj. R-squared:

0.275

Method:

Least Squares F-statistic:

60.16

Date:

jeu., 16 mai 2019 Prob (F-statistic):

1.09e-12

Time:

20:18:24 Log-Likelihood:

289.38

(continues on next page)

110

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

(continued from previous page)

No. Observations:

157 AIC:

-574.8

Df Residuals:

155 BIC:

-568.7

Df Model:

1

Covariance Type:

nonrobust

==============================================================================

coef std err

t

P>|t|

[0.025

0.975]

------------------------------------------------------------------------------

Intercept

0.5569

0.009 60.817

0.000

0.539

0.575

age

-0.0019

0.000 -7.756

0.000

-0.002

-0.001

==============================================================================

Omnibus:

2.310 Durbin-Watson:

1.325

Prob(Omnibus):

0.315 Jarque-Bera (JB):

1.854

Skew:

0.230 Prob(JB):

0.396

Kurtosis:

3.268 Cond. No.

111.

==============================================================================

Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly‚ê£ Àì‚Üíspecified. Age explains 27.96% of the grey matter fraction variance

Before testing for differences of atrophy between the patients ans the controls Preliminary tests for age x group effect (patients would be older or younger than Controls) Plot
sns.violinplot("group", "age", data=brain_vol1)

4.2. Lab 1: Brain volumes study

111

Statistics and Machine Learning in Python, Release 0.2

Stats with scipy print(scipy.stats.ttest_ind(brain_vol1_ctl.age, brain_vol1_pat.age))

Out: Ttest_indResult(statistic=-1.2155557697674162, pvalue=0.225343592508479)

Stats with statsmodels
print(smfrmla.ols("age ~ group", data=brain_vol1).fit().summary()) print("No significant difference in age between patients and controls")

Out:

OLS Regression Results

==============================================================================

Dep. Variable:

age R-squared:

0.006

Model:

OLS Adj. R-squared:

0.002

Method:

Least Squares F-statistic:

1.478

Date:

jeu., 16 mai 2019 Prob (F-statistic):

0.225

Time:

20:18:24 Log-Likelihood:

-949.69

No. Observations:

243 AIC:

1903.

Df Residuals:

241 BIC:

1910.

Df Model:

1

Covariance Type:

nonrobust

====================================================================================

coef std err

t

P>|t|

[0.025

0.975]

------------------------------------------------------------------------------------

Intercept

33.2558

1.305 25.484

0.000

30.685

35.826

group[T.Patient] 1.9735

1.624

1.216

0.225

-1.225

5.172

==============================================================================

Omnibus:

35.711 Durbin-Watson:

2.096

Prob(Omnibus):

0.000 Jarque-Bera (JB):

20.726

Skew:

0.569 Prob(JB):

3.16e-05

Kurtosis:

2.133 Cond. No.

3.12

==============================================================================

Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly‚ê£ Àì‚Üíspecified. No significant difference in age between patients and controls

Preliminary tests for sex x group (more/less males in patients than in Controls)
crosstab = pd.crosstab(brain_vol1.sex, brain_vol1.group) print("Obeserved contingency table") print(crosstab)
chi2, pval, dof, expected = scipy.stats.chi2_contingency(crosstab)
print("Chi2 = %f, pval = %f" % (chi2, pval)) print("Expected contingency table under the null hypothesis") print(expected) print("No significant difference in sex between patients and controls")

Out:

112

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

Obeserved contingency table

group Control Patient

sex

F

33

55

M

53

102

Chi2 = 0.143253, pval = 0.705068

Expected contingency table under the null hypothesis

[[ 31.14403292 56.85596708]

[ 54.85596708 100.14403292]]

No significant difference in sex between patients and controls

3. Test for differences of atrophy between the patients and the controls
print(sm.stats.anova_lm(smfrmla.ols("gm_f ~ group", data=brain_vol1).fit(), typ=2)) print("No significant difference in age between patients and controls")

Out:

sum_sq df F PR(>F)

group

0.00 1.00 0.01 0.92

Residual 0.46 241.00 nan nan

No significant difference in age between patients and controls

This model is simplistic we should adjust for age and site
print(sm.stats.anova_lm(smfrmla.ols( "gm_f ~ group + age + site", data=brain_vol1).fit(), typ=2))
print("No significant difference in age between patients and controls")

Out:

sum_sq df F PR(>F)

group

0.00 1.00 1.82 0.18

site

0.11 5.00 19.79 0.00

age

0.09 1.00 86.86 0.00

Residual 0.25 235.00 nan nan

No significant difference in age between patients and controls

4. Test for interaction between age and clinical status, ie: is the brain atrophy process in patient population faster than in the control population.
ancova = smfrmla.ols("gm_f ~ group:age + age + site", data=brain_vol1).fit() print(sm.stats.anova_lm(ancova, typ=2))
print("= Parameters =") print(ancova.params)
print("%.3f%% of grey matter loss per year (almost %.1f%% per decade)" %\ (ancova.params.age * 100, ancova.params.age * 100 * 10))
print("grey matter loss in patients is accelerated by %.3f%% per decade" % (ancova.params[ group[T.Patient]:age ] * 100 * 10))

Out:

4.2. Lab 1: Brain volumes study

113

Statistics and Machine Learning in Python, Release 0.2

sum_sq df F PR(>F)

site

0.11 5.00 20.28 0.00

age

0.10 1.00 89.37 0.00

group:age 0.00 1.00 3.28 0.07

Residual 0.25 235.00 nan nan

= Parameters =

Intercept

0.52

site[T.S3]

0.01

site[T.S4]

0.03

site[T.S5]

0.01

site[T.S7]

0.06

site[T.S8]

0.02

age

-0.00

group[T.Patient]:age -0.00

dtype: float64

-0.148% of grey matter loss per year (almost -1.5% per decade)

grey matter loss in patients is accelerated by -0.232% per decade

Total running time of the script: ( 0 minutes 5.184 seconds)

4.3 Multivariate statistics

Multivariate statistics includes all statistical techniques for analyzing samples made of two or more variables. The data set (a Ì†µÌ±Å √ó Ì†µÌ±É matrix X) is a collection of Ì†µÌ±Å independent samples column vectors [x1, . . . , xÌ†µÌ±ñ, . . . , xÌ†µÌ±Å ] of length Ì†µÌ±É

‚é°‚àíxÌ†µ1Ì±á ‚àí‚é§ ‚é° Ì†µÌ±•11 ¬∑ ¬∑ ¬∑ Ì†µÌ±•1Ì†µÌ±ó ¬∑ ¬∑ ¬∑ Ì†µÌ±•1Ì†µÌ±É ‚é§ ‚é° Ì†µÌ±•11 . . . Ì†µÌ±•1Ì†µÌ±É ‚é§

‚é¢ ‚é¢

...

‚é• ‚é•

‚é¢ ‚é¢

...

...

...

‚é• ‚é•

‚é¢ ‚é¢

...

...

‚é• ‚é•

X

=

‚é¢‚é¢‚àíxÌ†µÌ†µÌ±ñÌ±á

‚àí‚é•‚é•

=

‚é¢ ‚é¢

Ì†µÌ±•Ì†µÌ±ñ1

¬∑¬∑¬∑

Ì†µÌ±•Ì†µÌ±ñÌ†µÌ±ó

¬∑¬∑¬∑

Ì†µÌ±•Ì†µÌ±ñÌ†µÌ±É

‚é• ‚é•

=

‚é¢ ‚é¢

X

‚é• ‚é•

.

‚é¢ ‚é¢ ‚é£

...

‚é• ‚é• ‚é¶

‚é¢ ‚é¢ ‚é£

...

...

...

‚é• ‚é• ‚é¶

‚é¢ ‚é¢ ‚é£

...

...

‚é• ‚é• ‚é¶

‚àíxÌ†µÌ†µÌ±áÌ±É ‚àí

Ì†µÌ±•Ì†µÌ±Å1 ¬∑ ¬∑ ¬∑ Ì†µÌ±•Ì†µÌ±ÅÌ†µÌ±ó ¬∑ ¬∑ ¬∑ Ì†µÌ±•Ì†µÌ±ÅÌ†µÌ±É

Ì†µÌ±•Ì†µÌ±Å1 . . . Ì†µÌ±•Ì†µÌ±ÅÌ†µÌ±É Ì†µÌ±Å√óÌ†µÌ±É

4.3.1 Linear Algebra

Euclidean norm and distance

The Euclidean norm of a vector a ‚àà RÌ†µÌ±É is denoted

‚éØ

‚é∏ Ì†µÌ±É

‚Äña‚Äñ2

=

‚é∏‚àëÔ∏Å ‚é∑

Ì†µÌ±éÌ†µÌ±ñ2

Ì†µÌ±ñ

The Euclidean distance between two vectors a, b ‚àà RÌ†µÌ±É is

‚éØ

‚é∏ Ì†µÌ±É

‚Äña

‚àí

b‚Äñ2

=

‚é∏‚àëÔ∏Å ‚é∑ (Ì†µÌ±éÌ†µÌ±ñ

‚àí

Ì†µÌ±èÌ†µÌ±ñ)2

Ì†µÌ±ñ

Dot product and projection Source: Wikipedia 114

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

Algebraic deÔ¨Ånition

The dot product, denoted ‚Äô‚Äò¬∑‚Äù of two Ì†µÌ±É -dimensional vectors a = [Ì†µÌ±é1, Ì†µÌ±é2, ..., Ì†µÌ±éÌ†µÌ±É ] and a = [Ì†µÌ±è1, Ì†µÌ±è2, ..., Ì†µÌ±èÌ†µÌ±É ] is deÔ¨Åned as

‚é° Ì†µÌ±è1 ‚é§

‚é¢ ‚é¢

...

‚é• ‚é•

a ¬∑ b = aÌ†µÌ±á b = ‚àëÔ∏Å Ì†µÌ±éÌ†µÌ±ñÌ†µÌ±èÌ†µÌ±ñ = [Ô∏ÄÌ†µÌ±é1

...

aÌ†µÌ±á

...

Ì†µÌ±éÌ†µÌ±É

]Ô∏Ä

‚é¢ ‚é¢

b

‚é• ‚é•

.

Ì†µÌ±ñ

‚é¢ ‚é¢ ‚é£

...

‚é• ‚é• ‚é¶

Ì†µÌ±èÌ†µÌ±É

The Euclidean norm of a vector can be computed using the dot product, as ‚àö
‚Äña‚Äñ2 = a ¬∑ a.

Geometric deÔ¨Ånition: projection

In Euclidean space, a Euclidean vector is a geometrical object that possesses both a magnitude and a direction. A vector can be pictured as an arrow. Its magnitude is its length, and its direction is the direction that the arrow points. The magnitude of a vector a is denoted by ‚Äña‚Äñ2. The dot product of two Euclidean vectors a and b is deÔ¨Åned by

a ¬∑ b = ‚Äña‚Äñ2 ‚Äñb‚Äñ2 cos Ì†µÌºÉ,

where Ì†µÌºÉ is the angle between a and b. In particular, if a and b are orthogonal, then the angle between them is 90¬∞ and

a ¬∑ b = 0.

At the other extreme, if they are codirectional, then the angle between them is 0¬∞ and

a ¬∑ b = ‚Äña‚Äñ2 ‚Äñb‚Äñ2 This implies that the dot product of a vector a by itself is
a ¬∑ a = ‚Äña‚Äñ22 . The scalar projection (or scalar component) of a Euclidean vector a in the direction of a Euclidean vector b is given by

Ì†µÌ±éÌ†µÌ±è = ‚Äña‚Äñ2 cos Ì†µÌºÉ,

where Ì†µÌºÉ is the angle between a and b.

In terms of the geometric deÔ¨Ånition of the dot product, this can be rewritten

a¬∑b

Ì†µÌ±éÌ†µÌ±è

=

, ‚Äñb‚Äñ2

import numpy as np np.random.seed(42)
a = np.random.randn(10) b = np.random.randn(10)
np.dot(a, b)

4.3. Multivariate statistics

115

Statistics and Machine Learning in Python, Release 0.2

-4.085788532659924

Fig. 5: Projection.

4.3.2 Mean vector

The mean (Ì†µÌ±É √ó 1) column-vector Ì†µÌºá whose estimator is

‚é° Ì†µÌ±•Ì†µÌ±ñ1 ‚é§ ‚é° Ì†µ¬ØÌ±•1 ‚é§

x¬Ø

=

1 Ì†µÌ±Å

Ì†µÌ±Å
‚àëÔ∏Å xi
Ì†µÌ±ñ=1

=

1 Ì†µÌ±Å

Ì†µÌ±Å

‚é¢ ‚é¢

...

‚àëÔ∏Å

‚é¢ ‚é¢

Ì†µÌ±•Ì†µÌ±ñÌ†µÌ±ó

‚é¢ Ì†µÌ±ñ=1 ‚é¢
‚é£

...

‚é• ‚é• ‚é• ‚é• ‚é• ‚é• ‚é¶

=

‚é¢ ‚é¢

...

‚é¢ ‚é¢

Ì†µ¬ØÌ±•Ì†µÌ±ó

‚é¢ ‚é¢ ‚é£

...

‚é•

‚é•

‚é• ‚é•

.

‚é•

‚é•

‚é¶

Ì†µÌ±•Ì†µÌ±ñÌ†µÌ±É

Ì†µ¬ØÌ±•Ì†µÌ±É

4.3.3 Covariance matrix

‚Ä¢ The covariance matrix Œ£XX is a symmetric positive semi-deÔ¨Ånite matrix whose element in the Ì†µÌ±ó, Ì†µÌ±ò position is the covariance between the Ì†µÌ±óÌ†µÌ±°‚Ñé and Ì†µÌ±òÌ†µÌ±°‚Ñé elements of a random vector
i.e. the Ì†µÌ±óÌ†µÌ±°‚Ñé and Ì†µÌ±òÌ†µÌ±°‚Ñé columns of X.

‚Ä¢ The covariance matrix generalizes the notion of covariance to multiple dimensions.

‚Ä¢ The covariance matrix describe the shape of the sample distribution around the mean assuming an elliptical distribution:

Œ£XX = Ì†µÌ∞∏(X ‚àí Ì†µÌ∞∏(X))Ì†µÌ±á Ì†µÌ∞∏(X ‚àí Ì†µÌ∞∏(X)),

whose estimator SXX is a Ì†µÌ±É √ó Ì†µÌ±É matrix given by

SXX

=

Ì†µÌ±Å

1 ‚àí

(X 1

‚àí

1x¬ØÌ†µÌ±á )Ì†µÌ±á (X

‚àí

1x¬ØÌ†µÌ±á ).

If we assume that X is centered, i.e. X is replaced by X ‚àí 1x¬ØÌ†µÌ±á then the estimator is

‚é° Ì†µÌ±•11

SXX

=

Ì†µÌ±Å

1 ‚àí

XÌ†µÌ±á X 1

=

Ì†µÌ±Å

1 ‚àí

1

‚é¢ Ì†µÌ±•1Ì†µÌ±ó

‚é¢ ‚é¢ ‚é£

...

Ì†µÌ±•1Ì†µÌ±É

¬∑¬∑¬∑ ¬∑¬∑¬∑
¬∑¬∑¬∑

Ì†µÌ±•Ì†µÌ±Å 1 Ì†µÌ±•Ì†µÌ±Å Ì†µÌ±ó
...
Ì†µÌ±•Ì†µÌ±Å Ì†µÌ±É

‚é§
‚é• ‚é• ‚é• ‚é¶

‚é° Ì†µÌ±•11

‚é¢ ‚é£

...

Ì†µÌ±•Ì†µÌ±Å 1

¬∑¬∑¬∑ ¬∑¬∑¬∑

Ì†µÌ±•1Ì†µÌ±ò ...
Ì†µÌ±•Ì†µÌ±Å Ì†µÌ±ò

Ì†µÌ±•1Ì†µÌ±É ‚é§ ‚é°Ì†µÌ±†1

...

‚é¢ ‚é•=‚é¢ ‚é¶‚é¢

Ì†µÌ±•Ì†µÌ±Å Ì†µÌ±É

‚é£

... ...

Ì†µÌ±†1Ì†µÌ±ò
Ì†µÌ±†Ì†µÌ±óÌ†µÌ±ò Ì†µÌ±†Ì†µÌ±ò

Ì†µÌ±†1Ì†µÌ±É ‚é§

...

‚é• ‚é•,

Ì†µÌ±†Ì†µÌ±òÌ†µÌ±É

‚é• ‚é¶

Ì†µÌ±†Ì†µÌ±É

116

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

where

Ì†µÌ±†Ì†µÌ±óÌ†µÌ±ò

=

Ì†µÌ±†Ì†µÌ±òÌ†µÌ±ó

=

Ì†µÌ±Å

1 ‚àí

1

xjÌ†µÌ±á

xk

=

Ì†µÌ±Å

1 ‚àí1

Ì†µÌ±Å
‚àëÔ∏Å Ì†µÌ±•Ì†µÌ±ñÌ†µÌ±ó Ì†µÌ±•Ì†µÌ±ñÌ†µÌ±ò

Ì†µÌ±ñ=1

is an estimator of the covariance between the Ì†µÌ±óÌ†µÌ±°‚Ñé and Ì†µÌ±òÌ†µÌ±°‚Ñé variables.

## Avoid warnings and force inline plot %matplotlib inline import warnings warnings.filterwarnings("ignore") ## import numpy as np import scipy import matplotlib.pyplot as plt import seaborn as sns import pystatsml.plot_utils import seaborn as sns # nice color

np.random.seed(42) colors = sns.color_palette()

n_samples, n_features = 100, 2

mean, Cov, X = [None] * 4, [None] * 4, [None] * 4 mean[0] = np.array([-2.5, 2.5]) Cov[0] = np.array([[1, 0],
[0, 1]])

mean[1] = np.array([2.5, 2.5]) Cov[1] = np.array([[1, .5],
[.5, 1]])

mean[2] = np.array([-2.5, -2.5]) Cov[2] = np.array([[1, .9],
[.9, 1]])

mean[3] = np.array([2.5, -2.5]) Cov[3] = np.array([[1, -.9],
[-.9, 1]])

# Generate dataset for i in range(len(mean)):
X[i] = np.random.multivariate_normal(mean[i], Cov[i], n_samples)

# Plot for i in range(len(mean)):
# Points plt.scatter(X[i][:, 0], X[i][:, 1], color=colors[i], label="class %i" % i) # Means plt.scatter(mean[i][0], mean[i][1], marker="o", s=200, facecolors= w ,
edgecolors=colors[i], linewidth=2) # Ellipses representing the covariance matrices pystatsml.plot_utils.plot_cov_ellipse(Cov[i], pos=mean[i], facecolor= none ,
linewidth=2, edgecolor=colors[i])

plt.axis( equal ) _ = plt.legend(loc= upper left )

4.3. Multivariate statistics

117

Statistics and Machine Learning in Python, Release 0.2

4.3.4 Correlation matrix
import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns
url = https://python-graph-gallery.com/wp-content/uploads/mtcars.csv df = pd.read_csv(url)
# Compute the correlation matrix corr = df.corr()
# Generate a mask for the upper triangle mask = np.zeros_like(corr, dtype=np.bool) mask[np.triu_indices_from(mask)] = True
f, ax = plt.subplots(figsize=(5.5, 4.5)) cmap = sns.color_palette("RdBu_r", 11) # Draw the heatmap with the mask and correct aspect ratio _ = sns.heatmap(corr, mask=None, cmap=cmap, vmax=1, center=0,
square=True, linewidths=.5, cbar_kws={"shrink": .5})

118

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

Re-order correlation matrix using AgglomerativeClustering
# convert correlation to distances d = 2 * (1 - np.abs(corr))
from sklearn.cluster import AgglomerativeClustering clustering = AgglomerativeClustering(n_clusters=3, linkage= single , affinity="precomputed Àì‚Üí").fit(d) lab=0
clusters = [list(corr.columns[clustering.labels_==lab]) for lab in set(clustering.labels_ Àì‚Üí)] print(clusters)
reordered = np.concatenate(clusters)
R = corr.loc[reordered, reordered]
f, ax = plt.subplots(figsize=(5.5, 4.5)) # Draw the heatmap with the mask and correct aspect ratio _ = sns.heatmap(R, mask=None, cmap=cmap, vmax=1, center=0,
square=True, linewidths=.5, cbar_kws={"shrink": .5})
[[ mpg , cyl , disp , hp , wt , qsec , vs , carb ], [ am , gear ], [ drat ]]

4.3. Multivariate statistics

119

Statistics and Machine Learning in Python, Release 0.2

4.3.5 Precision matrix
In statistics, precision is the reciprocal of the variance, and the precision matrix is the matrix inverse of the covariance matrix.
It is related to partial correlations that measures the degree of association between two variables, while controlling the effect of other variables.
import numpy as np
Cov = np.array([[1.0, 0.9, 0.9, 0.0, 0.0, 0.0], [0.9, 1.0, 0.9, 0.0, 0.0, 0.0], [0.9, 0.9, 1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.9, 0.0], [0.0, 0.0, 0.0, 0.9, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])
print("# Precision matrix:") Prec = np.linalg.inv(Cov) print(Prec.round(2))
print("# Partial correlations:") Pcor = np.zeros(Prec.shape) Pcor[::] = np.NaN
for i, j in zip(*np.triu_indices_from(Prec, 1)): Pcor[i, j] = - Prec[i, j] / np.sqrt(Prec[i, i] * Prec[j, j])
print(Pcor.round(2))

120

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

# Precision matrix: [[ 6.79 -3.21 -3.21 0. 0. 0. ] [-3.21 6.79 -3.21 0. 0. 0. ] [-3.21 -3.21 6.79 0. 0. 0. ] [ 0. -0. -0. 5.26 -4.74 -0. ] [ 0. 0. 0. -4.74 5.26 0. ] [ 0. 0. 0. 0. 0. 1. ]] # Partial correlations: [[ nan 0.47 0.47 -0. -0. -0. ] [ nan nan 0.47 -0. -0. -0. ] [ nan nan nan -0. -0. -0. ] [ nan nan nan nan 0.9 0. ] [ nan nan nan nan nan -0. ] [ nan nan nan nan nan nan]]

4.3.6 Mahalanobis distance

‚Ä¢ The Mahalanobis distance is a measure of the distance between two points x and Ì†µÌºá where the dispersion (i.e. the covariance structure) of the samples is taken into account.
‚Ä¢ The dispersion is considered through covariance matrix.
This is formally expressed as

‚àöÔ∏Å Ì†µÌ∞∑Ì†µÌ±Ä (x, Ì†µÌºá) = (x ‚àí Ì†µÌºá)Ì†µÌ±á Œ£‚àí1(x ‚àí Ì†µÌºá).

Intuitions

‚Ä¢ Distances along the principal directions of dispersion are contracted since they correspond to likely dispersion of points.

‚Ä¢ Distances othogonal to the principal directions of dispersion are dilated since they correspond to unlikely dispersion of points.

For example

‚àö Ì†µÌ∞∑Ì†µÌ±Ä (1) = 1Ì†µÌ±á Œ£‚àí11.

ones = np.ones(Cov.shape[0]) d_euc = np.sqrt(np.dot(ones, ones)) d_mah = np.sqrt(np.dot(np.dot(ones, Prec), ones))
print("Euclidean norm of ones=%.2f. Mahalanobis norm of ones=%.2f" % (d_euc, d_mah))

Euclidean norm of ones=2.45. Mahalanobis norm of ones=1.77

The Ô¨Årst dot product that distances along the principal directions of dispersion are contracted: print(np.dot(ones, Prec))

[0.35714286 0.35714286 0.35714286 0.52631579 0.52631579 1.

]

4.3. Multivariate statistics

121

Statistics and Machine Learning in Python, Release 0.2

import numpy as np import scipy import matplotlib.pyplot as plt import seaborn as sns import pystatsml.plot_utils %matplotlib inline np.random.seed(40) colors = sns.color_palette()

mean = np.array([0, 0]) Cov = np.array([[1, .8],
[.8, 1]]) samples = np.random.multivariate_normal(mean, Cov, 100) x1 = np.array([0, 2]) x2 = np.array([2, 2])

plt.scatter(samples[:, 0], samples[:, 1], color=colors[0]) plt.scatter(mean[0], mean[1], color=colors[0], s=200, label="mean") plt.scatter(x1[0], x1[1], color=colors[1], s=200, label="x1") plt.scatter(x2[0], x2[1], color=colors[2], s=200, label="x2")

# plot covariance ellipsis pystatsml.plot_utils.plot_cov_ellipse(Cov, pos=mean, facecolor= none ,
linewidth=2, edgecolor=colors[0]) # Compute distances d2_m_x1 = scipy.spatial.distance.euclidean(mean, x1) d2_m_x2 = scipy.spatial.distance.euclidean(mean, x2)

Covi = scipy.linalg.inv(Cov) dm_m_x1 = scipy.spatial.distance.mahalanobis(mean, x1, Covi) dm_m_x2 = scipy.spatial.distance.mahalanobis(mean, x2, Covi)

# Plot distances vm_x1 = (x1 - mean) / d2_m_x1 vm_x2 = (x2 - mean) / d2_m_x2 jitter = .1 plt.plot([mean[0] - jitter, d2_m_x1 * vm_x1[0] - jitter],
[mean[1], d2_m_x1 * vm_x1[1]], color= k ) plt.plot([mean[0] - jitter, d2_m_x2 * vm_x2[0] - jitter],
[mean[1], d2_m_x2 * vm_x2[1]], color= k )

plt.plot([mean[0] + jitter, dm_m_x1 * vm_x1[0] + jitter], [mean[1], dm_m_x1 * vm_x1[1]], color= r )
plt.plot([mean[0] + jitter, dm_m_x2 * vm_x2[0] + jitter], [mean[1], dm_m_x2 * vm_x2[1]], color= r )

plt.legend(loc= lower right ) plt.text(-6.1, 3,
Euclidian: d(m, x1) = %.1f<d(m, x2) = %.1f plt.text(-6.1, 3.5,
Mahalanobis: d(m, x1) = %.1f>d(m, x2) = %.1f

% (d2_m_x1, d2_m_x2), color= k ) % (dm_m_x1, dm_m_x2), color= r )

plt.axis( equal ) print( Euclidian d(m, x1) = %.2f < d(m, x2) = %.2f % (d2_m_x1, d2_m_x2)) print( Mahalanobis d(m, x1) = %.2f > d(m, x2) = %.2f % (dm_m_x1, dm_m_x2))

122

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2
Euclidian d(m, x1) = 2.00 < d(m, x2) = 2.83 Mahalanobis d(m, x1) = 3.33 > d(m, x2) = 2.11

If the covariance matrix is the identity matrix, the Mahalanobis distance reduces to the Euclidean distance. If the covariance matrix is diagonal, then the resulting distance measure is called a normalized Euclidean distance.
More generally, the Mahalanobis distance is a measure of the distance between a point x and a distribution Ì†µÌ≤© (x|Ì†µÌºá, Œ£). It is a multi-dimensional generalization of the idea of measuring how many standard deviations away x is from the mean. This distance is zero if x is at the mean, and grows as x moves away from the mean: along each principal component axis, it measures the number of standard deviations from x to the mean of the distribution.

4.3.7 Multivariate normal distribution

The distribution, or probability density function (PDF) (sometimes just density), of a continuous random variable is a function that describes the relative likelihood for this random variable to take on a given value.

The multivariate normal distribution, or multivariate Gaussian distribution, of a Ì†µÌ±É -dimensional random vector x = [Ì†µÌ±•1, Ì†µÌ±•2, . . . , Ì†µÌ±•Ì†µÌ±É ]Ì†µÌ±á is

Ì†µÌ≤© (x|Ì†µÌºá, Œ£) =

1

exp{‚àí 1 (x ‚àí Ì†µÌºá)Ì†µÌ±á Œ£‚àí1(x ‚àí Ì†µÌºá)}.

(2Ì†µÌºã )Ì†µÌ±É /2 |Œ£|1/2

2

import numpy as np import matplotlib.pyplot as plt import scipy.stats from scipy.stats import multivariate_normal from mpl_toolkits.mplot3d import Axes3D

4.3. Multivariate statistics

(continues on next page)
123

Statistics and Machine Learning in Python, Release 0.2
(continued from previous page) def multivariate_normal_pdf(X, mean, sigma):
"""Multivariate normal probability density function over X (n_samples x n_features)""" P = X.shape[1] det = np.linalg.det(sigma) norm_const = 1.0 / (((2*np.pi) ** (P/2)) * np.sqrt(det)) X_mu = X - mu inv = np.linalg.inv(sigma) d2 = np.sum(np.dot(X_mu, inv) * X_mu, axis=1) return norm_const * np.exp(-0.5 * d2)
# mean and covariance mu = np.array([0, 0]) sigma = np.array([[1, -.5],
[-.5, 1]])
# x, y grid x, y = np.mgrid[-3:3:.1, -3:3:.1] X = np.stack((x.ravel(), y.ravel())).T norm = multivariate_normal_pdf(X, mean, sigma).reshape(x.shape)
# Do it with scipy norm_scpy = multivariate_normal(mu, sigma).pdf(np.stack((x, y), axis=2)) assert np.allclose(norm, norm_scpy)
# Plot fig = plt.figure(figsize=(10, 7)) ax = fig.gca(projection= 3d ) surf = ax.plot_surface(x, y, norm, rstride=3,
cstride=3, cmap=plt.cm.coolwarm, linewidth=1, antialiased=False )
ax.set_zlim(0, 0.2) ax.zaxis.set_major_locator(plt.LinearLocator(10)) ax.zaxis.set_major_formatter(plt.FormatStrFormatter( %.02f ))
ax.set_xlabel( X ) ax.set_ylabel( Y ) ax.set_zlabel( p(x) )
plt.title( Bivariate Normal/Gaussian distribution ) fig.colorbar(surf, shrink=0.5, aspect=7, cmap=plt.cm.coolwarm) plt.show()

124

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

4.3.8 Exercises
Dot product and Euclidean norm
Given a = [2, 1]Ì†µÌ±á and b = [1, 1]Ì†µÌ±á 1. Write a function euclidean(x) that computes the Euclidean norm of vector, x. 2. Compute the Euclidean norm of a. 3. Compute the Euclidean distance of ‚Äña ‚àí b‚Äñ2. 4. Compute the projection of b in the direction of vector a: Ì†µÌ±èÌ†µÌ±é. 5. Simulate a dataset X of Ì†µÌ±Å = 100 samples of 2-dimensional vectors. 6. Project all samples in the direction of the vector a.

Covariance matrix and Mahalanobis norm

1. Sample a dataset X of Ì†µÌ±Å = 100 samples of 2-dimensional vectors from the bivariate

normal

distribution

Ì†µÌ≤© (Ì†µÌºá, Œ£)

where

Ì†µÌºá

=

[1, 1]Ì†µÌ±á

and

Œ£

=

[Ô∏Ç 1 0.8, 1

0.8]Ô∏Ç.

2. Compute the mean vector x¬Ø and center X. Compare the estimated mean x¬Ø to the true mean, Ì†µÌºá.

3. Compute the empirical covariance matrix S. Compare the estimated covariance matrix S to the true covariance matrix, Œ£.

4.3. Multivariate statistics

125

Statistics and Machine Learning in Python, Release 0.2

4. Compute S‚àí1 (Sinv) the inverse of the covariance matrix by using scipy.linalg.inv(S).
5. Write a function mahalanobis(x, xbar, Sinv) that computes the Mahalanobis distance of a vector x to the mean, x¬Ø.
6. Compute the Mahalanobis and Euclidean distances of each sample xÌ†µÌ±ñ to the mean x¬Ø. Store the results in a 100 √ó 2 dataframe.

4.4 Time Series in python
Two libraries: ‚Ä¢ Pandas: https://pandas.pydata.org/pandas-docs/stable/timeseries.html ‚Ä¢ scipy http://www.statsmodels.org/devel/tsa.html

4.4.1 Stationarity
A TS is said to be stationary if its statistical properties such as mean, variance remain constant over time.
‚Ä¢ constant mean ‚Ä¢ constant variance ‚Ä¢ an autocovariance that does not depend on time. what is making a TS non-stationary. There are 2 major reasons behind non-stationaruty of a TS: 1. Trend ‚Äì varying mean over time. For eg, in this case we saw that on average, the number
of passengers was growing over time. 2. Seasonality ‚Äì variations at speciÔ¨Åc time-frames. eg people might have a tendency to buy
cars in a particular month because of pay increment or festivals.

4.4.2 Pandas Time Series Data Structure

A Series is similar to a list or an array in Python. It represents a series of values (numeric or otherwise) such as a column of data. It provides additional functionality, methods, and operators, which make it a more powerful version of a list.

import pandas as pd import numpy as np

# Create a Series from a list ser = pd.Series([1, 3]) print(ser)

# String as index prices = { apple : 4.99,
banana : 1.99, orange : 3.99} ser = pd.Series(prices)

(continues on next page)

126

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

print(ser)
x = pd.Series(np.arange(1,3), index=[x for x in ab ]) print(x) print(x[ b ])
01 13 dtype: int64 apple 4.99 banana 1.99 orange 3.99 dtype: float64 a1 b2 dtype: int64 2

(continued from previous page)

4.4.3 Time Series Analysis of Google Trends
source: https://www.datacamp.com/community/tutorials/time-series-analysis-tutorial
Get Google Trends data of keywords such as ‚Äòdiet‚Äô and ‚Äògym‚Äô and see how they vary over time while learning about trends and seasonality in time series data.
In the Facebook Live code along session on the 4th of January, we checked out Google trends data of keywords ‚Äòdiet‚Äô, ‚Äògym‚Äô and ‚ÄòÔ¨Ånance‚Äô to see how they vary over time. We asked ourselves if there could be more searches for these terms in January when we‚Äôre all trying to turn over a new leaf?
In this tutorial, you‚Äôll go through the code that we put together during the session step by step. You‚Äôre not going to do much mathematics but you are going to do the following:
‚Ä¢ Read data
‚Ä¢ Recode data
‚Ä¢ Exploratory Data Analysis

4.4.4 Read data

import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns

# Plot appears on its own windows %matplotlib inline # Tools / Preferences / Ipython Console / Graphics Àì‚Üí‚Äúautomatic‚Äù # Interactive Matplotlib Jupyter Notebook # %matplotlib inline

/ Graphics Backend / Backend:‚ê£

(continues on next page)

4.4. Time Series in python

127

Statistics and Machine Learning in Python, Release 0.2

(continued from previous page) try:
url = "https://raw.githubusercontent.com/datacamp/datacamp_facebook_live_ny_ Àì‚Üíresolution/master/datasets/multiTimeline.csv"
df = pd.read_csv(url, skiprows=2) except:
df = pd.read_csv("../datasets/multiTimeline.csv", skiprows=2)
print(df.head())
# Rename columns df.columns = [ month , diet , gym , finance ]
# Describe print(df.describe())

Month diet: (Worldwide)

0 2004-01

100

1 2004-02

75

2 2004-03

67

3 2004-04

70

4 2004-05

72

diet

gym

count 168.000000 168.000000

mean 49.642857 34.690476

std

8.033080 8.134316

min 34.000000 22.000000

25% 44.000000 28.000000

50% 48.500000 32.500000

75% 53.000000 41.000000

max 100.000000 58.000000

gym: (Worldwide) 31 26 24 22 22
finance 168.000000 47.148810
4.972547 38.000000 44.000000 46.000000 50.000000 73.000000

finance: (Worldwide) 48 49 47 48 43

4.4.5 Recode data

Next, you‚Äôll turn the ‚Äòmonth‚Äô column into a DateTime data type and make it the index of the DataFrame.
Note that you do this because you saw in the result of the .info() method that the ‚ÄòMonth‚Äô column was actually an of data type object. Now, that generic data type encapsulates everything from strings to integers, etc. That‚Äôs not exactly what you want when you want to be looking at time series data. That‚Äôs why you‚Äôll use .to_datetime() to convert the ‚Äòmonth‚Äô column in your DataFrame to a DateTime.
Be careful! Make sure to include the inplace argument when you‚Äôre setting the index of the DataFrame df so that you actually alter the original index and set it to the ‚Äòmonth‚Äô column.
df.month = pd.to_datetime(df.month) df.set_index( month , inplace=True)
print(df.head())

diet gym finance

month

2004-01-01 100 31

48

(continues on next page)

128

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

2004-02-01 75 26

49

2004-03-01 67 24

47

2004-04-01 70 22

48

2004-05-01 72 22

43

(continued from previous page)

4.4.6 Exploratory Data Analysis
You can use a built-in pandas visualization method .plot() to plot your data as 3 line plots on a single Ô¨Ågure (one for each column, namely, ‚Äòdiet‚Äô, ‚Äògym‚Äô, and ‚ÄòÔ¨Ånance‚Äô).
df.plot() plt.xlabel( Year );
# change figure parameters # df.plot(figsize=(20,10), linewidth=5, fontsize=20)
# Plot single column # df[[ diet ]].plot(figsize=(20,10), linewidth=5, fontsize=20) # plt.xlabel( Year , fontsize=20);

Note that this data is relative. As you can read on Google trends:
Numbers represent search interest relative to the highest point on the chart for the given region and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. Likewise a score of 0 means the term was less than 1% as popular as the peak.

4.4. Time Series in python

129

Statistics and Machine Learning in Python, Release 0.2
4.4.7 Resampling, Smoothing, Windowing, Rolling average: Trends
Rolling average, for each time point, take the average of the points on either side of it. Note that the number of points is speciÔ¨Åed by a window size. Remove Seasonality with pandas Series. See: http://pandas.pydata.org/pandas-docs/stable/timeseries.html A: ‚Äòyear end frequency‚Äô year frequency diet = df[ diet ]
diet_resamp_yr = diet.resample( A ).mean() diet_roll_yr = diet.rolling(12).mean()
ax = diet.plot(alpha=0.5, style= - ) # store axis (ax) for latter plots diet_resamp_yr.plot(style= : , label= Resample at year frequency , ax=ax) diet_roll_yr.plot(style= -- , label= Rolling average (smooth), window size=12 , ax=ax) ax.legend()
<matplotlib.legend.Legend at 0x7f12ec1eec50>

Rolling average (smoothing) with Numpy
x = np.asarray(df[[ diet ]]) win = 12 win_half = int(win / 2) # print([((idx-win_half), (idx+win_half)) for idx in np.arange(win_half, len(x))])
diet_smooth = np.array([x[(idx-win_half):(idx+win_half)].mean() for idx in np.arange(win_ Àì‚Üíhalf, len(x))]) plt.plot(diet_smooth)

130

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2 [<matplotlib.lines.Line2D at 0x7f12ec163f98>]
Trends Plot Diet and Gym Build a new DataFrame which is the concatenation diet and gym smoothed data gym = df[ gym ] df_avg = pd.concat([diet.rolling(12).mean(), gym.rolling(12).mean()], axis=1) df_avg.plot() plt.xlabel( Year ) Text(0.5, 0, Year )

4.4. Time Series in python

131

Statistics and Machine Learning in Python, Release 0.2
Detrending df_dtrend = df[["diet", "gym"]] - df_avg df_dtrend.plot() plt.xlabel( Year ) Text(0.5, 0, Year )

132

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2
4.4.8 First-order diÔ¨Äerencing: Seasonal Patterns
# diff = original - shiftted data # (exclude first term for some implementation details) assert np.all((diet.diff() == diet - diet.shift())[1:]) df.diff().plot() plt.xlabel( Year )
Text(0.5, 0, Year )

4.4.9 Periodicity and Correlation

df.plot() plt.xlabel( Year ); print(df.corr())

diet

gym finance

diet 1.000000 -0.100764 -0.034639

gym -0.100764 1.000000 -0.284279

finance -0.034639 -0.284279 1.000000

4.4. Time Series in python

133

Statistics and Machine Learning in Python, Release 0.2

Plot correlation matrix sns.heatmap(df.corr(), cmap="coolwarm") <matplotlib.axes._subplots.AxesSubplot at 0x7f12e9e05a90>

‚Äòdiet‚Äô and ‚Äògym‚Äô are negatively correlated! Remember that you have a seasonal and a trend component. From the correlation coefÔ¨Åcient, ‚Äòdiet‚Äô and ‚Äògym‚Äô are negatively correlated:
‚Ä¢ trends components are negatively correlated. ‚Ä¢ seasonal components would positively correlated and their The actual correlation coefÔ¨Åcient is actually capturing both of those.

134

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

Seasonal correlation: correlation of the Ô¨Årst-order differences of these time series
df.diff().plot() plt.xlabel( Year );
print(df.diff().corr())

diet gym finance

diet 1.000000 0.758707 0.373828

gym 0.758707 1.000000 0.301111

finance 0.373828 0.301111 1.000000

Plot correlation matrix sns.heatmap(df.diff().corr(), cmap="coolwarm") <matplotlib.axes._subplots.AxesSubplot at 0x7f12ec06c438>

4.4. Time Series in python

135

Statistics and Machine Learning in Python, Release 0.2

Decomposing time serie in trend, seasonality and residuals
from statsmodels.tsa.seasonal import seasonal_decompose
x = gym
x = x.astype(float) # force float decomposition = seasonal_decompose(x) trend = decomposition.trend seasonal = decomposition.seasonal residual = decomposition.resid
plt.subplot(411) plt.plot(x, label= Original ) plt.legend(loc= best ) plt.subplot(412) plt.plot(trend, label= Trend ) plt.legend(loc= best ) plt.subplot(413) plt.plot(seasonal,label= Seasonality ) plt.legend(loc= best ) plt.subplot(414) plt.plot(residual, label= Residuals ) plt.legend(loc= best ) plt.tight_layout()

136

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

4.4.10 Autocorrelation
A time series is periodic if it repeats itself at equally spaced intervals, say, every 12 months. Autocorrelation Function (ACF): It is a measure of the correlation between the TS with a lagged version of itself. For instance at lag 5, ACF would compare series at time instant t1. . . t2 with series at instant t1-5. . . t2-5 (t1-5 and t2 being end points). Plot
# from pandas.plotting import autocorrelation_plot from pandas.plotting import autocorrelation_plot
x = df["diet"].astype(float) autocorrelation_plot(x)
<matplotlib.axes._subplots.AxesSubplot at 0x7f12e9a6d4e0>

4.4. Time Series in python

137

Statistics and Machine Learning in Python, Release 0.2

Compute Autocorrelation Function (ACF)
from statsmodels.tsa.stattools import acf
x_diff = x.diff().dropna() # first item is NA lag_acf = acf(x_diff, nlags=36) plt.plot(lag_acf) plt.title( Autocorrelation Function )
Text(0.5, 1.0, Autocorrelation Function )

ACF peaks every 12 months: Time series is correlated with itself shifted by 12 months.

138

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

4.4.11 Time Series Forecasting with Python using Autoregressive Moving Average (ARMA) models

Source:

‚Ä¢ https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/ 9781783553358/7/ch07lvl1sec77/arma-models

‚Ä¢ http://en.wikipedia.org/wiki/Autoregressive%E2%80%93moving-average_model

‚Ä¢ ARIMA:

https://www.analyticsvidhya.com/blog/2016/02/

time-series-forecasting-codes-python/

ARMA models are often used to forecast a time series. These models combine autoregressive and moving average models. In moving average models, we assume that a variable is the sum of the mean of the time series and a linear combination of noise components.

The autoregressive and moving average models can have different orders. In general, we can deÔ¨Åne an ARMA model with p autoregressive terms and q moving average terms as follows:

Ì†µÌ±ù

Ì†µÌ±û

‚àëÔ∏Å

‚àëÔ∏Å

Ì†µÌ±•Ì†µÌ±° = Ì†µÌ±éÌ†µÌ±ñÌ†µÌ±•Ì†µÌ±°‚àíÌ†µÌ±ñ + Ì†µÌ±èÌ†µÌ±ñÌ†µÌºÄÌ†µÌ±°‚àíÌ†µÌ±ñ + Ì†µÌºÄÌ†µÌ±°

Ì†µÌ±ñ

Ì†µÌ±ñ

Choosing p and q

Plot the partial autocorrelation functions for an estimate of p, and likewise using the autocorrelation functions for an estimate of q.
Partial Autocorrelation Function (PACF): This measures the correlation between the TS with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons. Eg at lag 5, it will check the correlation but remove the effects already explained by lags 1 to 4.

from statsmodels.tsa.stattools import acf, pacf

x = df["gym"].astype(float)

x_diff = x.diff().dropna() # first item is NA # ACF and PACF plots:

lag_acf = acf(x_diff, nlags=20) lag_pacf = pacf(x_diff, nlags=20, method= ols )

#Plot ACF: plt.subplot(121) plt.plot(lag_acf) plt.axhline(y=0,linestyle= -- ,color= gray ) plt.axhline(y=-1.96/np.sqrt(len(x_diff)),linestyle= -- ,color= gray ) plt.axhline(y=1.96/np.sqrt(len(x_diff)),linestyle= -- ,color= gray ) plt.title( Autocorrelation Function (q=1) )

#Plot PACF: plt.subplot(122) plt.plot(lag_pacf) plt.axhline(y=0,linestyle= -- ,color= gray )

(continues on next page)

4.4. Time Series in python

139

Statistics and Machine Learning in Python, Release 0.2
(continued from previous page) plt.axhline(y=-1.96/np.sqrt(len(x_diff)),linestyle= -- ,color= gray ) plt.axhline(y=1.96/np.sqrt(len(x_diff)),linestyle= -- ,color= gray ) plt.title( Partial Autocorrelation Function (p=1) ) plt.tight_layout()

In this plot, the two dotted lines on either sides of 0 are the conÔ¨Ådence interevals. These can be used to determine the p and q values as:
‚Ä¢ p: The lag value where the PACF chart crosses the upper conÔ¨Ådence interval for the Ô¨Årst time, in this case p=1.
‚Ä¢ q: The lag value where the ACF chart crosses the upper conÔ¨Ådence interval for the Ô¨Årst time, in this case q=1.
Fit ARMA model with statsmodels
1. DeÔ¨Åne the model by calling ARMA() and passing in the p and q parameters. 2. The model is prepared on the training data by calling the fit() function. 3. Predictions can be made by calling the predict() function and specifying the index of the
time or times to be predicted.
from statsmodels.tsa.arima_model import ARMA
model = ARMA(x, order=(1, 1)).fit() # fit model
print(model.summary()) plt.plot(x) plt.plot(model.predict(), color= red ) plt.title( RSS: %.4f % sum((model.fittedvalues-x)**2))

140

Chapter 4. Statistics

Statistics and Machine Learning in Python, Release 0.2

/home/edouard/anaconda3/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_ Àì‚Üímodel.py:171: ValueWarning: No frequency information was provided, so inferred‚ê£ Àì‚Üífrequency MS will be used.
% freq, ValueWarning) /home/edouard/anaconda3/lib/python3.7/site-packages/statsmodels/tsa/base/ Àì‚Üítsa_model.py:191: FutureWarning: Creating a DatetimeIndex by passing range‚ê£ Àì‚Üíendpoints is deprecated. Use pandas.date_range instead.
start=index[0], end=index[-1], freq=freq)

ARMA Model Results

==============================================================================

Dep. Variable:

gym No. Observations:

168

Model:

ARMA(1, 1) Log Likelihood

-436.852

Method:

css-mle S.D. of innovations

3.229

Date:

Thu, 16 May 2019 AIC

881.704

Time:

20:15:20 BIC

894.200

Sample:

01-01-2004 HQIC

886.776

- 12-01-2017

==============================================================================

coef std err

z

P>|z|

[0.025

0.975]

------------------------------------------------------------------------------

const

36.4315

8.827

4.127

0.000

19.131

53.732

ar.L1.gym

0.9967

0.005 220.566

0.000

0.988

1.006

ma.L1.gym -0.7494

0.054 -13.931

0.000

-0.855

-0.644

Roots

=============================================================================

Real

Imaginary

Modulus

Frequency

-----------------------------------------------------------------------------

AR.1

1.0033

+0.0000j

1.0033

0.0000

MA.1

1.3344

+0.0000j

1.3344

0.0000

-----------------------------------------------------------------------------

Text(0.5, 1.0, RSS: 1794.4651 )

4.4. Time Series in python

141

Statistics and Machine Learning in Python, Release 0.2

142

Chapter 4. Statistics

CHAPTER
FIVE
MACHINE LEARNING

5.1 Dimension reduction and feature extraction

5.1.1 Introduction

In machine learning and statistics, dimensionality reduction or dimension reduction is the process of reducing the number of features under consideration, and can be divided into feature selection (not addressed here) and feature extraction.

Feature extraction starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations. Feature extraction is related to dimensionality reduction.

The input matrix X, of dimension Ì†µÌ±Å √ó Ì†µÌ±É , is

‚é° Ì†µÌ±•11 . . . Ì†µÌ±•1Ì†µÌ±É ‚é§

‚é¢

‚é•

‚é¢ ‚é¢ ‚é¢

...

X

...

‚é• ‚é• ‚é•

‚é¢

‚é•

‚é£

‚é¶

Ì†µÌ±•Ì†µÌ±Å1 . . . Ì†µÌ±•Ì†µÌ±ÅÌ†µÌ±É

where the rows represent the samples and columns represent the variables.

The goal is to learn a transformation that extracts a few relevant features. This is generally done by exploiting the covariance Œ£XX between the input features.

5.1.2 Singular value decomposition and matrix factorization
Matrix factorization principles
Decompose the data matrix XÌ†µÌ±Å√óÌ†µÌ±É into a product of a mixing matrix UÌ†µÌ±Å√óÌ†µÌ∞æ and a dictionary matrix VÌ†µÌ±É √óÌ†µÌ∞æ .
X = UVÌ†µÌ±á ,
If we consider only a subset of components Ì†µÌ∞æ < Ì†µÌ±üÌ†µÌ±éÌ†µÌ±õÌ†µÌ±ò(X) < min(Ì†µÌ±É, Ì†µÌ±Å ‚àí 1) , X is approximated by a matrix XÀÜ :
X ‚âà XÀÜ = UVÌ†µÌ±á ,

143

Statistics and Machine Learning in Python, Release 0.2
Each line of xi is a linear combination (mixing ui) of dictionary items V. Ì†µÌ±Å Ì†µÌ±É -dimensional data points lie in a space whose dimension is less than Ì†µÌ±Å ‚àí 1 (2 dots lie on a line, 3 on a plane, etc.).

Fig. 1: Matrix factorization

Singular value decomposition (SVD) principles

Singular-value decomposition (SVD) factorises the data matrix XÌ†µÌ±Å√óÌ†µÌ±É into a product: X = UDVÌ†µÌ±á ,

where

‚é° Ì†µÌ±•11

Ì†µÌ±•1Ì†µÌ±É ‚é§ ‚é° Ì†µÌ±¢11

Ì†µÌ±¢1Ì†µÌ∞æ ‚é§

‚é¢

‚é¢ ‚é¢

X

‚é•‚é¢

‚é• ‚é•

=

‚é¢ ‚é¢

U

‚é• ‚é°Ì†µÌ±ë1

0 ‚é§ ‚é° Ì†µÌ±£11

Ì†µÌ±£1Ì†µÌ±É ‚é§

‚é• ‚é•‚é£

D

‚é¶‚é£

VÌ†µÌ±á

‚é¶.

‚é¢

‚é•‚é¢

‚é•

‚é£

‚é¶‚é£

‚é¶0

Ì†µÌ±ëÌ†µÌ∞æ Ì†µÌ±£Ì†µÌ∞æ1

Ì†µÌ±£Ì†µÌ∞æÌ†µÌ±É

Ì†µÌ±•Ì†µÌ±Å 1

Ì†µÌ±•Ì†µÌ±Å Ì†µÌ±É

Ì†µÌ±¢Ì†µÌ±Å 1

Ì†µÌ±¢Ì†µÌ±Å Ì†µÌ∞æ

U: right-singular

‚Ä¢ V = [v1, ¬∑ ¬∑ ¬∑ , vÌ†µÌ∞æ] is a Ì†µÌ±É √ó Ì†µÌ∞æ orthogonal matrix.
‚Ä¢ It is a dictionary of patterns to be combined (according to the mixing coefÔ¨Åcients) to reconstruct the original samples.

‚Ä¢ V perfoms the initial rotations (projection) along the Ì†µÌ∞æ = min(Ì†µÌ±Å, Ì†µÌ±É ) principal component directions, also called loadings.

‚Ä¢ Each vÌ†µÌ±ó performs the linear combination of the variables that has maximum sample variance, subject to being uncorrelated with the previous vÌ†µÌ±ó‚àí1.
D: singular values

‚Ä¢ D is a Ì†µÌ∞æ √ó Ì†µÌ∞æ diagonal matrix made of the singular values of X with Ì†µÌ±ë1 ‚â• Ì†µÌ±ë2 ‚â• ¬∑ ¬∑ ¬∑ ‚â• Ì†µÌ±ëÌ†µÌ∞æ ‚â• 0.
‚Ä¢ D scale the projection along the coordinate axes by Ì†µÌ±ë1, Ì†µÌ±ë2, ¬∑ ¬∑ ¬∑ , Ì†µÌ±ëÌ†µÌ∞æ.

144

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2
‚Ä¢ Singular values are the square roots of the eigenvalues of XÌ†µÌ±á X. V: left-singular vectors
‚Ä¢ U = [u1, ¬∑ ¬∑ ¬∑ , uÌ†µÌ∞æ] is an Ì†µÌ±Å √ó Ì†µÌ∞æ orthogonal matrix. ‚Ä¢ Each row vi provides the mixing coefÔ¨Åcients of dictionary items to reconstruct the sample
xi ‚Ä¢ It may be understood as the coordinates on the new orthogonal basis (obtained after the
initial rotation) called principal components in the PCA.
SVD for variables transformation
V transforms correlated variables (X) into a set of uncorrelated ones (UD) that better expose the various relationships among the original data items.

X = UDVÌ†µÌ±á , XV = UDVÌ†µÌ±á V, XV = UDI, XV = UD

(5.1) (5.2) (5.3) (5.4)

At the same time, SVD is a method for identifying and ordering the dimensions along which data points exhibit the most variation.
import numpy as np import scipy from sklearn.decomposition import PCA import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline
np.random.seed(42)
# dataset n_samples = 100 experience = np.random.normal(size=n_samples) salary = 1500 + experience + np.random.normal(size=n_samples, scale=.5) X = np.column_stack([experience, salary])
# PCA using SVD X -= X.mean(axis=0) # Centering is required U, s, Vh = scipy.linalg.svd(X, full_matrices=False) # U : Unitary matrix having left singular vectors as columns. # Of shape (n_samples,n_samples) or (n_samples,n_comps), depending on # full_matrices. # # s : The singular values, sorted in non-increasing order. Of shape (n_comps,), # with n_comps = min(n_samples, n_features). # # Vh: Unitary matrix having right singular vectors as rows. # Of shape (n_features, n_features) or (n_comps, n_features) depending # on full_matrices.
(continues on next page)

5.1. Dimension reduction and feature extraction

145

Statistics and Machine Learning in Python, Release 0.2
(continued from previous page)
plt.figure(figsize=(9, 3))
plt.subplot(131) plt.scatter(U[:, 0], U[:, 1], s=50) plt.axis( equal ) plt.title("U: Rotated and scaled data")
plt.subplot(132)
# Project data PC = np.dot(X, Vh.T) plt.scatter(PC[:, 0], PC[:, 1], s=50) plt.axis( equal ) plt.title("XV: Rotated data") plt.xlabel("PC1") plt.ylabel("PC2")
plt.subplot(133) plt.scatter(X[:, 0], X[:, 1], s=50) for i in range(Vh.shape[0]):
plt.arrow(x=0, y=0, dx=Vh[i, 0], dy=Vh[i, 1], head_width=0.2, head_length=0.2, linewidth=2, fc= r , ec= r )
plt.text(Vh[i, 0], Vh[i, 1], v%i % (i+1), color="r", fontsize=15, horizontalalignment= right , verticalalignment= top )
plt.axis( equal ) plt.ylim(-4, 4)
plt.title("X: original data (v1, v2:PC dir.)") plt.xlabel("experience") plt.ylabel("salary")
plt.tight_layout()

5.1.3 Principal components analysis (PCA)
Sources: ‚Ä¢ C. M. Bishop Pattern Recognition and Machine Learning, Springer, 2006 ‚Ä¢ Everything you did and didn‚Äôt know about PCA ‚Ä¢ Principal Component Analysis in 3 Simple Steps

146

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

Principles
‚Ä¢ Principal components analysis is the main method used for linear dimension reduction.
‚Ä¢ The idea of principal component analysis is to Ô¨Ånd the Ì†µÌ∞æ principal components directions (called the loadings) VÌ†µÌ∞æ√óÌ†µÌ±É that capture the variation in the data as much as possible.
‚Ä¢ It converts a set of Ì†µÌ±Å Ì†µÌ±É -dimensional observations NÌ†µÌ±Å√óÌ†µÌ±É of possibly correlated variables into a set of Ì†µÌ±Å Ì†µÌ∞æ-dimensional samples CÌ†µÌ±Å√óÌ†µÌ∞æ, where the Ì†µÌ∞æ < Ì†µÌ±É . The new variables are linearly uncorrelated. The columns of CÌ†µÌ±Å√óÌ†µÌ∞æ are called the principal components.
‚Ä¢ The dimension reduction is obtained by using only Ì†µÌ∞æ < Ì†µÌ±É components that exploit correlation (covariance) among the original variables.
‚Ä¢ PCA is mathematically deÔ¨Åned as an orthogonal linear transformation VÌ†µÌ∞æ√óÌ†µÌ±É that transforms the data to a new coordinate system such that the greatest variance by some projection of the data comes to lie on the Ô¨Årst coordinate (called the Ô¨Årst principal component), the second greatest variance on the second coordinate, and so on.
CÌ†µÌ±Å√óÌ†µÌ∞æ = XÌ†µÌ±Å√óÌ†µÌ±É VÌ†µÌ±É √óÌ†µÌ∞æ
‚Ä¢ PCA can be thought of as Ô¨Åtting a Ì†µÌ±É -dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. If some axis of the ellipse is small, then the variance along that axis is also small, and by omitting that axis and its corresponding principal component from our representation of the dataset, we lose only a commensurately small amount of information.
‚Ä¢ Finding the Ì†µÌ∞æ largest axes of the ellipse will permit to project the data onto a space having dimensionality Ì†µÌ∞æ < Ì†µÌ±É while maximizing the variance of the projected data.

Dataset preprocessing
Centering
Consider a data matrix, X , with column-wise zero empirical mean (the sample mean of each column has been shifted to zero), ie. X is replaced by X ‚àí 1x¬ØÌ†µÌ±á .

Standardizing
Optionally, standardize the columns, i.e., scale them by their standard-deviation. Without standardization, a variable with a high variance will capture most of the effect of the PCA. The principal direction will be aligned with this variable. Standardization will, however, raise noise variables to the save level as informative variables. The covariance matrix of centered standardized data is the correlation matrix.

Eigendecomposition of the data covariance matrix

To begin with, consider the projection onto a one-dimensional space (Ì†µÌ∞æ = 1). We can deÔ¨Åne the direction of this space using a Ì†µÌ±É -dimensional vector v, which for convenience (and without loss of generality) we shall choose to be a unit vector so that ‚Äñv‚Äñ2 = 1 (note that we are only

5.1. Dimension reduction and feature extraction

147

Statistics and Machine Learning in Python, Release 0.2

interested in the direction deÔ¨Åned by v, not in the magnitude of v itself). PCA consists of two mains steps:
Projection in the directions that capture the greatest variance
Each Ì†µÌ±É -dimensional data point xÌ†µÌ±ñ is then projected onto v, where the coordinate (in the coordinate system of v) is a scalar value, namely xÌ†µÌ†µÌ±ñÌ±á v. I.e., we want to Ô¨Ånd the vector v that maximizes these coordinates along v, which we will see corresponds to maximizing the variance of the projected data. This is equivalently expressed as

v

=

arg max
‚Äñv‚Äñ=1

1 Ì†µÌ±Å

‚àëÔ∏Å (Ô∏ÄxÌ†µÌ†µÌ±ñÌ±á v)Ô∏Ä2 .

Ì†µÌ±ñ

We can write this in matrix form as

v

=

arg max
‚Äñv‚Äñ=1

1 Ì†µÌ±Å

‚ÄñXv‚Äñ2

=

1 Ì†µÌ±Å

vÌ†µÌ±á XÌ†µÌ±á Xv

=

vÌ†µÌ±á SXXv,

where SXX is a biased estiamte of the covariance matrix of the data, i.e.

SXX

=

1 XÌ†µÌ±á X. Ì†µÌ±Å

We now maximize the projected variance vÌ†µÌ±á SXXv with respect to v. Clearly, this has to be a constrained maximization to prevent ‚Äñv2‚Äñ ‚Üí ‚àû. The appropriate constraint comes from the normalization condition ‚Äñv‚Äñ2 ‚â° ‚Äñv‚Äñ22 = vÌ†µÌ±á v = 1. To enforce this constraint, we introduce a Lagrange multiplier that we shall denote by Ì†µÌºÜ, and then make an unconstrained maximization
of

vÌ†µÌ±á SXXv ‚àí Ì†µÌºÜ(vÌ†µÌ±á v ‚àí 1).

By setting the gradient with respect to v equal to zero, we see that this quantity has a stationary point when

SXXv = Ì†µÌºÜv.

We note that v is an eigenvector of SXX.
If we left-multiply the above equation by vÌ†µÌ±á and make use of vÌ†µÌ±á v = 1, we see that the variance is given by

vÌ†µÌ±á SXXv = Ì†µÌºÜ,

and so the variance will be at a maximum when v is equal to the eigenvector corresponding to the largest eigenvalue, Ì†µÌºÜ. This eigenvector is known as the Ô¨Årst principal component.
We can deÔ¨Åne additional principal components in an incremental fashion by choosing each new direction to be that which maximizes the projected variance amongst all possible directions that are orthogonal to those already considered. If we consider the general case of a Ì†µÌ∞æ-dimensional projection space, the optimal linear projection for which the variance of the projected data is maximized is now deÔ¨Åned by the Ì†µÌ∞æ eigenvectors, v1, . . . , vK, of the data covariance matrix SXX that corresponds to the Ì†µÌ∞æ largest eigenvalues, Ì†µÌºÜ1 ‚â• Ì†µÌºÜ2 ‚â• ¬∑ ¬∑ ¬∑ ‚â• Ì†µÌºÜÌ†µÌ∞æ.

148

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

Back to SVD

The sample covariance matrix of centered data X is given by

SXX

=

Ì†µÌ±Å

1 ‚àí

XÌ†µÌ±á X. 1

We rewrite XÌ†µÌ±á X using the SVD decomposition of X as

XÌ†µÌ±á X = (UDVÌ†µÌ±á )Ì†µÌ±á (UDVÌ†µÌ±á )

= VDÌ†µÌ±á UÌ†µÌ±á UDVÌ†µÌ±á

= VD2VÌ†µÌ±á

VÌ†µÌ±á XÌ†µÌ±á XV = D2

1 VÌ†µÌ±á XÌ†µÌ±á XV = 1 D2

Ì†µÌ±Å ‚àí 1

Ì†µÌ±Å ‚àí 1

VÌ†µÌ±á SXXV

=

Ì†µÌ±Å

1 ‚àí

D2 1

.

Considering only the Ì†µÌ±òÌ†µÌ±°‚Ñé right-singular vectors vÌ†µÌ±ò associated to the singular value Ì†µÌ±ëÌ†µÌ±ò

vkÌ†µÌ±á SXXvk

=

Ì†µÌ±Å

1 ‚àí

1 Ì†µÌ±ë2Ì†µÌ±ò,

It turns out that if you have done the singular value decomposition then you already have

the Eigenvalue decomposition for XÌ†µÌ±á X. Where - The eigenvectors of SXX are equivalent to

the right singular vectors, V, of X. - The eigenvalues, Ì†µÌºÜÌ†µÌ±ò, of SXX, i.e. the variances of the

components,

are

equal

to

1 Ì†µÌ±Å ‚àí1

times

the

squared

singular

values,

Ì†µÌ±ëÌ†µÌ±ò .

Moreover computing PCA with SVD do not require to form the matrix XÌ†µÌ±á X, so computing the SVD is now the standard way to calculate a principal components analysis from a data matrix, unless only a handful of components are required.

PCA outputs
The SVD or the eigendecomposition of the data covariance matrix provides three main quantities:
1. Principal component directions or loadings are the eigenvectors of XÌ†µÌ±á X. The VÌ†µÌ∞æ√óÌ†µÌ±É or the right-singular vectors of an SVD of X are called principal component directions of X. They are generally computed using the SVD of X.
2. Principal components is the Ì†µÌ±Å √ó Ì†µÌ∞æ matrix C which is obtained by projecting X onto the principal components directions, i.e.
CÌ†µÌ±Å√óÌ†µÌ∞æ = XÌ†µÌ±Å√óÌ†µÌ±É VÌ†µÌ±É √óÌ†µÌ∞æ . Since X = UDVÌ†µÌ±á and V is orthogonal (VÌ†µÌ±á V = I):

5.1. Dimension reduction and feature extraction

149

Statistics and Machine Learning in Python, Release 0.2

CÌ†µÌ±Å√óÌ†µÌ∞æ = UDVÌ†µÌ†µÌ±áÌ±Å√óÌ†µÌ±É VÌ†µÌ±É √óÌ†µÌ∞æ CÌ†µÌ±Å√óÌ†µÌ∞æ = UDÌ†µÌ†µÌ±áÌ±Å√óÌ†µÌ∞æ IÌ†µÌ∞æ√óÌ†µÌ∞æ CÌ†µÌ±Å√óÌ†µÌ∞æ = UDÌ†µÌ†µÌ±áÌ±Å√óÌ†µÌ∞æ

(5.5) (5.6) (5.7) (5.8)

Thus cÌ†µÌ±ó = XvÌ†µÌ±ó = uÌ†µÌ±óÌ†µÌ±ëÌ†µÌ±ó, for Ì†µÌ±ó = 1, . . . Ì†µÌ∞æ. Hence uÌ†µÌ±ó is simply the projection of the row vectors of X, i.e., the input predictor vectors, on the direction vÌ†µÌ±ó, scaled by Ì†µÌ±ëÌ†µÌ±ó.

‚é° Ì†µÌ±•1,1Ì†µÌ±£1,1 + . . . + Ì†µÌ±•1,Ì†µÌ±É Ì†µÌ±£1,Ì†µÌ±É ‚é§

‚é¢ Ì†µÌ±•2,1Ì†µÌ±£1,1 + . . . + Ì†µÌ±•2,Ì†µÌ±É Ì†µÌ±£1,Ì†µÌ±É ‚é•

c1

=

‚é¢ ‚é¢

‚é£

...

‚é• ‚é• ‚é¶

Ì†µÌ±•Ì†µÌ±Å,1Ì†µÌ±£1,1 + . . . + Ì†µÌ±•Ì†µÌ±Å,Ì†µÌ±É Ì†µÌ±£1,Ì†µÌ±É

3. The variance of each component is given by the eigen values Ì†µÌºÜÌ†µÌ±ò, Ì†µÌ±ò = 1, . . . Ì†µÌ∞æ. It can be obtained from the singular values:

Ì†µÌ±£Ì†µÌ±éÌ†µÌ±ü(cÌ†µÌ±ò)

=

Ì†µÌ±Å

1 ‚àí

1

(XvÌ†µÌ±ò )2

=

Ì†µÌ±Å

1 ‚àí

1

(uÌ†µÌ±ò Ì†µÌ±ëÌ†µÌ±ò )2

=

Ì†µÌ±Å

1 ‚àí

1

Ì†µÌ±ë2Ì†µÌ±ò

(5.9) (5.10) (5.11)

Determining the number of PCs

We must choose Ì†µÌ∞æ* ‚àà [1, . . . , Ì†µÌ∞æ], the number of required components. This can be done by calculating the explained variance ratio of the Ì†µÌ∞æ* Ô¨Årst components and by choosing Ì†µÌ∞æ* such that the cumulative explained variance ratio is greater than some given threshold (e.g., ‚âà 90%). This is expressed as

cumulative

explained

variance(cÌ†µÌ±ò)

=

‚àëÔ∏ÄÌ†µÌ∞æ*
Ì†µÌ±ó
‚àëÔ∏ÄÌ†µÌ∞æ
Ì†µÌ±ó

Ì†µÌ±£Ì†µÌ±éÌ†µÌ±ü(cÌ†µÌ±ò) Ì†µÌ±£Ì†µÌ±éÌ†µÌ±ü(cÌ†µÌ±ò)

.

Interpretation and visualization
PCs Plot the samples projeted on Ô¨Årst the principal components as e.g. PC1 against PC2. PC directions Exploring the loadings associated with a component provides the contribution of each original variable in the component. Remark: The loadings (PC directions) are the coefÔ¨Åcients of multiple regression of PC on original variables:

150

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

c = Xv XÌ†µÌ±á c = XÌ†µÌ±á Xv (XÌ†µÌ±á X)‚àí1XÌ†µÌ±á c = v

(5.12) (5.13) (5.14)

Another way to evaluate the contribution of the original variables in each PC can be obtained by computing the correlation between the PCs and the original variables, i.e. columns of X, denoted xÌ†µÌ±ó, for Ì†µÌ±ó = 1, . . . , Ì†µÌ±É . For the Ì†µÌ±òÌ†µÌ±°‚Ñé PC, compute and plot the correlations with all original variables
Ì†µÌ±êÌ†µÌ±úÌ†µÌ±ü(cÌ†µÌ±ò, xÌ†µÌ±ó), Ì†µÌ±ó = 1 . . . Ì†µÌ∞æ, Ì†µÌ±ó = 1 . . . Ì†µÌ∞æ.
These quantities are sometimes called the correlation loadings.
import numpy as np from sklearn.decomposition import PCA import matplotlib.pyplot as plt
np.random.seed(42)
# dataset n_samples = 100 experience = np.random.normal(size=n_samples) salary = 1500 + experience + np.random.normal(size=n_samples, scale=.5) X = np.column_stack([experience, salary])
# PCA with scikit-learn pca = PCA(n_components=2) pca.fit(X) print(pca.explained_variance_ratio_)
PC = pca.transform(X)
plt.subplot(121) plt.scatter(X[:, 0], X[:, 1]) plt.xlabel("x1"); plt.ylabel("x2")
plt.subplot(122) plt.scatter(PC[:, 0], PC[:, 1]) plt.xlabel("PC1 (var=%.2f)" % pca.explained_variance_ratio_[0]) plt.ylabel("PC2 (var=%.2f)" % pca.explained_variance_ratio_[1]) plt.axis( equal ) plt.tight_layout()

[0.93646607 0.06353393]

5.1. Dimension reduction and feature extraction

151

Statistics and Machine Learning in Python, Release 0.2

5.1.4 Multi-dimensional Scaling (MDS)

Resources:

‚Ä¢ http://www.stat.pitt.edu/sungkyu/course/2221Fall13/lec8_mds_combined.pdf

‚Ä¢ https://en.wikipedia.org/wiki/Multidimensional_scaling

‚Ä¢ Hastie, Tibshirani and Friedman (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. New York: Springer, Second Edition.

The purpose of MDS is to Ô¨Ånd a low-dimensional projection of the data in which the pairwise distances between data points is preserved, as closely as possible (in a least-squares sense).

‚Ä¢ Let D be the (Ì†µÌ±Å √ó Ì†µÌ±Å ) pairwise distance matrix where Ì†µÌ±ëÌ†µÌ±ñÌ†µÌ±ó is a distance between points Ì†µÌ±ñ and Ì†µÌ±ó.

‚Ä¢ The MDS concept can be extended to a wide variety of data types speciÔ¨Åed in terms of a similarity matrix.

Given the dissimilarity (distance) matrix DÌ†µÌ±Å√óÌ†µÌ±Å = [Ì†µÌ±ëÌ†µÌ±ñÌ†µÌ±ó], MDS attempts to Ô¨Ånd Ì†µÌ∞æ-dimensional projections of the Ì†µÌ±Å points x1, . . . , xÌ†µÌ±Å ‚àà RÌ†µÌ∞æ , concatenated in an XÌ†µÌ±Å√óÌ†µÌ∞æ matrix, so that Ì†µÌ±ëÌ†µÌ±ñÌ†µÌ±ó ‚âà ‚ÄñxÌ†µÌ±ñ ‚àí xÌ†µÌ±ó‚Äñ are as close as possible. This can be obtained by the minimization of a loss function called the stress function
stress(X) = ‚àëÔ∏Å (Ì†µÌ±ëÌ†µÌ±ñÌ†µÌ±ó ‚àí ‚ÄñxÌ†µÌ±ñ ‚àí xÌ†µÌ±ó‚Äñ)2 .
Ì†µÌ±ñÃ∏=Ì†µÌ±ó

This loss function is known as least-squares or Kruskal-Shepard scaling.

A modiÔ¨Åcation of least-squares scaling is the Sammon mapping

stressSammon(X)

=

‚àëÔ∏Å
Ì†µÌ±ñÃ∏=Ì†µÌ±ó

(Ì†µÌ±ëÌ†µÌ±ñÌ†µÌ±ó

‚àí

‚ÄñxÌ†µÌ±ñ ‚àí Ì†µÌ±ëÌ†µÌ±ñÌ†µÌ±ó

xÌ†µÌ±ó ‚Äñ)2

.

152

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

The Sammon mapping performs better at preserving small distances compared to the leastsquares scaling.

Classical multidimensional scaling
Also known as principal coordinates analysis, PCoA.
‚Ä¢ The distance matrix, D, is transformed to a similarity matrix, B, often using centered inner products.
‚Ä¢ The loss function becomes stressclassical(X) = ‚àëÔ∏Å (Ô∏ÄÌ†µÌ±èÌ†µÌ±ñÌ†µÌ±ó ‚àí ‚ü®xÌ†µÌ±ñ, xÌ†µÌ±ó‚ü©)Ô∏Ä2.
Ì†µÌ±ñÃ∏=Ì†µÌ±ó
‚Ä¢ The stress function in classical MDS is sometimes called strain.
‚Ä¢ The solution for the classical MDS problems can be found from the eigenvectors of the similarity matrix.
‚Ä¢ If the distances in D are Euclidean and double centered inner products are used, the results are equivalent to PCA.

Example
The eurodist datset provides the road distances (in kilometers) between 21 cities in Europe. Given this matrix of pairwise (non-Euclidean) distances D = [Ì†µÌ±ëÌ†µÌ±ñÌ†µÌ±ó], MDS can be used to recover the coordinates of the cities in some Euclidean referential whose orientation is arbitrary.
import pandas as pd import numpy as np import matplotlib.pyplot as plt
# Pairwise distance between European cities try:
url = ../datasets/eurodist.csv df = pd.read_csv(url) except: url = https://raw.github.com/neurospin/pystatsml/master/datasets/eurodist.csv df = pd.read_csv(url)
print(df.iloc[:5, :5])
city = df["city"] D = np.array(df.iloc[:, 1:]) # Distance matrix
# Arbitrary choice of K=2 components from sklearn.manifold import MDS mds = MDS(dissimilarity= precomputed , n_components=2, random_state=40, max_iter=3000,‚ê£ Àì‚Üíeps=1e-9) X = mds.fit_transform(D)

city Athens Barcelona Brussels Calais

0 Athens

0

3313

2963 3175

1 Barcelona 3313

0

1318 1326

(continues on next page)

5.1. Dimension reduction and feature extraction

153

Statistics and Machine Learning in Python, Release 0.2

2 Brussels 3 Calais 4 Cherbourg

2963 3175 3339

1318 1326 1294

0 204

204

0

583 460

(continued from previous page)

Recover coordinates of the cities in Euclidean referential whose orientation is arbitrary:
from sklearn import metrics Deuclidean = metrics.pairwise.pairwise_distances(X, metric= euclidean ) print(np.round(Deuclidean[:5, :5]))

[[ 0. 3116. 2994. 3181. 3428.] [3116. 0. 1317. 1289. 1128.] [2994. 1317. 0. 198. 538.] [3181. 1289. 198. 0. 358.] [3428. 1128. 538. 358. 0.]]

Plot the results:
# Plot: apply some rotation and flip theta = 80 * np.pi / 180. rot = np.array([[np.cos(theta), -np.sin(theta)],
[np.sin(theta), np.cos(theta)]]) Xr = np.dot(X, rot) # flip x Xr[:, 0] *= -1 plt.scatter(Xr[:, 0], Xr[:, 1])
for i in range(len(city)): plt.text(Xr[i, 0], Xr[i, 1], city[i])
plt.axis( equal )

(-1894.1017744377398, 2914.3652937179477, -1712.9885463201906, 2145.4522453884565)

154

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

Determining the number of components
We must choose Ì†µÌ∞æ* ‚àà {1, . . . , Ì†µÌ∞æ} the number of required components. Plotting the values of the stress function, obtained using Ì†µÌ±ò ‚â§ Ì†µÌ±Å ‚àí 1 components. In general, start with 1, . . . Ì†µÌ∞æ ‚â§ 4. Choose Ì†µÌ∞æ* where you can clearly distinguish an elbow in the stress curve.
Thus, in the plot below, we choose to retain information accounted for by the Ô¨Årst two components, since this is where the elbow is in the stress curve.
k_range = range(1, min(5, D.shape[0]-1)) stress = [MDS(dissimilarity= precomputed , n_components=k,
random_state=42, max_iter=300, eps=1e-9).fit(D).stress_ for k in k_range]
print(stress) plt.plot(k_range, stress) plt.xlabel("k") plt.ylabel("stress")
[48644495.28571428, 3356497.365752386, 2858455.495887962, 2756310.637628011]
Text(0, 0.5, stress )

5.1. Dimension reduction and feature extraction

155

Statistics and Machine Learning in Python, Release 0.2

5.1.5 Nonlinear dimensionality reduction
Sources: ‚Ä¢ Scikit-learn documentation ‚Ä¢ Wikipedia
Nonlinear dimensionality reduction or manifold learning cover unsupervised methods that attempt to identify low-dimensional manifolds within the original Ì†µÌ±É -dimensional space that represent high data density. Then those methods provide a mapping from the high-dimensional space to the low-dimensional embedding.

Isomap

Isomap is a nonlinear dimensionality reduction method that combines a procedure to compute the distance matrix with MDS. The distances calculation is based on geodesic distances evaluated on neighborhood graph:
1. Determine the neighbors of each point. All points in some Ô¨Åxed radius or K nearest neighbors.
2. Construct a neighborhood graph. Each point is connected to other if it is a K nearest neighbor. Edge length equal to Euclidean distance.
3. Compute shortest path between pairwise of points Ì†µÌ±ëÌ†µÌ±ñÌ†µÌ±ó to build the distance matrix D.
4. Apply MDS on D.

import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from sklearn import manifold, datasets

(continues on next page)

156

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2
(continued from previous page)
X, color = datasets.samples_generator.make_s_curve(1000, random_state=42)
fig = plt.figure(figsize=(10, 5)) plt.suptitle("Isomap Manifold Learning", fontsize=14)
ax = fig.add_subplot(121, projection= 3d ) ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral) ax.view_init(4, -72) plt.title( 2D "S shape" manifold in 3D )
Y = manifold.Isomap(n_neighbors=10, n_components=2).fit_transform(X) ax = fig.add_subplot(122) plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral) plt.title("Isomap") plt.xlabel("First component") plt.ylabel("Second component") plt.axis( tight )
(-5.4164373180970316, 5.276311544714793, -1.2910940054965336, 1.23497771017066)

5.1.6 Exercises
PCA Write a basic PCA class Write a class BasicPCA with two methods:

5.1. Dimension reduction and feature extraction

157

Statistics and Machine Learning in Python, Release 0.2
‚Ä¢ fit(X) that estimates the data mean, principal components directions V and the explained variance of each component.
‚Ä¢ transform(X) that projects the data onto the principal components. Check that your BasicPCA gave similar results, compared to the results from sklearn.
Apply your Basic PCA on the iris dataset
The data set is available at: https://raw.github.com/neurospin/pystatsml/master/datasets/iris. csv
‚Ä¢ Describe the data set. Should the dataset been standardized? ‚Ä¢ Describe the structure of correlations among variables. ‚Ä¢ Compute a PCA with the maximum number of components. ‚Ä¢ Compute the cumulative explained variance ratio. Determine the number of components
Ì†µÌ∞æ by your computed values. ‚Ä¢ Print the Ì†µÌ∞æ principal components directions and correlations of the Ì†µÌ∞æ principal compo-
nents with the original variables. Interpret the contribution of the original variables into the PC. ‚Ä¢ Plot the samples projected into the Ì†µÌ∞æ Ô¨Årst PCs. ‚Ä¢ Color samples by their species.
MDS
Apply MDS from sklearn on the iris dataset available at: https://raw.github.com/neurospin/pystatsml/master/datasets/iris.csv
‚Ä¢ Center and scale the dataset. ‚Ä¢ Compute Euclidean pairwise distances matrix. ‚Ä¢ Select the number of components. ‚Ä¢ Show that classical MDS on Euclidean pairwise distances matrix is equivalent to PCA.
5.2 Clustering
Wikipedia: Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). Clustering is one of the main task of exploratory data mining, and a common technique for statistical data analysis, used in many Ô¨Åelds, including machine learning, pattern recognition, image analysis, information retrieval, and bioinformatics. Sources: http://scikit-learn.org/stable/modules/clustering.html

158

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

5.2.1 K-means clustering

Source: C. M. Bishop Pattern Recognition and Machine Learning, Springer, 2006
Suppose we have a data set Ì†µÌ±ã = {Ì†µÌ±•1, ¬∑ ¬∑ ¬∑ , Ì†µÌ±•Ì†µÌ±Å } that consists of Ì†µÌ±Å observations of a random Ì†µÌ∞∑-dimensional Euclidean variable Ì†µÌ±•. Our goal is to partition the data set into some number, Ì†µÌ∞æ, of clusters, where we shall suppose for the moment that the value of Ì†µÌ∞æ is given. Intuitively, we might think of a cluster as comprising a group of data points whose inter-point distances are small compared to the distances to points outside of the cluster. We can formalize this notion by Ô¨Årst introducing a set of Ì†µÌ∞∑-dimensional vectors Ì†µÌºáÌ†µÌ±ò, where Ì†µÌ±ò = 1, . . . , Ì†µÌ∞æ, in which Ì†µÌºáÌ†µÌ±ò is a prototype associated with the Ì†µÌ±òÌ†µÌ±°‚Ñé cluster. As we shall see shortly, we can think of the Ì†µÌºáÌ†µÌ±ò as representing the centres of the clusters. Our goal is then to Ô¨Ånd an assignment of data points to clusters, as well as a set of vectors {Ì†µÌºáÌ†µÌ±ò}, such that the sum of the squares of the distances of each data point to its closest prototype vector Ì†µÌºáÌ†µÌ±ò, is at a minimum.
It is convenient at this point to deÔ¨Åne some notation to describe the assignment of data points to clusters. For each data point Ì†µÌ±•Ì†µÌ±ñ , we introduce a corresponding set of binary indicator variables Ì†µÌ±üÌ†µÌ±ñÌ†µÌ±ò ‚àà {0, 1}, where Ì†µÌ±ò = 1, . . . , Ì†µÌ∞æ, that describes which of the Ì†µÌ∞æ clusters the data point Ì†µÌ±•Ì†µÌ±ñ is assigned to, so that if data point Ì†µÌ±•Ì†µÌ±ñ is assigned to cluster Ì†µÌ±ò then Ì†µÌ±üÌ†µÌ±ñÌ†µÌ±ò = 1, and Ì†µÌ±üÌ†µÌ±ñÌ†µÌ±ó = 0 for Ì†µÌ±ó Ã∏= Ì†µÌ±ò. This is known as the 1-of-Ì†µÌ∞æ coding scheme. We can then deÔ¨Åne an objective function, denoted inertia, as
Ì†µÌ±Å Ì†µÌ∞æ
Ì†µÌ∞Ω (Ì†µÌ±ü, Ì†µÌºá) = ‚àëÔ∏Å ‚àëÔ∏Å Ì†µÌ±üÌ†µÌ±ñÌ†µÌ±ò‚ÄñÌ†µÌ±•Ì†µÌ±ñ ‚àí Ì†µÌºáÌ†µÌ±ò‚Äñ22
Ì†µÌ±ñ Ì†µÌ±ò
which represents the sum of the squares of the Euclidean distances of each data point to its assigned vector Ì†µÌºáÌ†µÌ±ò. Our goal is to Ô¨Ånd values for the {Ì†µÌ±üÌ†µÌ±ñÌ†µÌ±ò} and the {Ì†µÌºáÌ†µÌ±ò} so as to minimize the function Ì†µÌ∞Ω. We can do this through an iterative procedure in which each iteration involves two successive steps corresponding to successive optimizations with respect to the Ì†µÌ±üÌ†µÌ±ñÌ†µÌ±ò and the Ì†µÌºáÌ†µÌ±ò . First we choose some initial values for the Ì†µÌºáÌ†µÌ±ò. Then in the Ô¨Årst phase we minimize Ì†µÌ∞Ω with respect to the Ì†µÌ±üÌ†µÌ±ñÌ†µÌ±ò, keeping the Ì†µÌºáÌ†µÌ±ò Ô¨Åxed. In the second phase we minimize Ì†µÌ∞Ω with respect to the Ì†µÌºáÌ†µÌ±ò, keeping Ì†µÌ±üÌ†µÌ±ñÌ†µÌ±ò Ô¨Åxed. This two-stage optimization process is then repeated until convergence. We shall see that these two stages of updating Ì†µÌ±üÌ†µÌ±ñÌ†µÌ±ò and Ì†µÌºáÌ†µÌ±ò correspond respectively to the expectation (E) and maximization (M) steps of the expectation-maximisation (EM) algorithm, and to emphasize this we shall use the terms E step and M step in the context of the Ì†µÌ∞æ-means algorithm.
Consider Ô¨Årst the determination of the Ì†µÌ±üÌ†µÌ±ñÌ†µÌ±ò . Because Ì†µÌ∞Ω in is a linear function of Ì†µÌ±üÌ†µÌ±ñÌ†µÌ±ò , this optimization can be performed easily to give a closed form solution. The terms involving different Ì†µÌ±ñ are independent and so we can optimize for each Ì†µÌ±ñ separately by choosing Ì†µÌ±üÌ†µÌ±ñÌ†µÌ±ò to be 1 for whichever value of Ì†µÌ±ò gives the minimum value of ||Ì†µÌ±•Ì†µÌ±ñ ‚àí Ì†µÌºáÌ†µÌ±ò||2 . In other words, we simply assign the Ì†µÌ±ñth data point to the closest cluster centre. More formally, this can be expressed as

{Ô∏É 1,
Ì†µÌ±üÌ†µÌ±ñÌ†µÌ±ò = 0,

if Ì†µÌ±ò = arg minÌ†µÌ±ó ||Ì†µÌ±•Ì†µÌ±ñ ‚àí Ì†µÌºáÌ†µÌ±ó||2. otherwise.

(5.15)

Now consider the optimization of the Ì†µÌºáÌ†µÌ±ò with the Ì†µÌ±üÌ†µÌ±ñÌ†µÌ±ò held Ô¨Åxed. The objective function Ì†µÌ∞Ω is a quadratic function of Ì†µÌºáÌ†µÌ±ò, and it can be minimized by setting its derivative with respect to Ì†µÌºáÌ†µÌ±ò to zero giving
‚àëÔ∏Å 2 Ì†µÌ±üÌ†µÌ±ñÌ†µÌ±ò(Ì†µÌ±•Ì†µÌ±ñ ‚àí Ì†µÌºáÌ†µÌ±ò) = 0
Ì†µÌ±ñ

5.2. Clustering

159

Statistics and Machine Learning in Python, Release 0.2

which we can easily solve for Ì†µÌºáÌ†µÌ±ò to give

Ì†µÌºáÌ†µÌ±ò

=

‚àëÔ∏Ä
Ì†µÌ±ñ

Ì†µÌ±üÌ†µÌ±ñÌ†µÌ±ò

Ì†µÌ±•Ì†µÌ±ñ

‚àëÔ∏Ä
Ì†µÌ±ñ

Ì†µÌ±üÌ†µÌ±ñÌ†µÌ±ò

.

The denominator in this expression is equal to the number of points assigned to cluster Ì†µÌ±ò, and so this result has a simple interpretation, namely set Ì†µÌºáÌ†µÌ±ò equal to the mean of all of the data points Ì†µÌ±•Ì†µÌ±ñ assigned to cluster Ì†µÌ±ò. For this reason, the procedure is known as the Ì†µÌ∞æ-means algorithm.

The two phases of re-assigning data points to clusters and re-computing the cluster means are repeated in turn until there is no further change in the assignments (or until some maximum number of iterations is exceeded). Because each phase reduces the value of the objective function Ì†µÌ∞Ω, convergence of the algorithm is assured. However, it may converge to a local rather than global minimum of Ì†µÌ∞Ω.

from sklearn import cluster, datasets import matplotlib.pyplot as plt import seaborn as sns # nice color %matplotlib inline

iris = datasets.load_iris() X = iris.data[:, :2] # use only sepal length and sepal width y_iris = iris.target

km2 = cluster.KMeans(n_clusters=2).fit(X) km3 = cluster.KMeans(n_clusters=3).fit(X) km4 = cluster.KMeans(n_clusters=4).fit(X)

plt.figure(figsize=(9, 3)) plt.subplot(131) plt.scatter(X[:, 0], X[:, 1], c=km2.labels_) plt.title("K=2, J=%.2f" % km2.inertia_)

plt.subplot(132) plt.scatter(X[:, 0], X[:, 1], c=km3.labels_) plt.title("K=3, J=%.2f" % km3.inertia_)

plt.subplot(133) plt.scatter(X[:, 0], X[:, 1], c=km4.labels_)#.astype(np.float)) plt.title("K=4, J=%.2f" % km4.inertia_)

Text(0.5, 1.0, K=4, J=28.03 )

160

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2
Exercises
1. Analyse clusters
‚Ä¢ Analyse the plot above visually. What would a good value of Ì†µÌ∞æ be? ‚Ä¢ If you instead consider the inertia, the value of Ì†µÌ∞Ω, what would a good value of Ì†µÌ∞æ be? ‚Ä¢ Explain why there is such difference. ‚Ä¢ For Ì†µÌ∞æ = 2 why did Ì†µÌ∞æ-means clustering not Ô¨Ånd the two ‚Äúnatural‚Äù clusters? See
the assumptions of Ì†µÌ∞æ-means: http://scikit-learn.org/stable/auto_examples/cluster/plot_ kmeans_assumptions.html#example-cluster-plot-kmeans-assumptions-py
2. Re-implement the Ì†µÌ∞æ-means clustering algorithm (homework)
Write a function kmeans(X, K) that return an integer vector of the samples‚Äô labels.
5.2.2 Hierarchical clustering
Hierarchical clustering is an approach to clustering that build hierarchies of clusters in two main approaches:
‚Ä¢ Agglomerative: A bottom-up strategy, where each observation starts in their own cluster, and pairs of clusters are merged upwards in the hierarchy.
‚Ä¢ Divisive: A top-down strategy, where all observations start out in the same cluster, and then the clusters are split recursively downwards in the hierarchy.
In order to decide which clusters to merge or to split, a measure of dissimilarity between clusters is introduced. More speciÔ¨Åc, this comprise a distance measure and a linkage criterion. The distance measure is just what it sounds like, and the linkage criterion is essentially a function of the distances between points, for instance the minimum distance between points in two clusters, the maximum distance between points in two clusters, the average distance between points in two clusters, etc. One particular linkage criterion, the Ward criterion, will be discussed next.
Ward clustering
Ward clustering belongs to the family of agglomerative hierarchical clustering algorithms. This means that they are based on a ‚Äúbottoms up‚Äù approach: each sample starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy. In Ward clustering, the criterion for choosing the pair of clusters to merge at each step is the minimum variance criterion. Ward‚Äôs minimum variance criterion minimizes the total withincluster variance by each merge. To implement this method, at each step: Ô¨Ånd the pair of clusters that leads to minimum increase in total within-cluster variance after merging. This increase is a weighted squared distance between cluster centers. The main advantage of agglomerative hierarchical clustering over Ì†µÌ∞æ-means clustering is that you can beneÔ¨Åt from known neighborhood information, for example, neighboring pixels in an image.

5.2. Clustering

161

Statistics and Machine Learning in Python, Release 0.2
from sklearn import cluster, datasets import matplotlib.pyplot as plt import seaborn as sns # nice color
iris = datasets.load_iris() X = iris.data[:, :2] # sepal length (cm) sepal width (cm) y_iris = iris.target
ward2 = cluster.AgglomerativeClustering(n_clusters=2, linkage= ward ).fit(X) ward3 = cluster.AgglomerativeClustering(n_clusters=3, linkage= ward ).fit(X) ward4 = cluster.AgglomerativeClustering(n_clusters=4, linkage= ward ).fit(X)
plt.figure(figsize=(9, 3)) plt.subplot(131) plt.scatter(X[:, 0], X[:, 1], c=ward2.labels_) plt.title("K=2")
plt.subplot(132) plt.scatter(X[:, 0], X[:, 1], c=ward3.labels_) plt.title("K=3")
plt.subplot(133) plt.scatter(X[:, 0], X[:, 1], c=ward4.labels_) # .astype(np.float)) plt.title("K=4")
Text(0.5, 1.0, K=4 )

5.2.3 Gaussian mixture models
The Gaussian mixture model (GMM) is a simple linear superposition of Gaussian components over the data, aimed at providing a rich class of density models. We turn to a formulation of Gaussian mixtures in terms of discrete latent variables: the Ì†µÌ∞æ hidden classes to be discovered.
Differences compared to Ì†µÌ∞æ-means:
‚Ä¢ Whereas the Ì†µÌ∞æ-means algorithm performs a hard assignment of data points to clusters, in which each data point is associated uniquely with one cluster, the GMM algorithm makes a soft assignment based on posterior probabilities.
‚Ä¢ Whereas the classic Ì†µÌ∞æ-means is only based on Euclidean distances, classic GMM use a

162

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

Mahalanobis distances that can deal with non-spherical distributions. It should be noted that Mahalanobis could be plugged within an improved version of Ì†µÌ∞æ-Means clustering. The Mahalanobis distance is unitless and scale-invariant, and takes into account the correlations of the data set.
The Gaussian mixture distribution can be written as a linear superposition of Ì†µÌ∞æ Gaussians in the form:

Ì†µÌ∞æ
‚àëÔ∏Å Ì†µÌ±ù(Ì†µÌ±•) = Ì†µÌ≤© (Ì†µÌ±• | Ì†µÌºáÌ†µÌ±ò, Œ£Ì†µÌ±ò)Ì†µÌ±ù(Ì†µÌ±ò),
Ì†µÌ±ò=1

where:

‚Ä¢ The Ì†µÌ±ù(Ì†µÌ±ò) are the mixing coefÔ¨Åcients also know as the class probability of class Ì†µÌ±ò, and they

sum

to

one:

‚àëÔ∏ÄÌ†µÌ∞æ
Ì†µÌ±ò=1

Ì†µÌ±ù(Ì†µÌ±ò)

=

1.

‚Ä¢ Ì†µÌ≤© (Ì†µÌ±• | Ì†µÌºáÌ†µÌ±ò, Œ£Ì†µÌ±ò) = Ì†µÌ±ù(Ì†µÌ±• | Ì†µÌ±ò) is the conditional distribution of Ì†µÌ±• given a particular class Ì†µÌ±ò. It is the multivariate Gaussian distribution deÔ¨Åned over a Ì†µÌ±É -dimensional vector Ì†µÌ±• of continuous variables.

The goal is to maximize the log-likelihood of the GMM:

Ì†µÌ±Å

Ì†µÌ±Å {Ô∏É Ì†µÌ∞æ

}Ô∏É Ì†µÌ±Å {Ô∏É Ì†µÌ∞æ

}Ô∏É

‚àèÔ∏Å

‚àèÔ∏Å ‚àëÔ∏Å

‚àëÔ∏Å ‚àëÔ∏Å

ln Ì†µÌ±ù(Ì†µÌ±•Ì†µÌ±ñ) = ln

Ì†µÌ≤© (Ì†µÌ±•Ì†µÌ±ñ | Ì†µÌºáÌ†µÌ±ò, Œ£Ì†µÌ±ò)Ì†µÌ±ù(Ì†µÌ±ò) = ln

Ì†µÌ≤© (Ì†µÌ±•Ì†µÌ±ñ | Ì†µÌºáÌ†µÌ±ò, Œ£Ì†µÌ±ò)Ì†µÌ±ù(Ì†µÌ±ò) .

Ì†µÌ±ñ=1

Ì†µÌ±ñ=1 Ì†µÌ±ò=1

Ì†µÌ±ñ=1

Ì†µÌ±ò=1

To compute the classes parameters: Ì†µÌ±ù(Ì†µÌ±ò), Ì†µÌºáÌ†µÌ±ò, Œ£Ì†µÌ±ò we sum over all samples, by weighting each

sample Ì†µÌ±ñ by its responsibility or contribution to class Ì†µÌ±ò: Ì†µÌ±ù(Ì†µÌ±ò | Ì†µÌ±•Ì†µÌ±ñ) such that for each point its

contribution

to

all

classes

sum

to

one

‚àëÔ∏Ä
Ì†µÌ±ò

Ì†µÌ±ù(Ì†µÌ±ò

|

Ì†µÌ±•Ì†µÌ±ñ)

=

1.

This contribution is the conditional

probability of class Ì†µÌ±ò given Ì†µÌ±•: Ì†µÌ±ù(Ì†µÌ±ò | Ì†µÌ±•) (sometimes called the posterior). It can be computed

using Bayes‚Äô rule:

Ì†µÌ±ù(Ì†µÌ±• | Ì†µÌ±ò)Ì†µÌ±ù(Ì†µÌ±ò) Ì†µÌ±ù(Ì†µÌ±ò | Ì†µÌ±•) =
Ì†µÌ±ù(Ì†µÌ±•)

=

Ì†µÌ≤© (Ì†µÌ±• | Ì†µÌºáÌ†µÌ±ò, Œ£Ì†µÌ±ò)Ì†µÌ±ù(Ì†µÌ±ò)

‚àëÔ∏ÄÌ†µÌ∞æ
Ì†µÌ±ò=1

Ì†µÌ≤©

(Ì†µÌ±•

|

Ì†µÌºáÌ†µÌ±ò

,

Œ£Ì†µÌ±ò

)Ì†µÌ±ù(Ì†µÌ±ò)

(5.16) (5.17)

Since the class parameters, Ì†µÌ±ù(Ì†µÌ±ò), Ì†µÌºáÌ†µÌ±ò and Œ£Ì†µÌ±ò, depend on the responsibilities Ì†µÌ±ù(Ì†µÌ±ò | Ì†µÌ±•) and the responsibilities depend on class parameters, we need a two-step iterative algorithm: the expectation-maximization (EM) algorithm. We discuss this algorithm next.

### The expectation-maximization (EM) algorithm for Gaussian mixtures

Given a Gaussian mixture model, the goal is to maximize the likelihood function with respect to the parameters (comprised of the means and covariances of the components and the mixing coefÔ¨Åcients).

Initialize the means Ì†µÌºáÌ†µÌ±ò, covariances Œ£Ì†µÌ±ò and mixing coefÔ¨Åcients Ì†µÌ±ù(Ì†µÌ±ò)

1. E step. For each sample Ì†µÌ±ñ, evaluate the responsibilities for each class Ì†µÌ±ò using the current parameter values

Ì†µÌ±ù(Ì†µÌ±ò

| Ì†µÌ±•Ì†µÌ±ñ)

=

Ì†µÌ≤© (Ì†µÌ±•Ì†µÌ±ñ | Ì†µÌºáÌ†µÌ±ò, Œ£Ì†µÌ±ò)Ì†µÌ±ù(Ì†µÌ±ò)

‚àëÔ∏ÄÌ†µÌ∞æ
Ì†µÌ±ò=1

Ì†µÌ≤©

(Ì†µÌ±•Ì†µÌ±ñ

|

Ì†µÌºáÌ†µÌ±ò

,

Œ£Ì†µÌ±ò

)Ì†µÌ±ù(Ì†µÌ±ò)

2. M step. For each class, re-estimate the parameters using the current responsibilities

5.2. Clustering

163

Statistics and Machine Learning in Python, Release 0.2

Ì†µÌºánÌ†µÌ±òew

=

1 Ì†µÌ±ÅÌ†µÌ±ò

Ì†µÌ±Å
‚àëÔ∏Å Ì†µÌ±ù(Ì†µÌ±ò | Ì†µÌ±•Ì†µÌ±ñ)Ì†µÌ±•Ì†µÌ±ñ
Ì†µÌ±ñ=1

Œ£nÌ†µÌ±òew

=

1 Ì†µÌ±ÅÌ†µÌ±ò

Ì†µÌ±Å
‚àëÔ∏Å Ì†µÌ±ù(Ì†µÌ±ò | Ì†µÌ±•Ì†µÌ±ñ)(Ì†µÌ±•Ì†µÌ±ñ
Ì†µÌ±ñ=1

‚àí Ì†µÌºánÌ†µÌ±òew)(Ì†µÌ±•Ì†µÌ±ñ

‚àí Ì†µÌºánÌ†µÌ±òew)Ì†µÌ±á

Ì†µÌ±ùnew(Ì†µÌ±ò) = Ì†µÌ±ÅÌ†µÌ±ò Ì†µÌ±Å

(5.18) (5.19) (5.20)

3. Evaluate the log-likelihood

Ì†µÌ±Å {Ô∏É Ì†µÌ∞æ

}Ô∏É

‚àëÔ∏Å ‚àëÔ∏Å

ln

Ì†µÌ≤© (Ì†µÌ±•|Ì†µÌºáÌ†µÌ±ò, Œ£Ì†µÌ±ò)Ì†µÌ±ù(Ì†µÌ±ò) ,

Ì†µÌ±ñ=1

Ì†µÌ±ò=1

and check for convergence of either the parameters or the log-likelihood. If the convergence criterion is not satisÔ¨Åed return to step 1.

import numpy as np from sklearn import datasets import matplotlib.pyplot as plt import seaborn as sns # nice color import sklearn from sklearn.mixture import GaussianMixture

import pystatsml.plot_utils

colors = sns.color_palette()

iris = datasets.load_iris() X = iris.data[:, :2] # sepal length (cm) sepal width (cm) y_iris = iris.target

gmm2 = GaussianMixture(n_components=2, covariance_type= full ).fit(X) gmm3 = GaussianMixture(n_components=3, covariance_type= full ).fit(X) gmm4 = GaussianMixture(n_components=4, covariance_type= full ).fit(X)

plt.figure(figsize=(9, 3)) plt.subplot(131) plt.scatter(X[:, 0], X[:, 1], c=[colors[lab] for lab in gmm2.predict(X)])#, color=colors) for i in range(gmm2.covariances_.shape[0]):
pystatsml.plot_utils.plot_cov_ellipse(cov=gmm2.covariances_[i, :], pos=gmm2.means_[i,‚ê£ Àì‚Üí:],
facecolor= none , linewidth=2, edgecolor=colors[i]) plt.scatter(gmm2.means_[i, 0], gmm2.means_[i, 1], edgecolor=colors[i],
marker="o", s=100, facecolor="w", linewidth=2) plt.title("K=2")

plt.subplot(132) plt.scatter(X[:, 0], X[:, 1], c=[colors[lab] for lab in gmm3.predict(X)]) for i in range(gmm3.covariances_.shape[0]):
pystatsml.plot_utils.plot_cov_ellipse(cov=gmm3.covariances_[i, :], pos=gmm3.means_[i,‚ê£ Àì‚Üí:],
facecolor= none , linewidth=2, edgecolor=colors[i]) plt.scatter(gmm3.means_[i, 0], gmm3.means_[i, 1], edgecolor=colors[i],
marker="o", s=100, facecolor="w", linewidth=2)
(continues on next page)

164

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

plt.title("K=3")

(continued from previous page)

plt.subplot(133) plt.scatter(X[:, 0], X[:, 1], c=[colors[lab] for lab in gmm4.predict(X)]) # .astype(np. Àì‚Üífloat)) for i in range(gmm4.covariances_.shape[0]):
pystatsml.plot_utils.plot_cov_ellipse(cov=gmm4.covariances_[i, :], pos=gmm4.means_[i,‚ê£ Àì‚Üí:],
facecolor= none , linewidth=2, edgecolor=colors[i]) plt.scatter(gmm4.means_[i, 0], gmm4.means_[i, 1], edgecolor=colors[i],
marker="o", s=100, facecolor="w", linewidth=2) _ = plt.title("K=4")

5.2.4 Model selection

### Bayesian information criterion
In statistics, the Bayesian information criterion (BIC) is a criterion for model selection among a Ô¨Ånite set of models; the model with the lowest BIC is preferred. It is based, in part, on the likelihood function and it is closely related to the Akaike information criterion (AIC).
X = iris.data y_iris = iris.target

bic = list() #print(X)

ks = np.arange(1, 10)

for k in ks: gmm = GaussianMixture(n_components=k, covariance_type= full ) gmm.fit(X) bic.append(gmm.bic(X))

k_chosen = ks[np.argmin(bic)]

plt.plot(ks, bic) plt.xlabel("k") plt.ylabel("BIC")

(continues on next page)

5.2. Clustering

165

Statistics and Machine Learning in Python, Release 0.2
print("Choose k=", k_chosen) Choose k= 2

(continued from previous page)

5.2.5 Exercises
Perform clustering of the iris dataset based on all variables using Gaussian mixture models. Use PCA to visualize clusters.
5.3 Linear methods for regression
5.3.1 Ordinary least squares
Linear regression models the output, or target variable Ì†µÌ±¶ ‚àà R as a linear combination of the (Ì†µÌ±É ‚àí 1)-dimensional input Ì†µÌ±• ‚àà R(Ì†µÌ±É ‚àí1). Let X be the Ì†µÌ±Å √ó Ì†µÌ±É matrix with each row an input vector (with a 1 in the Ô¨Årst position), and similarly let Ì†µÌ±¶ be the Ì†µÌ±Å -dimensional vector of outputs in the training set, the linear model will predict the y given X using the parameter vector, or weight vector Ì†µÌªΩ ‚àà RÌ†µÌ±É according to
y = XÌ†µÌªΩ + Ì†µÌºÄ,
where Ì†µÌºÄ ‚àà RÌ†µÌ±Å are the residuals, or the errors of the prediction. The Ì†µÌªΩ is found by minimizing an objective function, which is the loss function, ‚Ñí(Ì†µÌªΩ), i.e. the error measured on the data. This error is the sum of squared errors (SSE) loss. Minimizing the SSE is the Ordinary Least Square OLS regression as objective function.

166

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

OLS(Ì†µÌªΩ) = ‚Ñí(Ì†µÌªΩ)

= SSE(Ì†µÌªΩ)

Ì†µÌ±Å

=

‚àëÔ∏Å (Ì†µÌ±¶Ì†µÌ±ñ

‚àí

xÌ†µÌ†µÌ±ñÌ±á

Ì†µÌªΩ)2

Ì†µÌ±ñ

= (y ‚àí XÌ†µÌªΩ)Ì†µÌ±á (y ‚àí XÌ†µÌªΩ)

= ‚Äñy ‚àí XÌ†µÌªΩ‚Äñ22,

which is a simple ordinary least squares (OLS) minimization.

(5.21) (5.22)
(5.23)
(5.24) (5.25)

5.3.2 Linear regression with scikit-learn
Scikit learn offer many models for supervised learning, and they all follow the same application programming interface (API), namely:
model = Estimator() model.fit(X, y) predictions = model.predict(X)
%matplotlib inline import warnings warnings.filterwarnings(action= once )

from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt import numpy as np import pandas as pd import sklearn.linear_model as lm import sklearn.metrics as metrics %matplotlib inline

# Fit Ordinary Least Squares: OLS csv = pd.read_csv( https://raw.githubusercontent.com/neurospin/pystatsml/master/datasets/ Àì‚ÜíAdvertising.csv , index_col=0) X = csv[[ TV , Radio ]] y = csv[ Sales ]

lr = lm.LinearRegression().fit(X, y) y_pred = lr.predict(X) print("R-squared =", metrics.r2_score(y, y_pred))

print("Coefficients =", lr.coef_)

# Plot fig = plt.figure() ax = fig.add_subplot(111, projection= 3d )

ax.scatter(csv[ TV ], csv[ Radio ], csv[ Sales ], c= r , marker= o )

xx1, xx2 = np.meshgrid( np.linspace(csv[ TV ].min(), csv[ TV ].max(), num=10),

(continues on next page)

5.3. Linear methods for regression

167

Statistics and Machine Learning in Python, Release 0.2
(continued from previous page) np.linspace(csv[ Radio ].min(), csv[ Radio ].max(), num=10))
XX = np.column_stack([xx1.ravel(), xx2.ravel()])
yy = lr.predict(XX) ax.plot_surface(xx1, xx2, yy.reshape(xx1.shape), color= None ) ax.set_xlabel( TV ) ax.set_ylabel( Radio ) _ = ax.set_zlabel( Sales )
/home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£ Àì‚Üígot 216 from PyObject
return f(*args, **kwds) /home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£ Àì‚Üígot 216 from PyObject
return f(*args, **kwds) /home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£ Àì‚Üígot 216 from PyObject
return f(*args, **kwds) /home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£ Àì‚Üígot 216 from PyObject
return f(*args, **kwds) /home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£ Àì‚Üígot 216 from PyObject
return f(*args, **kwds) /home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£ Àì‚Üígot 216 from PyObject
return f(*args, **kwds)
R-squared = 0.8971942610828956 Coefficients = [0.04575482 0.18799423]

168

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

5.3.3 OverÔ¨Åtting
In statistics and machine learning, overÔ¨Åtting occurs when a statistical model describes random errors or noise instead of the underlying relationships. OverÔ¨Åtting generally occurs when a model is excessively complex, such as having too many parameters relative to the number of observations. A model that has been overÔ¨Åt will generally have poor predictive performance, as it can exaggerate minor Ô¨Çuctuations in the data.
A learning algorithm is trained using some set of training samples. If the learning algorithm has the capacity to overÔ¨Åt the training samples the performance on the training sample set will improve while the performance on unseen test sample set will decline.
The overÔ¨Åtting phenomenon has three main explanations: - excessively complex models, - multicollinearity, and - high dimensionality.
Model complexity
Complex learners with too many parameters relative to the number of observations may overÔ¨Åt the training dataset.
Multicollinearity
Predictors are highly correlated, meaning that one can be linearly predicted from the others. In this situation the coefÔ¨Åcient estimates of the multiple regression may change erratically in response to small changes in the model or the data. Multicollinearity does not reduce the predictive power or reliability of the model as a whole, at least not within the sample data set; it only affects computations regarding individual predictors. That is, a multiple regression model with correlated predictors can indicate how well the entire bundle of predictors predicts the outcome variable, but it may not give valid results about any individual predictor, or about which predictors are redundant with respect to others. In case of perfect multicollinearity the predictor matrix is singular and therefore cannot be inverted. Under these circumstances, for a

5.3. Linear methods for regression

169

Statistics and Machine Learning in Python, Release 0.2

general linear model y = XÌ†µÌªΩ + Ì†µÌºÄ, the ordinary least-squares estimator, Ì†µÌªΩÌ†µÌ±ÇÌ†µÌ∞øÌ†µÌ±Ü = (XÌ†µÌ±á X)‚àí1XÌ†µÌ±á y, does not exist.
An example where correlated predictor may produce an unstable model follows:

import numpy as np from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt

bv = np.array([10, 20, 30, 40, 50])

# business volume

tax = .2 * bv

# Tax

bp = .1 * bv + np.array([-.1, .2, .1, -.2, .1]) # business potential

X = np.column_stack([bv, tax]) beta_star = np.array([.1, 0]) # true solution

Since tax and bv are correlated, there is an infinite number of linear combinations leading to the same prediction.

# 10 times the bv then subtract it 9 times using the tax variable: beta_medium = np.array([.1 * 10, -.1 * 9 * (1/.2)]) # 100 times the bv then subtract it 99 times using the tax variable: beta_large = np.array([.1 * 100, -.1 * 99 * (1/.2)])
# Check that all model lead to the same result assert np.all(np.dot(X, beta_star) == np.dot(X, beta_medium)) assert np.all(np.dot(X, beta_star) == np.dot(X, beta_large))

Multicollinearity between the predictors:

business volumes and

tax produces unstable models with arbitrary large coefÔ¨Åcients.

170

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

Dealing with multicollinearity: ‚Ä¢ Regularisation by e.g. ‚Ñì2 shrinkage: Introduce a bias in the solution by making (Ì†µÌ±ãÌ†µÌ±á Ì†µÌ±ã)‚àí1 non-singular. See ‚Ñì2 shrinkage.
‚Ä¢ Feature selection: select a small number of features. See: Isabelle Guyon and Andr√© Elisseeff An introduction to variable and feature selection The Journal of Machine Learning Research, 2003.
‚Ä¢ Feature selection: select a small number of features using ‚Ñì1 shrinkage.
‚Ä¢ Extract few independent (uncorrelated) features using e.g. principal components analysis (PCA), partial least squares regression (PLS-R) or regression methods that cut the number of predictors to a smaller set of uncorrelated components.
High dimensionality
High dimensions means a large number of input features. Linear predictor associate one parameter to each input feature, so a high-dimensional situation (Ì†µÌ±É , number of features, is large) with a relatively small number of samples Ì†µÌ±Å (so-called large Ì†µÌ±É small Ì†µÌ±Å situation) generally lead to an overÔ¨Åt of the training data. Thus it is generally a bad idea to add many input features into the learner. This phenomenon is called the curse of dimensionality.
One of the most important criteria to use when choosing a learning algorithm is based on the relative size of Ì†µÌ±É and Ì†µÌ±Å .
‚Ä¢ Remenber that the ‚Äúcovariance‚Äù matrix XÌ†µÌ±á X used in the linear model is a Ì†µÌ±É √ó Ì†µÌ±É matrix of rank min(Ì†µÌ±Å, Ì†µÌ±É ). Thus if Ì†µÌ±É > Ì†µÌ±Å the equation system is overparameterized and admit an

5.3. Linear methods for regression

171

Statistics and Machine Learning in Python, Release 0.2

inÔ¨Ånity of solutions that might be speciÔ¨Åc to the learning dataset. See also ill-conditioned or singular matrices.

‚Ä¢ The sampling density of Ì†µÌ±Å samples in an Ì†µÌ±É -dimensional space is proportional to Ì†µÌ±Å 1/Ì†µÌ±É . Thus a high-dimensional space becomes very sparse, leading to poor estimations of samples densities.

‚Ä¢ Another consequence of the sparse sampling in high dimensions is that all sample points are close to an edge of the sample. Consider Ì†µÌ±Å data points uniformly distributed in a Ì†µÌ±É -dimensional unit ball centered at the origin. Suppose we consider a nearest-neighbor estimate at the origin. The median distance from the origin to the closest data point is given by the expression

(Ô∏Ç 1 Ì†µÌ±Å )Ô∏Ç1/Ì†µÌ±É

Ì†µÌ±ë(Ì†µÌ±É, Ì†µÌ±Å ) = 1 ‚àí

.

2

A more complicated expression exists for the mean distance to the closest point. For N = 500, P = 10 , Ì†µÌ±ë(Ì†µÌ±É, Ì†µÌ±Å ) ‚âà 0.52, more than halfway to the boundary. Hence most data points are closer to the boundary of the sample space than to any other data point. The reason that this presents a problem is that prediction is much more difÔ¨Åcult near the edges of the training sample. One must extrapolate from neighboring sample points rather than interpolate between them. (Source: T Hastie, R Tibshirani, J Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Second Edition, 2009.)

‚Ä¢ Structural risk minimization provides a theoretical background of this phenomenon. (See VC dimension.)

‚Ä¢ See also bias‚Äìvariance trade-off.

import seaborn # nicer plots

def fit_on_increasing_size(model): n_samples = 100 n_features_ = np.arange(10, 800, 20) r2_train, r2_test, snr = [], [], [] for n_features in n_features_: # Sample the dataset (* 2 nb of samples) n_features_info = int(n_features/10) np.random.seed(42) # Make reproducible X = np.random.randn(n_samples * 2, n_features) beta = np.zeros(n_features) beta[:n_features_info] = 1 Xbeta = np.dot(X, beta) eps = np.random.randn(n_samples * 2) y = Xbeta + eps # Split the dataset into train and test sample Xtrain, Xtest = X[:n_samples, :], X[n_samples:, :] ytrain, ytest = y[:n_samples], y[n_samples:] # fit/predict lr = model.fit(Xtrain, ytrain) y_pred_train = lr.predict(Xtrain) y_pred_test = lr.predict(Xtest) snr.append(Xbeta.std() / eps.std()) r2_train.append(metrics.r2_score(ytrain, y_pred_train)) r2_test.append(metrics.r2_score(ytest, y_pred_test)) return n_features_, np.array(r2_train), np.array(r2_test), np.array(snr)

(continues on next page)

172

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2
(continued from previous page) def plot_r2_snr(n_features_, r2_train, r2_test, xvline, snr, ax):
""" Two scales plot. Left y-axis: train test r-squared. Right y-axis SNR. """ ax.plot(n_features_, r2_train, label="Train r-squared", linewidth=2) ax.plot(n_features_, r2_test, label="Test r-squared", linewidth=2) ax.axvline(x=xvline, linewidth=2, color= k , ls= -- ) ax.axhline(y=0, linewidth=1, color= k , ls= -- ) ax.set_ylim(-0.2, 1.1) ax.set_xlabel("Number of input features") ax.set_ylabel("r-squared") ax.legend(loc= best ) ax.set_title("Prediction perf.") ax_right = ax.twinx() ax_right.plot(n_features_, snr, r- , label="SNR", linewidth=1) ax_right.set_ylabel("SNR", color= r ) for tl in ax_right.get_yticklabels():
tl.set_color( r )
# Model = linear regression mod = lm.LinearRegression()
# Fit models on dataset n_features, r2_train, r2_test, snr = fit_on_increasing_size(model=mod)
argmax = n_features[np.argmax(r2_test)]
# plot fig, axis = plt.subplots(1, 2, figsize=(9, 3))
# Left pane: all features plot_r2_snr(n_features, r2_train, r2_test, argmax, snr, axis[0])
# Right pane: Zoom on 100 first features plot_r2_snr(n_features[n_features <= 100],
r2_train[n_features <= 100], r2_test[n_features <= 100], argmax, snr[n_features <= 100], axis[1]) plt.tight_layout()
/home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 216, got 192
return f(*args, **kwds) /home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: ImportWarning: can t‚ê£ Àì‚Üíresolve package from __spec__ or __package__, falling back on __name__ and __path__
return f(*args, **kwds) /home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£ Àì‚Üígot 216 from PyObject
return f(*args, **kwds)

5.3. Linear methods for regression

173

Statistics and Machine Learning in Python, Release 0.2

Exercises
Study the code above and: ‚Ä¢ Describe the datasets: Ì†µÌ±Å : nb_samples, Ì†µÌ±É : nb_features. ‚Ä¢ What is n_features_info? ‚Ä¢ Give the equation of the generative model. ‚Ä¢ What is modiÔ¨Åed by the loop? ‚Ä¢ What is the SNR?
Comment the graph above, in terms of training and test performances: ‚Ä¢ How does the train and test performance changes as a function of Ì†µÌ±•? ‚Ä¢ Is it the expected results when compared to the SNR? ‚Ä¢ What can you conclude?

5.3.4 Ridge regression (‚Ñì2-regularization)
OverÔ¨Åtting generally leads to excessively complex weight vectors, accounting for noise or spurious correlations within predictors. To avoid this phenomenon the learning should constrain the solution in order to Ô¨Åt a global pattern. This constraint will reduce (bias) the capacity of the learning algorithm. Adding such a penalty will force the coefÔ¨Åcients to be small, i.e. to shrink them toward zeros. Therefore the loss function ‚Ñí(Ì†µÌªΩ) (generally the SSE) is combined with a penalty function ‚Ñ¶(Ì†µÌªΩ) leading to the general form:
Penalized(Ì†µÌªΩ) = ‚Ñí(Ì†µÌªΩ) + Ì†µÌºÜ‚Ñ¶(Ì†µÌªΩ)
The respective contribution of the loss and the penalty is controlled by the regularization parameter Ì†µÌºÜ. Ridge regression impose a ‚Ñì2 penalty on the coefÔ¨Åcients, i.e. it penalizes with the Euclidean norm of the coefÔ¨Åcients while minimizing SSE. The objective function becomes:
Ridge(Ì†µÌªΩ) = ‚Äñy ‚àí XÌ†µÌªΩ‚Äñ22 + Ì†µÌºÜ‚ÄñÌ†µÌªΩ‚Äñ22.
The Ì†µÌªΩ that minimises Ì†µÌ∞πÌ†µÌ±ÖÌ†µÌ±ñÌ†µÌ±ëÌ†µÌ±îÌ†µÌ±í(Ì†µÌªΩ) can be found by the following derivation:

174

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

‚àáÌ†µÌªΩRidge(Ì†µÌªΩ) = 0 ‚àáÌ†µÌªΩ(Ô∏Ä(y ‚àí XÌ†µÌªΩ)Ì†µÌ±á (y ‚àí XÌ†µÌªΩ) + Ì†µÌºÜÌ†µÌªΩÌ†µÌ±á Ì†µÌªΩ)Ô∏Ä = 0 ‚àáÌ†µÌªΩ(Ô∏Ä(yÌ†µÌ±á y ‚àí 2Ì†µÌªΩÌ†µÌ±á XÌ†µÌ±á y + Ì†µÌªΩÌ†µÌ±á XÌ†µÌ±á XÌ†µÌªΩ + Ì†µÌºÜÌ†µÌªΩÌ†µÌ±á Ì†µÌªΩ))Ô∏Ä = 0
‚àí2XÌ†µÌ±á y + 2XÌ†µÌ±á XÌ†µÌªΩ + 2Ì†µÌºÜÌ†µÌªΩ = 0 ‚àíXÌ†µÌ±á y + (XÌ†µÌ±á X + Ì†µÌºÜI)Ì†µÌªΩ = 0 (XÌ†µÌ±á X + Ì†µÌºÜI)Ì†µÌªΩ = XÌ†µÌ±á y Ì†µÌªΩ = (XÌ†µÌ±á X + Ì†µÌºÜI)‚àí1XÌ†µÌ±á y

(5.26) (5.27) (5.28) (5.29) (5.30) (5.31) (5.32)

‚Ä¢ The solution adds a positive constant to the diagonal of XÌ†µÌ±á X before inversion. This makes the problem nonsingular, even if XÌ†µÌ±á X is not of full rank, and was the main motivation
behind ridge regression.

‚Ä¢ Increasing Ì†µÌºÜ shrinks the Ì†µÌªΩ coefÔ¨Åcients toward 0.

‚Ä¢ This approach penalizes the objective function by the Euclidian (:math:‚Äòell_2‚Äò) norm of the coefÔ¨Åcients such that solutions with large coefÔ¨Åcients become unattractive.

The ridge penalty shrinks the coefÔ¨Åcients toward zero. The Ô¨Ågure illustrates: the OLS solution on the left. The ‚Ñì1 and ‚Ñì2 penalties in the middle pane. The penalized OLS in the right pane. The right pane shows how the penalties shrink the coefÔ¨Åcients toward zero. The black points are the minimum found in each case, and the white points represents the true solution used to generate the data.

Fig. 2: ‚Ñì1 and ‚Ñì2 shrinkages import matplotlib.pyplot as plt import numpy as np import sklearn.linear_model as lm
5.3. Linear methods for regression

(continues on next page)
175

Statistics and Machine Learning in Python, Release 0.2

# lambda is alpha! mod = lm.Ridge(alpha=10)

(continued from previous page)

# Fit models on dataset n_features, r2_train, r2_test, snr = fit_on_increasing_size(model=mod)

argmax = n_features[np.argmax(r2_test)]

# plot fig, axis = plt.subplots(1, 2, figsize=(9, 3))

# Left pane: all features plot_r2_snr(n_features, r2_train, r2_test, argmax, snr, axis[0])

# Right pane: Zoom on 100 first features plot_r2_snr(n_features[n_features <= 100],
r2_train[n_features <= 100], r2_test[n_features <= 100], argmax, snr[n_features <= 100], axis[1]) plt.tight_layout()

Exercice
What beneÔ¨Åt has been obtained by using ‚Ñì2 regularization?
5.3.5 Lasso regression (‚Ñì1-regularization)
Lasso regression penalizes the coefÔ¨Åcients by the ‚Ñì1 norm. This constraint will reduce (bias) the capacity of the learning algorithm. To add such a penalty forces the coefÔ¨Åcients to be small, i.e. it shrinks them toward zero. The objective function to minimize becomes:

Lasso(Ì†µÌªΩ) = ‚Äñy ‚àí XÌ†µÌªΩ‚Äñ22 + Ì†µÌºÜ‚ÄñÌ†µÌªΩ‚Äñ1.

(5.33)

This penalty forces some coefÔ¨Åcients to be exactly zero, providing a feature selection property.

176

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2
import matplotlib.pyplot as plt import numpy as np import sklearn.linear_model as lm
# lambda is alpha ! mod = lm.Lasso(alpha=.1)
# Fit models on dataset n_features, r2_train, r2_test, snr = fit_on_increasing_size(model=mod)
argmax = n_features[np.argmax(r2_test)]
# plot fig, axis = plt.subplots(1, 2, figsize=(9, 3))
# Left pane: all features plot_r2_snr(n_features, r2_train, r2_test, argmax, snr, axis[0])
# Right pane: Zoom on 200 first features plot_r2_snr(n_features[n_features <= 200],
r2_train[n_features <= 200], r2_test[n_features <= 200], argmax, snr[n_features <= 200], axis[1]) plt.tight_layout()

Sparsity of the ‚Ñì1 norm
Occam‚Äôs razor
Occam‚Äôs razor (also written as Ockham‚Äôs razor, and lex parsimoniae in Latin, which means law of parsimony) is a problem solving principle attributed to William of Ockham (1287-1347), who was an English Franciscan friar and scholastic philosopher and theologian. The principle can be interpreted as stating that among competing hypotheses, the one with the fewest assumptions should be selected.

Principle of parsimony
The simplest of two competing theories is to be preferred. DeÔ¨Ånition of parsimony: Economy of explanation in conformity with Occam‚Äôs razor.

5.3. Linear methods for regression

177

Statistics and Machine Learning in Python, Release 0.2

Among possible models with similar loss, choose the simplest one:

‚Ä¢ Choose the model with the smallest coefÔ¨Åcient vector, i.e. smallest ‚Ñì2 (‚ÄñÌ†µÌªΩ‚Äñ2) or ‚Ñì1 (‚ÄñÌ†µÌªΩ‚Äñ1) norm of Ì†µÌªΩ, i.e. ‚Ñì2 or ‚Ñì1 penalty. See also bias-variance tradeoff.
‚Ä¢ Choose the model that uses the smallest number of predictors. In other words, choose the model that has many predictors with zero weights. Two approaches are available to obtain this: (i) Perform a feature selection as a preprocessing prior to applying the learning algorithm, or (ii) embed the feature selection procedure within the learning process.

#### Sparsity-induced penalty or embedded feature selection with the ‚Ñì1 penalty
The penalty based on the ‚Ñì1 norm promotes sparsity (scattered, or not dense): it forces many coefÔ¨Åcients to be exactly zero. This also makes the coefÔ¨Åcient vector scattered.

The Ô¨Ågure bellow illustrates the OLS loss under a constraint acting on the ‚Ñì1 norm of the coefÔ¨Åcient vector. I.e., it illustrates the following optimization problem:

minimize
Ì†µÌªΩ

‚Äñy

‚àí

XÌ†µÌªΩ‚Äñ22

subject to ‚ÄñÌ†µÌªΩ‚Äñ1 ‚â§ 1.

Optimization issues Section to be completed 178

Fig. 3: Sparsity of L1 norm Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

‚Ä¢ No more closed-form solution. ‚Ä¢ Convex but not differentiable. ‚Ä¢ Requires speciÔ¨Åc optimization algorithms, such as the fast iterative shrinkage-thresholding
algorithm (FISTA): Amir Beck and Marc Teboulle, A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems SIAM J. Imaging Sci., 2009.
5.3.6 Elastic-net regression (‚Ñì2-‚Ñì1-regularization)
The Elastic-net estimator combines the ‚Ñì1 and ‚Ñì2 penalties, and results in the problem to

Enet(Ì†µÌªΩ) = ‚Äñy ‚àí XÌ†µÌ±á Ì†µÌªΩ‚Äñ22 + Ì†µÌªº (Ô∏ÄÌ†µÌºå ‚ÄñÌ†µÌªΩ‚Äñ1 + (1 ‚àí Ì†µÌºå) ‚ÄñÌ†µÌªΩ‚Äñ22)Ô∏Ä ,

(5.34)

where Ì†µÌªº acts as a global penalty and Ì†µÌºå as an ‚Ñì1/‚Ñì2 ratio.
### Rationale
‚Ä¢ If there are groups of highly correlated variables, Lasso tends to arbitrarily select only one from each group. These models are difÔ¨Åcult to interpret because covariates that are strongly associated with the outcome are not included in the predictive model. Conversely, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together.
‚Ä¢ Studies on real world data and simulation studies show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation.
import matplotlib.pyplot as plt import numpy as np import sklearn.linear_model as lm
mod = lm.ElasticNet(alpha=.5, l1_ratio=.5)
# Fit models on dataset n_features, r2_train, r2_test, snr = fit_on_increasing_size(model=mod)
argmax = n_features[np.argmax(r2_test)]
# plot fig, axis = plt.subplots(1, 2, figsize=(9, 3))
# Left pane: all features plot_r2_snr(n_features, r2_train, r2_test, argmax, snr, axis[0])
# Right pane: Zoom on 100 first features plot_r2_snr(n_features[n_features <= 100],
r2_train[n_features <= 100], r2_test[n_features <= 100], argmax, snr[n_features <= 100], axis[1]) plt.tight_layout()

5.3. Linear methods for regression

179

Statistics and Machine Learning in Python, Release 0.2

5.4 Linear classiÔ¨Åcation

Given a training set of Ì†µÌ±Å samples, Ì†µÌ∞∑ = {(Ì†µÌ±•1, Ì†µÌ±¶1), . . . , (Ì†µÌ±•Ì†µÌ±Å , Ì†µÌ±¶Ì†µÌ±Å )} , where Ì†µÌ±•Ì†µÌ±ñ is a multidimensional input vector with dimension Ì†µÌ±É and class label (target or response) Ì†µÌ±¶Ì†µÌ±ñ ‚àà {0, 1} (binary classiÔ¨Åcation problem).
The vector of parameters Ì†µÌ±§ performs a linear combination of the input variables, Ì†µÌ±•Ì†µÌ±á Ì†µÌ±§. This step performs a projection or a rotation of input sample into a good discriminative one-dimensional sub-space.
This score (a.k.a decision function) is tranformed, using the nonlinear activation funtion Ì†µÌ±¶(.), to a ‚Äúposterior probabilities‚Äù of class 1
Ì†µÌ±ù(Ì†µÌ±¶ = 1|Ì†µÌ±•) = Ì†µÌ±¶(Ì†µÌ±•Ì†µÌ±á Ì†µÌ±§), Ì†µÌ±§‚ÑéÌ†µÌ±íÌ†µÌ±üÌ†µÌ±í

Ì†µÌ±ù(Ì†µÌ±¶ = 1|Ì†µÌ±•) = 1 ‚àí Ì†µÌ±ù(Ì†µÌ±¶ = 0|Ì†µÌ±•).
The decision surfaces correspond to Ì†µÌ±¶(Ì†µÌ±•) = constant, so that Ì†µÌ±•Ì†µÌ±á Ì†µÌ±§ = constant and hence the decision surfaces are linear functions of Ì†µÌ±•, even if the function Ì†µÌ±ì (.) is nonlinear.
A thresholding of the activation provides the predicted class label.
The vector of parameters, that deÔ¨Ånes the discriminative axis, minimizes an objective function Ì†µÌ±ì that is a sum of of loss function ‚Ñí(Ì†µÌ±§) and some penalties on the weights vector ‚Ñ¶(Ì†µÌ±§).

min
Ì†µÌ±§

Ì†µÌ±ì

=

‚àëÔ∏Å ‚Ñí(Ì†µÌ±¶Ì†µÌ±ñ, Ì†µÌ±•Ì†µÌ±ñÌ†µÌ±á Ì†µÌ±§) +

‚Ñ¶(Ì†µÌ±§),

Ì†µÌ±ñ

5.4.1 Fisher‚Äôs linear discriminant with equal class covariance
This geometric method does not make any probabilistic assumptions, instead it relies on distances. It looks for the linear projection of the data points onto a vector, Ì†µÌ±§, that maximizes the between/within variance ratio, denoted Ì†µÌ∞π (Ì†µÌ±§). Under a few assumptions, it will provide the same results as linear discriminant analysis (LDA), explained below.
Suppose two classes of observations, Ì†µÌ∞∂0 and Ì†µÌ∞∂1, have means Ì†µÌºá0 and Ì†µÌºá1 and the same total within-class scatter (‚Äúcovariance‚Äù) matrix,

180

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

Ì†µÌ±ÜÌ†µÌ±ä

=

‚àëÔ∏Å (Ì†µÌ±•Ì†µÌ±ñ

‚àí Ì†µÌºá0)(Ì†µÌ±•Ì†µÌ±ñ ‚àí Ì†µÌºá0)Ì†µÌ±á

+

‚àëÔ∏Å (Ì†µÌ±•Ì†µÌ±ó

‚àí Ì†µÌºá1)(Ì†µÌ±•Ì†µÌ±ó

‚àí Ì†µÌºá1)Ì†µÌ±á

Ì†µÌ±ñ‚ààÌ†µÌ∞∂0

Ì†µÌ±ó‚ààÌ†µÌ∞∂1

= Ì†µÌ±ãÌ†µÌ±êÌ†µÌ±á Ì†µÌ±ãÌ†µÌ±ê,

(5.35) (5.36)

where Ì†µÌ±ãÌ†µÌ±ê is the (Ì†µÌ±Å √ó Ì†µÌ±É ) matrix of data centered on their respective means:

Ì†µÌ±ãÌ†µÌ±ê

=

[Ô∏ÇÌ†µÌ±ã0 Ì†µÌ±ã1

‚àí ‚àí

Ì†µÌºá0]Ô∏Ç Ì†µÌºá1

,

where Ì†µÌ±ã0 and Ì†µÌ±ã1 are the (Ì†µÌ±Å0 √ó Ì†µÌ±É ) and (Ì†µÌ±Å1 √ó Ì†µÌ±É ) matrices of samples of classes Ì†µÌ∞∂0 and Ì†µÌ∞∂1. Let Ì†µÌ±ÜÌ†µÌ∞µ being the scatter ‚Äúbetween-class‚Äù matrix, given by

Ì†µÌ±ÜÌ†µÌ∞µ = (Ì†µÌºá1 ‚àí Ì†µÌºá0)(Ì†µÌºá1 ‚àí Ì†µÌºá0)Ì†µÌ±á .

The linear combination of features Ì†µÌ±§Ì†µÌ±á Ì†µÌ±• have means Ì†µÌ±§Ì†µÌ±á Ì†µÌºáÌ†µÌ±ñ for Ì†µÌ±ñ = 0, 1, and variance Ì†µÌ±§Ì†µÌ±á Ì†µÌ±ãÌ†µÌ†µÌ±êÌ±á Ì†µÌ±ãÌ†µÌ±êÌ†µÌ±§. Fisher deÔ¨Åned the separation between these two distributions to be the ratio of the variance between the classes to the variance within the classes:

Ì†µÌ∞πFisher(Ì†µÌ±§)

=

Ì†µÌºéb2etween Ì†µÌºéw2 ithin

=

(Ì†µÌ±§Ì†µÌ±á Ì†µÌºá1 ‚àí Ì†µÌ±§Ì†µÌ±á Ì†µÌºá0)2 Ì†µÌ±§Ì†µÌ±á Ì†µÌ±ãÌ†µÌ†µÌ±êÌ±á Ì†µÌ±ãÌ†µÌ±êÌ†µÌ±§

=

(Ì†µÌ±§Ì†µÌ±á (Ì†µÌºá1 ‚àí Ì†µÌºá0))2 Ì†µÌ±§Ì†µÌ±á Ì†µÌ±ãÌ†µÌ†µÌ±êÌ±á Ì†µÌ±ãÌ†µÌ±êÌ†µÌ±§

=

Ì†µÌ±§Ì†µÌ±á (Ì†µÌºá1 ‚àí Ì†µÌºá0)(Ì†µÌºá1 ‚àí Ì†µÌºá0)Ì†µÌ±á Ì†µÌ±§ Ì†µÌ±§Ì†µÌ±á Ì†µÌ±ãÌ†µÌ†µÌ±êÌ±á Ì†µÌ±ãÌ†µÌ±êÌ†µÌ±§

=

Ì†µÌ±§Ì†µÌ±á Ì†µÌ±ÜÌ†µÌ∞µ Ì†µÌ±§Ì†µÌ±á Ì†µÌ±ÜÌ†µÌ±ä

Ì†µÌ±§ Ì†µÌ±§

.

(5.37) (5.38) (5.39) (5.40) (5.41)

The Fisher most discriminant projection
In the two-class case, the maximum separation occurs by a projection on the (Ì†µÌºá1 ‚àí Ì†µÌºá0) using the Mahalanobis metric Ì†µÌ±ÜÌ†µÌ±ä ‚àí1, so that
Ì†µÌ±§ ‚àù Ì†µÌ±ÜÌ†µÌ±ä ‚àí1(Ì†µÌºá1 ‚àí Ì†µÌºá0).

Demonstration Differentiating Ì†µÌ∞πFisher(Ì†µÌ±§) with respect to Ì†µÌ±§ gives

5.4. Linear classiÔ¨Åcation

181

Statistics and Machine Learning in Python, Release 0.2

‚àáÌ†µÌ±§Ì†µÌ∞πFisher(Ì†µÌ±§) = 0

‚àáÌ†µÌ±§

(Ô∏Ç Ì†µÌ±§Ì†µÌ±á Ì†µÌ±ÜÌ†µÌ∞µÌ†µÌ±§ )Ô∏Ç Ì†µÌ±§Ì†µÌ±á Ì†µÌ±ÜÌ†µÌ±ä Ì†µÌ±§

=

0

(Ì†µÌ±§Ì†µÌ±á Ì†µÌ±ÜÌ†µÌ±ä Ì†µÌ±§)(2Ì†µÌ±ÜÌ†µÌ∞µÌ†µÌ±§) ‚àí (Ì†µÌ±§Ì†µÌ±á Ì†µÌ±ÜÌ†µÌ∞µÌ†µÌ±§)(2Ì†µÌ±ÜÌ†µÌ±ä Ì†µÌ±§) = 0

(Ì†µÌ±§Ì†µÌ±á Ì†µÌ±ÜÌ†µÌ±ä Ì†µÌ±§)(Ì†µÌ±ÜÌ†µÌ∞µÌ†µÌ±§) = (Ì†µÌ±§Ì†µÌ±á Ì†µÌ±ÜÌ†µÌ∞µÌ†µÌ±§)(Ì†µÌ±ÜÌ†µÌ±ä Ì†µÌ±§)

Ì†µÌ±ÜÌ†µÌ∞µ Ì†µÌ±§

=

Ì†µÌ±§Ì†µÌ±á Ì†µÌ±ÜÌ†µÌ∞µÌ†µÌ±§ Ì†µÌ±§Ì†µÌ±á Ì†µÌ±ÜÌ†µÌ±ä Ì†µÌ±§

(Ì†µÌ±ÜÌ†µÌ±ä

Ì†µÌ±§)

Ì†µÌ±ÜÌ†µÌ∞µÌ†µÌ±§ = Ì†µÌºÜ(Ì†µÌ±ÜÌ†µÌ±ä Ì†µÌ±§)

Ì†µÌ±ÜÌ†µÌ±ä ‚àí1Ì†µÌ±ÜÌ†µÌ∞µÌ†µÌ±§ = Ì†µÌºÜÌ†µÌ±§.

Since we do not care about the magnitude of Ì†µÌ±§, only its direction, we replaced the scalar factor (Ì†µÌ±§Ì†µÌ±á Ì†µÌ±ÜÌ†µÌ∞µÌ†µÌ±§)/(Ì†µÌ±§Ì†µÌ±á Ì†µÌ±ÜÌ†µÌ±ä Ì†µÌ±§) by Ì†µÌºÜ.
In the multiple-class case, the solutions Ì†µÌ±§ are determined by the eigenvectors of Ì†µÌ±ÜÌ†µÌ±ä ‚àí1Ì†µÌ±ÜÌ†µÌ∞µ that correspond to the Ì†µÌ∞æ ‚àí 1 largest eigenvalues.
However, in the two-class case (in which Ì†µÌ±ÜÌ†µÌ∞µ = (Ì†µÌºá1 ‚àí Ì†µÌºá0)(Ì†µÌºá1 ‚àí Ì†µÌºá0)Ì†µÌ±á ) it is easy to show that Ì†µÌ±§ = Ì†µÌ±ÜÌ†µÌ±ä ‚àí1(Ì†µÌºá1 ‚àí Ì†µÌºá0) is the unique eigenvector of Ì†µÌ±ÜÌ†µÌ±ä ‚àí1Ì†µÌ±ÜÌ†µÌ∞µ:

Ì†µÌ±ÜÌ†µÌ±ä ‚àí1(Ì†µÌºá1 ‚àí Ì†µÌºá0)(Ì†µÌºá1 ‚àí Ì†µÌºá0)Ì†µÌ±á Ì†µÌ±§ = Ì†µÌºÜÌ†µÌ±§ Ì†µÌ±ÜÌ†µÌ±ä ‚àí1(Ì†µÌºá1 ‚àí Ì†µÌºá0)(Ì†µÌºá1 ‚àí Ì†µÌºá0)Ì†µÌ±á Ì†µÌ±ÜÌ†µÌ±ä ‚àí1(Ì†µÌºá1 ‚àí Ì†µÌºá0) = Ì†µÌºÜÌ†µÌ±ÜÌ†µÌ±ä ‚àí1(Ì†µÌºá1 ‚àí Ì†µÌºá0),
where here Ì†µÌºÜ = (Ì†µÌºá1 ‚àí Ì†µÌºá0)Ì†µÌ±á Ì†µÌ±ÜÌ†µÌ±ä ‚àí1(Ì†µÌºá1 ‚àí Ì†µÌºá0). Which leads to the result Ì†µÌ±§ ‚àù Ì†µÌ±ÜÌ†µÌ±ä ‚àí1(Ì†µÌºá1 ‚àí Ì†µÌºá0).
The separating hyperplane
The separating hyperplane is a Ì†µÌ±É ‚àí 1-dimensional hyper surface, orthogonal to the projection vector, Ì†µÌ±§. There is no single best way to Ô¨Ånd the origin of the plane along Ì†µÌ±§, or equivalently the classiÔ¨Åcation threshold that determines whether a point should be classiÔ¨Åed as belonging to Ì†µÌ∞∂0 or to Ì†µÌ∞∂1. However, if the projected points have roughly the same distribution, then the threshold can be chosen as the hyperplane exactly between the projections of the two means, i.e. as
1 Ì†µÌ±á = Ì†µÌ±§ ¬∑ 2 (Ì†µÌºá1 ‚àí Ì†µÌºá0).

%matplotlib inline import warnings warnings.filterwarnings(action= once )

5.4.2 Linear discriminant analysis (LDA)

Linear discriminant analysis (LDA) is a probabilistic generalization of Fisher‚Äôs linear discriminant. It uses Bayes‚Äô rule to Ô¨Åx the threshold based on prior probabilities of classes.

182

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

Fig. 4: The Fisher most discriminant projection

5.4. Linear classiÔ¨Åcation

183

Statistics and Machine Learning in Python, Release 0.2

1. First compute the class-conditional distributions of Ì†µÌ±• given class Ì†µÌ∞∂Ì†µÌ±ò: Ì†µÌ±ù(Ì†µÌ±•|Ì†µÌ∞∂Ì†µÌ±ò) = Ì†µÌ≤© (Ì†µÌ±•|Ì†µÌºáÌ†µÌ±ò, Ì†µÌ±ÜÌ†µÌ±ä ). Where Ì†µÌ≤© (Ì†µÌ±•|Ì†µÌºáÌ†µÌ±ò, Ì†µÌ±ÜÌ†µÌ±ä ) is the multivariate Gaussian distribution deÔ¨Åned over a P-dimensional vector Ì†µÌ±• of continuous variables, which is given by

Ì†µÌ≤© (Ì†µÌ±•|Ì†µÌºáÌ†µÌ±ò, Ì†µÌ±ÜÌ†µÌ±ä )

=

1 (2Ì†µÌºã)Ì†µÌ±É/2|Ì†µÌ±ÜÌ†µÌ±ä |1/2

1 exp{‚àí (Ì†µÌ±• ‚àí
2

Ì†µÌºáÌ†µÌ±ò)Ì†µÌ±á Ì†µÌ±ÜÌ†µÌ±ä ‚àí1(Ì†µÌ±• ‚àí Ì†µÌºáÌ†µÌ±ò)}

2. Estimate the prior probabilities of class Ì†µÌ±ò, Ì†µÌ±ù(Ì†µÌ∞∂Ì†µÌ±ò) = Ì†µÌ±ÅÌ†µÌ±ò/Ì†µÌ±Å .

3. Compute posterior probabilities (ie. the probability of a each class given a sample) combining conditional with priors using Bayes‚Äô rule:

Ì†µÌ±ù(Ì†µÌ∞∂Ì†µÌ±ò|Ì†µÌ±•)

=

Ì†µÌ±ù(Ì†µÌ∞∂Ì†µÌ±ò)Ì†µÌ±ù(Ì†µÌ±•|Ì†µÌ∞∂Ì†µÌ±ò) Ì†µÌ±ù(Ì†µÌ±•)

Where Ì†µÌ±ù(Ì†µÌ±•) is the marginal distribution obtained by suming of classes: As usual, the denominator in Bayes‚Äô theorem can be found in terms of the quantities appearing in the numerator, because

‚àëÔ∏Å Ì†µÌ±ù(Ì†µÌ±•) = Ì†µÌ±ù(Ì†µÌ±•|Ì†µÌ∞∂Ì†µÌ±ò)Ì†µÌ±ù(Ì†µÌ∞∂Ì†µÌ±ò)
Ì†µÌ±ò

4. Classify Ì†µÌ±• using the Maximum-a-Posteriori probability: Ì†µÌ∞∂Ì†µÌ±ò = arg maxÌ†µÌ∞∂Ì†µÌ±ò Ì†µÌ±ù(Ì†µÌ∞∂Ì†µÌ±ò|Ì†µÌ±•)
LDA is a generative model since the class-conditional distributions cal be used to generate samples of each classes.

LDA is useful to deal with imbalanced group sizes (eg.: Ì†µÌ±Å1 ‚â´ Ì†µÌ±Å0) since priors probabilities can be used to explicitly re-balance the classiÔ¨Åcation by setting Ì†µÌ±ù(Ì†µÌ∞∂0) = Ì†µÌ±ù(Ì†µÌ∞∂1) = 1/2 or whatever seems relevant.

LDA can be generalised to the multiclass case with Ì†µÌ∞æ > 2.

With Ì†µÌ±Å1 = Ì†µÌ±Å0, LDA lead to the same solution than Fisher‚Äôs linear discriminant.

Exercise
How many parameters are required to estimate to perform a LDA ?
import numpy as np from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
# Dataset n_samples, n_features = 100, 2 mean0, mean1 = np.array([0, 0]), np.array([0, 2]) Cov = np.array([[1, .8],[.8, 1]]) np.random.seed(42) X0 = np.random.multivariate_normal(mean0, Cov, n_samples) X1 = np.random.multivariate_normal(mean1, Cov, n_samples) X = np.vstack([X0, X1]) y = np.array([0] * X0.shape[0] + [1] * X1.shape[0])
# LDA with scikit-learn lda = LDA() proj = lda.fit(X, y).transform(X) y_pred_lda = lda.predict(X)
errors = y_pred_lda != y print("Nb errors=%i, error rate=%.2f" % (errors.sum(), errors.sum() / len(y_pred_lda)))

184

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

/home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£ Àì‚Üígot 216 from PyObject
return f(*args, **kwds) /home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£ Àì‚Üígot 216 from PyObject
return f(*args, **kwds) /home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£ Àì‚Üígot 216 from PyObject
return f(*args, **kwds) /home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£ Àì‚Üígot 216 from PyObject
return f(*args, **kwds)
Nb errors=10, error rate=0.05
/home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£ Àì‚Üígot 216 from PyObject
return f(*args, **kwds)

5.4.3 Logistic regression
Logistic regression is called a generalized linear models. ie.: it is a linear model with a link function that maps the output of linear multiple regression to the posterior probability of each class Ì†µÌ±ù(Ì†µÌ∞∂Ì†µÌ±ò|Ì†µÌ±•) ‚àà [0, 1] using the logistic sigmoid function:
1 Ì†µÌ±ù(Ì†µÌ∞∂Ì†µÌ±ò|Ì†µÌ±§, Ì†µÌ±•Ì†µÌ±ñ) = 1 + exp(‚àíÌ†µÌ±§ ¬∑ Ì†µÌ±•Ì†µÌ±ñ)
Logistic regression seeks to minimizes the likelihood Ì†µÌ∞ø as Loss function ‚Ñí:
min Ì†µÌ∞ø(Ì†µÌ±§) = Œ†Ì†µÌ†µÌ±ñÌ±Å Ì†µÌ±ù(Ì†µÌ∞∂Ì†µÌ±ò|Ì†µÌ±§, Ì†µÌ±•Ì†µÌ±ñ)
Partically, the Loss function ‚Ñí is the log-likelihood:
Ì†µÌ±Å
‚àëÔ∏Å min ‚Ñí(Ì†µÌ±§) = log Ì†µÌ∞ø(Ì†µÌ±§) = log Ì†µÌ±ù(Ì†µÌ∞∂Ì†µÌ±ò|Ì†µÌ±§, Ì†µÌ±•Ì†µÌ±ñ)
Ì†µÌ±ñ
In the two-class case the algorithms simplify considerably by coding the two-classes (Ì†µÌ∞∂0 and Ì†µÌ∞∂1) via a 0/1 response Ì†µÌ±¶Ì†µÌ±ñ. Indeed, since Ì†µÌ±ù(Ì†µÌ∞∂0|Ì†µÌ±§, Ì†µÌ±•Ì†µÌ±ñ) = 1 ‚àí Ì†µÌ±ù(Ì†µÌ∞∂1|Ì†µÌ±§, Ì†µÌ±•Ì†µÌ±ñ), the log-likelihood can be re-witten:

Ì†µÌ±Å
‚àëÔ∏Å log Ì†µÌ∞ø(Ì†µÌ±§) = {Ì†µÌ±¶Ì†µÌ±ñ log Ì†µÌ±ù(Ì†µÌ∞∂1|Ì†µÌ±§, Ì†µÌ±•Ì†µÌ±ñ) + (1 ‚àí Ì†µÌ±¶Ì†µÌ±ñ) log(1 ‚àí Ì†µÌ±ù(Ì†µÌ∞∂1|Ì†µÌ±§, Ì†µÌ±•Ì†µÌ±ñ))}

Ì†µÌ±ñ

Ì†µÌ±Å

log

Ì†µÌ∞ø(Ì†µÌ±§)

=

‚àëÔ∏Å {Ì†µÌ±¶Ì†µÌ±ñ

Ì†µÌ±§

¬∑

Ì†µÌ±•Ì†µÌ±ñ

‚àí

log(1

+

expÌ†µÌ±§¬∑Ì†µÌ±•Ì†µÌ±ñ )}

Ì†µÌ±ñ

5.4. Linear classiÔ¨Åcation

(5.42) (5.43) (5.44)
185

Statistics and Machine Learning in Python, Release 0.2

Fig. 5: logistic sigmoid function
Logistic regression is a discriminative model since it focuses only on the posterior probability of each class Ì†µÌ±ù(Ì†µÌ∞∂Ì†µÌ±ò|Ì†µÌ±•). It only requires to estimate the Ì†µÌ±É weight of the Ì†µÌ±§ vector. Thus it should be favoured over LDA with many input features. In small dimension and balanced situations it would provide similar predictions than LDA.
However imbalanced group sizes cannot be explicitly controlled. It can be managed using a reweighting of the input samples.
from sklearn import linear_model logreg = linear_model.LogisticRegression(C=1e8, solver= lbfgs ) # This class implements regularized logistic regression. C is the Inverse of‚ê£ Àì‚Üíregularization strength. # Large value => no regularization.
logreg.fit(X, y) y_pred_logreg = logreg.predict(X)
errors = y_pred_logreg != y print("Nb errors=%i, error rate=%.2f" % (errors.sum(), errors.sum() / len(y_pred_logreg))) print(logreg.coef_)
Nb errors=10, error rate=0.05 [[-5.1516729 5.57303883]]

186

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2
Exercise
Explore the Logistic Regression parameters and proposes a solution in cases of highly imbalanced training dataset Ì†µÌ±Å1 ‚â´ Ì†µÌ±Å0 when we know that in reality both classes have the same probability Ì†µÌ±ù(Ì†µÌ∞∂1) = Ì†µÌ±ù(Ì†µÌ∞∂0).
5.4.4 OverÔ¨Åtting
VC dimension (for Vapnik‚ÄìChervonenkis dimension) is a measure of the capacity (complexity, expressive power, richness, or Ô¨Çexibility) of a statistical classiÔ¨Åcation algorithm, deÔ¨Åned as the cardinality of the largest set of points that the algorithm can shatter. Theorem: Linear classiÔ¨Åer in Ì†µÌ±ÖÌ†µÌ±É have VC dimension of Ì†µÌ±É + 1. Hence in dimension two (Ì†µÌ±É = 2) any random partition of 3 points can be learned.

Fig. 6: In 2D we can shatter any three non-collinear points

5.4.5 Ridge Fisher‚Äôs linear classiÔ¨Åcation (L2-regularization)
When the matrix Ì†µÌ±ÜÌ†µÌ±ä is not full rank or Ì†µÌ±É ‚â´ Ì†µÌ±Å , the The Fisher most discriminant projection estimate of the is not unique. This can be solved using a biased version of Ì†µÌ±ÜÌ†µÌ±ä :
Ì†µÌ±ÜÌ†µÌ±ä Ì†µÌ±ÖÌ†µÌ±ñÌ†µÌ±ëÌ†µÌ±îÌ†µÌ±í = Ì†µÌ±ÜÌ†µÌ±ä + Ì†µÌºÜÌ†µÌ∞º where Ì†µÌ∞º is the Ì†µÌ±É √ó Ì†µÌ±É identity matrix. This leads to the regularized (ridge) estimator of the Fisher‚Äôs linear discriminant analysis:
Ì†µÌ±§Ì†µÌ±ÖÌ†µÌ±ñÌ†µÌ±ëÌ†µÌ±îÌ†µÌ±í ‚àù (Ì†µÌ±ÜÌ†µÌ±ä + Ì†µÌºÜÌ†µÌ∞º)‚àí1(Ì†µÌºá1 ‚àí Ì†µÌºá0)

Increasing Ì†µÌºÜ will:
‚Ä¢ Shrinks the coefÔ¨Åcients toward zero.
‚Ä¢ The covariance will converge toward the diagonal matrix, reducing the contribution of the pairwise covariances.

5.4.6 Ridge logistic regression (L2-regularization)

The objective function to be minimized is now the combination of the logistic loss log Ì†µÌ∞ø(Ì†µÌ±§) with a penalty of the L2 norm of the weights vector. In the two-class case, using the 0/1 coding we obtain:

5.4. Linear classiÔ¨Åcation

187

Statistics and Machine Learning in Python, Release 0.2

Fig. 7: The Ridge Fisher most discriminant projection

min Logistic ridge(Ì†µÌ±§) = log Ì†µÌ∞ø(Ì†µÌ±§) + Ì†µÌºÜ ‚ÄñÌ†µÌ±§‚Äñ2

Ì†µÌ±Å

=

‚àëÔ∏Å {Ì†µÌ±¶Ì†µÌ±ñ

Ì†µÌ±§Ì†µÌ±á

Ì†µÌ±•Ì†µÌ±ñ

‚àí

log(1

+

expÌ†µÌ±§Ì†µÌ±á

Ì†µÌ±•Ì†µÌ±ñ )}

+

Ì†µÌºÜ

‚ÄñÌ†µÌ±§‚Äñ2

Ì†µÌ±ñ

(5.45) (5.46) (5.47)

# Dataset # Build a classification task using 3 informative features from sklearn import datasets
X, y = datasets.make_classification(n_samples=100, n_features=20, n_informative=3, n_redundant=0, n_repeated=0, n_classes=2, random_state=0, shuffle=False)
/home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£ Àì‚Üígot 216 from PyObject
return f(*args, **kwds)
from sklearn import linear_model lr = linear_model.LogisticRegression(C=1) # This class implements regularized logistic regression. C is the Inverse of‚ê£ Àì‚Üíregularization strength. # Large value => no regularization.
lr.fit(X, y) y_pred_lr = lr.predict(X)
errors = y_pred_lr != y print("Nb errors=%i, error rate=%.2f" % (errors.sum(), errors.sum() / len(y))) print(lr.coef_)

188

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

Nb errors=26, error rate=0.26 [[-0.12061092 0.7357655 -0.01842318 -0.10835785 0.25328562 0.4221318
0.15152184 0.16522461 0.84404799 0.01962765 -0.15995078 -0.01925974 -0.02807379 0.42939869 -0.06368702 -0.07922044 0.15529371 0.29963205 0.54633137 0.03866807]]
/home/edouard/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433:‚ê£ Àì‚ÜíFutureWarning: Default solver will be changed to lbfgs in 0.22. Specify a solver to‚ê£ Àì‚Üísilence this warning.
FutureWarning)

5.4.7 Lasso logistic regression (L1-regularization)
The objective function to be minimized is now the combination of the logistic loss log Ì†µÌ∞ø(Ì†µÌ±§) with a penalty of the L1 norm of the weights vector. In the two-class case, using the 0/1 coding we obtain:

min Logistic Lasso(Ì†µÌ±§) = log Ì†µÌ∞ø(Ì†µÌ±§) + Ì†µÌºÜ ‚ÄñÌ†µÌ±§‚Äñ1

Ì†µÌ±Å

=

‚àëÔ∏Å {Ì†µÌ±¶Ì†µÌ±ñ

Ì†µÌ±§

¬∑

Ì†µÌ±•Ì†µÌ±ñ

‚àí

log(1

+

expÌ†µÌ±§¬∑Ì†µÌ±•Ì†µÌ±ñ )}

+

Ì†µÌºÜ

‚ÄñÌ†µÌ±§‚Äñ1

Ì†µÌ±ñ

from sklearn import linear_model lrl1 = linear_model.LogisticRegression(penalty= l1 ) # This class implements regularized logistic regression. C is the Inverse of‚ê£ Àì‚Üíregularization strength. # Large value => no regularization.

(5.48) (5.49)

lrl1.fit(X, y) y_pred_lrl1 = lrl1.predict(X)

errors = y_pred_lrl1 != y print("Nb errors=%i, error rate=%.2f" % (errors.sum(), errors.sum() / len(y_pred_lrl1))) print(lrl1.coef_)

Nb errors=27, error rate=0.27

[[-0.11337193 0.68158741 0.

0.

0.08057281 0.06205131 0.76016935 0.

0.

0.33749287 0.

0.

0.48384386 0.

]]

0.19755791 0.36483513 -0.10808194 0. 0.0790326 0.20158542

/home/edouard/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433:‚ê£ Àì‚ÜíFutureWarning: Default solver will be changed to lbfgs in 0.22. Specify a solver to‚ê£ Àì‚Üísilence this warning.
FutureWarning)

5.4.8 Ridge linear Support Vector Machine (L2-regularization)

Support Vector Machine seek for separating hyperplane with maximum margin to enforce robustness against noise. Like logistic regression it is a discriminative method that only focuses of predictions.

5.4. Linear classiÔ¨Åcation

189

Statistics and Machine Learning in Python, Release 0.2
Here we present the non separable case of Maximum Margin ClassiÔ¨Åers with ¬±1 coding (ie.: Ì†µÌ±¶Ì†µÌ±ñ {‚àí1, +1}). In the next Ô¨Ågure the legend aply to samples of ‚Äúdot‚Äù class.

Fig. 8: Linear lar margin classiÔ¨Åers

Linear SVM for classiÔ¨Åcation (also called SVM-C or SVC) minimizes:

min Linear SVM(Ì†µÌ±§) = penalty(Ì†µÌ±§) + Ì†µÌ∞∂ Hinge loss(Ì†µÌ±§)

= ‚ÄñÌ†µÌ±§‚Äñ2 + Ì†µÌ∞∂

‚àëÔ∏ÄÌ†µÌ±Å
Ì†µÌ±ñ

Ì†µÌºâÌ†µÌ±ñ

with ‚àÄÌ†µÌ±ñ

Ì†µÌ±¶Ì†µÌ±ñ(Ì†µÌ±§ ¬∑ Ì†µÌ±•Ì†µÌ±ñ) ‚â• 1 ‚àí Ì†µÌºâÌ†µÌ±ñ

Here we introduced the slack variables: Ì†µÌºâÌ†µÌ±ñ, with Ì†µÌºâÌ†µÌ±ñ = 0 for points that are on or inside the correct margin boundary and Ì†µÌºâÌ†µÌ±ñ = |Ì†µÌ±¶Ì†µÌ±ñ ‚àí (Ì†µÌ±§ Ì†µÌ±êÌ†µÌ±ëÌ†µÌ±úÌ†µÌ±° ¬∑ Ì†µÌ±•Ì†µÌ±ñ)| for other points. Thus:
1. If Ì†µÌ±¶Ì†µÌ±ñ(Ì†µÌ±§ ¬∑ Ì†µÌ±•Ì†µÌ±ñ) ‚â• 1 then the point lies outside the margin but on the correct side of the decision boundary. In this case Ì†µÌºâÌ†µÌ±ñ = 0. The constraint is thus not active for this point. It does not contribute to the prediction.

2. If 1 > Ì†µÌ±¶Ì†µÌ±ñ(Ì†µÌ±§ ¬∑ Ì†µÌ±•Ì†µÌ±ñ) ‚â• 0 then the point lies inside the margin and on the correct side of the decision boundary. In this case 0 < Ì†µÌºâÌ†µÌ±ñ ‚â§ 1. The constraint is active for this point. It does contribute to the prediction as a support vector.

3. If 0 < Ì†µÌ±¶Ì†µÌ±ñ(Ì†µÌ±§ ¬∑ Ì†µÌ±•Ì†µÌ±ñ)) then the point is on the wrong side of the decision boundary (missclassiÔ¨Åcation). In this case 0 < Ì†µÌºâÌ†µÌ±ñ > 1. The constraint is active for this point. It does contribute to the prediction as a support vector.

This loss is called the hinge loss, deÔ¨Åned as:

max(0, 1 ‚àí Ì†µÌ±¶Ì†µÌ±ñ (Ì†µÌ±§ ¬∑ Ì†µÌ±•Ì†µÌ±ñ))

So linear SVM is closed to Ridge logistic regression, using the hinge loss instead of the logistic loss. Both will provide very similar predictions.
from sklearn import svm

svmlin = svm.LinearSVC() # Remark: by default LinearSVC uses squared_hinge as loss svmlin.fit(X, y)

(continues on next page)

190

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

y_pred_svmlin = svmlin.predict(X)

(continued from previous page)

errors = y_pred_svmlin != y print("Nb errors=%i, error rate=%.2f" % (errors.sum(), errors.sum() / len(y_pred_svmlin))) print(svmlin.coef_)

Nb errors=26, error rate=0.26 [[-0.05604866 0.31187658 0.00275339 -0.05153703 0.09938239 0.17724479
0.06520118 0.08921076 0.35336289 0.00599675 -0.06201396 -0.00742017 -0.02159905 0.18271488 -0.02164397 -0.04061358 0.0720389 0.13085078 0.23720691 0.00826935]]

/home/edouard/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931:‚ê£ Àì‚ÜíConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
"the number of iterations.", ConvergenceWarning)

5.4.9 Lasso linear Support Vector Machine (L1-regularization)

Linear SVM for classiÔ¨Åcation (also called SVM-C or SVC) with l1-regularization

min

Ì†µÌ∞πLasso linear SVM(Ì†µÌ±§)

= Ì†µÌºÜ ||Ì†µÌ±§||1 + Ì†µÌ∞∂

‚àëÔ∏ÄÌ†µÌ±Å
Ì†µÌ±ñ

Ì†µÌºâÌ†µÌ±ñ

with ‚àÄÌ†µÌ±ñ

Ì†µÌ±¶Ì†µÌ±ñ(Ì†µÌ±§ ¬∑ Ì†µÌ±•Ì†µÌ±ñ) ‚â• 1 ‚àí Ì†µÌºâÌ†µÌ±ñ

from sklearn import svm
svmlinl1 = svm.LinearSVC(penalty= l1 , dual=False) # Remark: by default LinearSVC uses squared_hinge as loss
svmlinl1.fit(X, y) y_pred_svmlinl1 = svmlinl1.predict(X)
errors = y_pred_svmlinl1 != y print("Nb errors=%i, error rate=%.2f" % (errors.sum(), errors.sum() / len(y_pred_ Àì‚Üísvmlinl1))) print(svmlinl1.coef_)

Nb errors=26, error rate=0.26

[[-0.05334026 0.29934473 0.

-0.03541313 0.09261404 0.16763294

0.05808033 0.07587505 0.34065177 0.

-0.05558916 -0.00194123

-0.01312461 0.1686629 -0.01450446 -0.02500537 0.06074148 0.11738861

0.22485536 0.00473342]]

## Exercise
Compare predictions of Logistic regression (LR) and their SVM counterparts, ie.: L2 LR vs L2 SVM and L1 LR vs L1 SVM
‚Ä¢ Compute the correlation between pairs of weights vectors.
‚Ä¢ Compare the predictions of two classiÔ¨Åers using their decision function:
‚Äì Give the equation of the decision function for a linear classiÔ¨Åer, assuming that their is no intercept.

5.4. Linear classiÔ¨Åcation

191

Statistics and Machine Learning in Python, Release 0.2

‚Äì Compute the correlation decision function. ‚Äì Plot the pairwise decision function of the classiÔ¨Åers. ‚Ä¢ Conclude on the differences between Linear SVM and logistic regression.
5.4.10 Elastic-net classiÔ¨Åcation (L2-L1-regularization)
The objective function to be minimized is now the combination of the logistic loss log Ì†µÌ∞ø(Ì†µÌ±§) or the hinge loss with combination of L1 and L2 penalties. In the two-class case, using the 0/1 coding we obtain:

min Logistic enet(Ì†µÌ±§) = log Ì†µÌ∞ø(Ì†µÌ±§) + Ì†µÌªº (Ô∏ÄÌ†µÌºå ‚ÄñÌ†µÌ±§‚Äñ1 + (1 ‚àí Ì†µÌºå) ‚ÄñÌ†µÌ±§‚Äñ22)Ô∏Ä min Hinge enet(Ì†µÌ±§) = Hinge loss(Ì†µÌ±§) + Ì†µÌªº (Ô∏ÄÌ†µÌºå ‚ÄñÌ†µÌ±§‚Äñ1 + (1 ‚àí Ì†µÌºå) ‚ÄñÌ†µÌ±§‚Äñ22)Ô∏Ä

(5.50) (5.51)

from sklearn import datasets from sklearn import linear_model as lm import matplotlib.pyplot as plt
X, y = datasets.make_classification(n_samples=100, n_features=20, n_informative=3, n_redundant=0, n_repeated=0, n_classes=2, random_state=0, shuffle=False)
enetloglike = lm.SGDClassifier(loss="log", penalty="elasticnet", alpha=0.0001, l1_ratio=0.15, class_weight= balanced )
enetloglike.fit(X, y)
enethinge = lm.SGDClassifier(loss="hinge", penalty="elasticnet", alpha=0.0001, l1_ratio=0.15, class_weight= balanced )
enethinge.fit(X, y)

/home/edouard/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_ Àì‚Üígradient.py:166: FutureWarning: max_iter and tol parameters have been added in‚ê£ Àì‚ÜíSGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None.‚ê£ Àì‚ÜíIf tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter‚ê£ Àì‚Üíwill be 1000, and default tol will be 1e-3.
FutureWarning) /home/edouard/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_ Àì‚Üígradient.py:166: FutureWarning: max_iter and tol parameters have been added in‚ê£ Àì‚ÜíSGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None.‚ê£ Àì‚ÜíIf tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter‚ê£ Àì‚Üíwill be 1000, and default tol will be 1e-3.
FutureWarning)

SGDClassifier(alpha=0.0001, average=False, class_weight= balanced , early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15, learning_rate= optimal , loss= hinge , max_iter=None,
(continues on next page)

192

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2
(continued from previous page) n_iter=None, n_iter_no_change=5, n_jobs=None, penalty= elasticnet , power_t=0.5, random_state=None, shuffle=True, tol=None, validation_fraction=0.1, verbose=0, warm_start=False)
Exercise
Compare predictions of Elastic-net Logistic regression (LR) and Hinge-loss Elastic-net ‚Ä¢ Compute the correlation between pairs of weights vectors. ‚Ä¢ Compare the predictions of two classiÔ¨Åers using their decision function: ‚Äì Compute the correlation decision function. ‚Äì Plot the pairwise decision function of the classiÔ¨Åers. ‚Ä¢ Conclude on the differences between the two losses.
5.4.11 Metrics of classiÔ¨Åcation performance evaluation
Metrics for binary classiÔ¨Åcation
source: https://en.wikipedia.org/wiki/Sensitivity_and_speciÔ¨Åcity Imagine a study evaluating a new test that screens people for a disease. Each person taking the test either has or does not have the disease. The test outcome can be positive (classifying the person as having the disease) or negative (classifying the person as not having the disease). The test results for each subject may or may not match the subject‚Äôs actual status. In that setting:
‚Ä¢ True positive (TP): Sick people correctly identiÔ¨Åed as sick ‚Ä¢ False positive (FP): Healthy people incorrectly identiÔ¨Åed as sick ‚Ä¢ True negative (TN): Healthy people correctly identiÔ¨Åed as healthy ‚Ä¢ False negative (FN): Sick people incorrectly identiÔ¨Åed as healthy ‚Ä¢ Accuracy (ACC):
ACC = (TP + TN) / (TP + FP + FN + TN) ‚Ä¢ Sensitivity (SEN) or recall of the positive class or true positive rate (TPR) or hit rate:
SEN = TP / P = TP / (TP+FN) ‚Ä¢ SpeciÔ¨Åcity (SPC) or recall of the negative class or true negative rate:
SPC = TN / N = TN / (TN+FP) ‚Ä¢ Precision or positive predictive value (PPV):
PPV = TP / (TP + FP) ‚Ä¢ Balanced accuracy (bACC):is a useful performance measure is the balanced accuracy
which avoids inÔ¨Çated performance estimates on imbalanced datasets (Brodersen, et al. (2010). ‚ÄúThe balanced accuracy and its posterior distribution‚Äù). It is deÔ¨Åned as the arithmetic mean of sensitivity and speciÔ¨Åcity, or the average accuracy obtained on either class:

5.4. Linear classiÔ¨Åcation

193

Statistics and Machine Learning in Python, Release 0.2
bACC = 1/2 * (SEN + SPC) ‚Ä¢ F1 Score (or F-score) which is a weighted average of precision and recall are usefull to
deal with imballaced datasets The four outcomes can be formulated in a 2√ó2 contingency table or confusion matrix https: //en.wikipedia.org/wiki/Sensitivity_and_speciÔ¨Åcity For more precision see: http://scikit-learn.org/stable/modules/model_evaluation.html
from sklearn import metrics y_pred = [0, 1, 0, 0] y_true = [0, 1, 0, 1]
metrics.accuracy_score(y_true, y_pred)
# The overall precision an recall metrics.precision_score(y_true, y_pred) metrics.recall_score(y_true, y_pred)
# Recalls on individual classes: SEN & SPC recalls = metrics.recall_score(y_true, y_pred, average=None) recalls[0] # is the recall of class 0: specificity recalls[1] # is the recall of class 1: sensitivity
# Balanced accuracy b_acc = recalls.mean()
# The overall precision an recall on each individual class p, r, f, s = metrics.precision_recall_fscore_support(y_true, y_pred)
SigniÔ¨Åcance of classiÔ¨Åcation rate
P-value associated to classiÔ¨Åcation rate. Compared the number of correct classiÔ¨Åcations (=accuracy √óÌ†µÌ±Å ) to the null hypothesis of Binomial distribution of parameters Ì†µÌ±ù (typically 50% of chance level) and Ì†µÌ±Å (Number of observations). Is 60% of accuracy a signiÔ¨Åcant prediction rate among 62 observations? Since this is an exact, two-sided test of the null hypothesis, the p-value can be divided by 2 since we test that the accuracy is superior to the chance level.
import scipy.stats
acc, N = 0.62, 62 pval = scipy.stats.binom_test(x=int(acc * N), n=N, p=0.5) / 2 print(pval)
0.04897722025975367
### Area Under Curve (AUC) of Receiver operating characteristic (ROC) Some classiÔ¨Åer may have found a good discriminative projection Ì†µÌ±§. However if the threshold to decide the Ô¨Ånal predicted class is poorly adjusted, the performances will highlight an high speciÔ¨Åcity and a low sensitivity or the contrary.

194

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

In this case it is recommended to use the AUC of a ROC analysis which basically provide a measure of overlap of the two classes when points are projected on the discriminative axis. For more detail on ROC and AUC see:https://en.wikipedia.org/wiki/Receiver_operating_characteristic.
from sklearn import metrics score_pred = np.array([.1 ,.2, .3, .4, .5, .6, .7, .8]) y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1]) thres = .9 y_pred = (score_pred > thres).astype(int)
print("Predictions:", y_pred) metrics.accuracy_score(y_true, y_pred)
# The overall precision an recall on each individual class p, r, f, s = metrics.precision_recall_fscore_support(y_true, y_pred) print("Recalls:", r) # 100% of specificity, 0% of sensitivity
# However AUC=1 indicating a perfect separation of the two classes auc = metrics.roc_auc_score(y_true, score_pred) print("AUC:", auc)
Predictions: [0 0 0 0 0 0 0 0] Recalls: [1. 0.] AUC: 1.0
/home/edouard/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification. Àì‚Üípy:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to‚ê£ Àì‚Üí0.0 in labels with no predicted samples.
precision , predicted , average, warn_for)

5.4.12 Imbalanced classes
Learning with discriminative (logistic regression, SVM) methods is generally based on minimizing the misclassiÔ¨Åcation of training samples, which may be unsuitable for imbalanced datasets where the recognition might be biased in favor of the most numerous class. This problem can be addressed with a generative approach, which typically requires more parameters to be determined leading to reduced performances in high dimension.
Dealing with imbalanced class may be addressed by three main ways (see Japkowicz and Stephen (2002) for a review), resampling, reweighting and one class learning.
In sampling strategies, either the minority class is oversampled or majority class is undersampled or some combination of the two is deployed. Undersampling (Zhang and Mani, 2003) the majority class would lead to a poor usage of the left-out samples. Sometime one cannot afford such strategy since we are also facing a small sample size problem even for the majority class. Informed oversampling, which goes beyond a trivial duplication of minority class samples, requires the estimation of class conditional distributions in order to generate synthetic samples. Here generative models are required. An alternative, proposed in (Chawla et al., 2002) generate samples along the line segments joining any/all of the k minority class nearest neighbors. Such procedure blindly generalizes the minority area without regard to the majority class, which may be particularly problematic with high-dimensional and potentially skewed class distribution.
Reweighting, also called cost-sensitive learning, works at an algorithmic level by adjusting

5.4. Linear classiÔ¨Åcation

195

Statistics and Machine Learning in Python, Release 0.2
the costs of the various classes to counter the class imbalance. Such reweighting can be implemented within SVM (Chang and Lin, 2001) or logistic regression (Friedman et al., 2010) classiÔ¨Åers. Most classiÔ¨Åers of Scikit learn offer such reweighting possibilities.
The class_weight parameter can be positioned into the "balanced" mode which uses the values of Ì†µÌ±¶ to automatically adjust weights inversely proportional to class frequencies in the input data as Ì†µÌ±Å/(2Ì†µÌ±ÅÌ†µÌ±ò).
import numpy as np from sklearn import linear_model from sklearn import datasets from sklearn import metrics import matplotlib.pyplot as plt
# dataset X, y = datasets.make_classification(n_samples=500,
n_features=5, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, random_state=1, shuffle=False)
print(*["#samples of class %i = %i;" % (lev, np.sum(y == lev)) for lev in np.unique(y)])
print( # No Reweighting balanced dataset ) lr_inter = linear_model.LogisticRegression(C=1) lr_inter.fit(X, y) p, r, f, s = metrics.precision_recall_fscore_support(y, lr_inter.predict(X)) print("SPC: %.3f; SEN: %.3f" % tuple(r)) print( # => The predictions are balanced in sensitivity and specificity\n )
# Create imbalanced dataset, by subsampling sample of class 0: keep only 10% of # class 0 s samples and all class 1 s samples. n0 = int(np.rint(np.sum(y == 0) / 20)) subsample_idx = np.concatenate((np.where(y == 0)[0][:n0], np.where(y == 1)[0])) Ximb = X[subsample_idx, :] yimb = y[subsample_idx] print(*["#samples of class %i = %i;" % (lev, np.sum(yimb == lev)) for lev in
np.unique(yimb)])
print( # No Reweighting on imbalanced dataset ) lr_inter = linear_model.LogisticRegression(C=1) lr_inter.fit(Ximb, yimb) p, r, f, s = metrics.precision_recall_fscore_support(yimb, lr_inter.predict(Ximb)) print("SPC: %.3f; SEN: %.3f" % tuple(r)) print( # => Sensitivity >> specificity\n )
print( # Reweighting on imbalanced dataset ) lr_inter_reweight = linear_model.LogisticRegression(C=1, class_weight="balanced") lr_inter_reweight.fit(Ximb, yimb) p, r, f, s = metrics.precision_recall_fscore_support(yimb,
lr_inter_reweight.predict(Ximb)) print("SPC: %.3f; SEN: %.3f" % tuple(r)) print( # => The predictions are balanced in sensitivity and specificity\n )

196

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

#samples of class 0 = 250; #samples of class 1 = 250; # No Reweighting balanced dataset SPC: 0.940; SEN: 0.928 # => The predictions are balanced in sensitivity and specificity
#samples of class 0 = 12; #samples of class 1 = 250; # No Reweighting on imbalanced dataset SPC: 0.750; SEN: 0.992 # => Sensitivity >> specificity
# Reweighting on imbalanced dataset SPC: 1.000; SEN: 0.972 # => The predictions are balanced in sensitivity and specificity
/home/edouard/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433:‚ê£ Àì‚ÜíFutureWarning: Default solver will be changed to lbfgs in 0.22. Specify a solver to‚ê£ Àì‚Üísilence this warning.
FutureWarning) /home/edouard/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433:‚ê£ Àì‚ÜíFutureWarning: Default solver will be changed to lbfgs in 0.22. Specify a solver to‚ê£ Àì‚Üísilence this warning.
FutureWarning) /home/edouard/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433:‚ê£ Àì‚ÜíFutureWarning: Default solver will be changed to lbfgs in 0.22. Specify a solver to‚ê£ Àì‚Üísilence this warning.
FutureWarning)

5.4.13 Exercise
Fisher linear discriminant rule
Write a class FisherLinearDiscriminant that implements the Fisher‚Äôs linear discriminant analysis. This class must be compliant with the scikit-learn API by providing two methods: - fit(X, y) which Ô¨Åts the model and returns the object itself; - predict(X) which returns a vector of the predicted values. Apply the object on the dataset presented for the LDA.

5.5 Non linear learning algorithms

5.5.1 Support Vector Machines (SVM)

SVM are based kernel methods require only a user-speciÔ¨Åed kernel function Ì†µÌ∞æ(Ì†µÌ±•Ì†µÌ±ñ, Ì†µÌ±•Ì†µÌ±ó), i.e., a similarity function over pairs of data points (Ì†µÌ±•Ì†µÌ±ñ, Ì†µÌ±•Ì†µÌ±ó) into kernel (dual) space on which learning algorithms operate linearly, i.e. every operation on points is a linear combination of Ì†µÌ∞æ(Ì†µÌ±•Ì†µÌ±ñ, Ì†µÌ±•Ì†µÌ±ó).
Outline of the SVM algorithm:
1. Map points Ì†µÌ±• into kernel space using a kernel function: Ì†µÌ±• ‚Üí Ì†µÌ∞æ(Ì†µÌ±•, .).
2. Learning algorithms operate linearly by dot product into high-kernel space Ì†µÌ∞æ(., Ì†µÌ±•Ì†µÌ±ñ) ¬∑ Ì†µÌ∞æ(., Ì†µÌ±•Ì†µÌ±ó).
‚Ä¢ Using the kernel trick (Mercer‚Äôs Theorem) replace dot product in hgh dimensional space by a simpler operation such that Ì†µÌ∞æ(., Ì†µÌ±•Ì†µÌ±ñ) ¬∑ Ì†µÌ∞æ(., Ì†µÌ±•Ì†µÌ±ó) = Ì†µÌ∞æ(Ì†µÌ±•Ì†µÌ±ñ, Ì†µÌ±•Ì†µÌ±ó). Thus we only

5.5. Non linear learning algorithms

197

Statistics and Machine Learning in Python, Release 0.2

need to compute a similarity measure for each pairs of point and store in a Ì†µÌ±Å √ó Ì†µÌ±Å Gram matrix.
‚Ä¢ Finally, The learning process consist of estimating the Ì†µÌªºÌ†µÌ±ñ of the decision function that maximises the hinge loss (of Ì†µÌ±ì (Ì†µÌ±•)) plus some penalty when applied on all training points.

(Ô∏É Ì†µÌ±Å

)Ô∏É

‚àëÔ∏Å

Ì†µÌ±ì (Ì†µÌ±•) = sign

Ì†µÌªºÌ†µÌ±ñ Ì†µÌ±¶Ì†µÌ±ñ Ì†µÌ∞æ(Ì†µÌ±•Ì†µÌ±ñ, Ì†µÌ±•) .

Ì†µÌ±ñ

3. Predict a new point Ì†µÌ±• using the decision function.

Fig. 9: Support Vector Machines.

Gaussian kernel (RBF, Radial Basis Function):
One of the most commonly used kernel is the Radial Basis Function (RBF) Kernel. For a pair of points Ì†µÌ±•Ì†µÌ±ñ, Ì†µÌ±•Ì†µÌ±ó the RBF kernel is deÔ¨Åned as:

Ì†µÌ∞æ (Ì†µÌ±•Ì†µÌ±ñ ,

Ì†µÌ±•Ì†µÌ±ó )

=

exp

(Ô∏Ç ‚àí

‚ÄñÌ†µÌ±•Ì†µÌ±ñ

‚àí Ì†µÌ±•Ì†µÌ±ó‚Äñ2 2Ì†µÌºé2

)Ô∏Ç

= exp (Ô∏Ä‚àíÌ†µÌªæ ‚ÄñÌ†µÌ±•Ì†µÌ±ñ ‚àí Ì†µÌ±•Ì†µÌ±ó‚Äñ2)Ô∏Ä

(5.52) (5.53)

Where Ì†µÌºé (or Ì†µÌªæ) deÔ¨Ånes the kernel width parameter. Basically, we consider a Gaussian function centered on each training sample Ì†µÌ±•Ì†µÌ±ñ. it has a ready interpretation as a similarity measure as it decreases with squared Euclidean distance between the two feature vectors.
Non linear SVM also exists for regression problems.
%matplotlib inline import warnings warnings.filterwarnings(action= once )

198

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2
import numpy as np from sklearn.svm import SVC from sklearn import datasets import matplotlib.pyplot as plt
# dataset X, y = datasets.make_classification(n_samples=10, n_features=2,n_redundant=0,
n_classes=2, random_state=1, shuffle=False) clf = SVC(kernel= rbf )#, gamma=1) clf.fit(X, y) print("#Errors: %i" % np.sum(y != clf.predict(X)))
clf.decision_function(X)
# Usefull internals: # Array of support vectors clf.support_vectors_
# indices of support vectors within original X np.all(X[clf.support_,:] == clf.support_vectors_)
/home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£ Àì‚Üígot 216 from PyObject
return f(*args, **kwds) /home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£ Àì‚Üígot 216 from PyObject
return f(*args, **kwds) /home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£ Àì‚Üígot 216 from PyObject
return f(*args, **kwds) /home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£ Àì‚Üígot 216 from PyObject
return f(*args, **kwds)
#Errors: 0
/home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£ Àì‚Üígot 216 from PyObject
return f(*args, **kwds) /home/edouard/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196:‚ê£ Àì‚ÜíFutureWarning: The default value of gamma will change from auto to scale in version‚ê£ Àì‚Üí0.22 to account better for unscaled features. Set gamma explicitly to auto or scale ‚ê£ Àì‚Üíto avoid this warning.
"avoid this warning.", FutureWarning)
True

5.5. Non linear learning algorithms

199

Statistics and Machine Learning in Python, Release 0.2
5.5.2 Random forest
A random forest is a meta estimator that Ô¨Åts a number of decision tree learners on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-Ô¨Åtting.
### Decision tree learner
A tree can be ‚Äúlearned‚Äù by splitting the training dataset into subsets based on an features value test.
Each internal node represents a ‚Äútest‚Äù on an feature resulting on the split of the current sample. At each step the algorithm selects the feature and a cutoff value that maximises a given metric. Different metrics exist for regression tree (target is continuous) or classiÔ¨Åcation tree (the target is qualitative).
This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the subset at a node has all the same value of the target variable, or when splitting no longer adds value to the predictions. This general principle is implemented by many recursive partitioning tree algorithms.

Fig. 10: ClassiÔ¨Åcation tree.

Decision trees are simple to understand and interpret however they tend to overÔ¨Åt the data. However decision trees tend to overÔ¨Åt the training set. Leo Breiman propose random forest to deal with this issue.
from sklearn.ensemble import RandomForestClassifier

forest = RandomForestClassifier(n_estimators = 100) forest.fit(X, y)

(continues on next page)

200

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

print("#Errors: %i" % np.sum(y != forest.predict(X)))

(continued from previous page)

/home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£ Àì‚Üígot 216 from PyObject
return f(*args, **kwds)

#Errors: 0

5.6 Resampling Methods

5.6.1 Left out samples validation
The training error can be easily calculated by applying the statistical learning method to the observations used in its training. But because of overÔ¨Åtting, the training error rate can dramatically underestimate the error that would be obtained on new samples.
The test error is the average error that results from a learning method to predict the response on a new samples that is, on samples that were not used in training the method. Given a data set, the use of a particular learning method is warranted if it results in a low test error. The test error can be easily calculated if a designated test set is available. Unfortunately, this is usually not the case.
Thus the original dataset is generally split in a training and a test (or validation) data sets. Large training set (80%) small test set (20%) might provide a poor estimation of the predictive performances. On the contrary, large test set and small training set might produce a poorly estimated learner. This is why, on situation where we cannot afford such split, it recommended to use cross-validation scheme to estimate the predictive power of a learning algorithm.

5.6.2 Cross-Validation (CV)

Cross-Validation scheme randomly divides the set of observations into Ì†µÌ∞æ groups, or folds, of approximately equal size. The Ô¨Årst fold is treated as a validation set, and the method Ì†µÌ±ì () is Ô¨Åtted on the remaining union of Ì†µÌ∞æ ‚àí 1 folds: (Ì†µÌ±ì (Ì†µÌ±ã‚àíÌ†µÌ∞æ , Ì†µÌ±¶‚àíÌ†µÌ∞æ )).
The measure of performance (the score function Ì†µÌ≤Æ), either a error measure or an correct prediction measure is an average of a loss error or correct prediction measure, noted ‚Ñí, between a true target value and the predicted target value. The score function is evaluated of the on the observations in the held-out fold. For each sample Ì†µÌ±ñ we consider the model estimated Ì†µÌ±ì (Ì†µÌ±ã‚àíÌ†µÌ±ò(Ì†µÌ±ñ), Ì†µÌ±¶‚àíÌ†µÌ±ò(Ì†µÌ±ñ) on the data set without the group Ì†µÌ±ò that contains Ì†µÌ±ñ noted ‚àíÌ†µÌ±ò(Ì†µÌ±ñ). This procedure is repeated Ì†µÌ∞æ times; each time, a different group of observations is treated as a test set. Then we compare the predicted value (Ì†µÌ±ì‚àíÌ†µÌ±ò(Ì†µÌ±ñ)(Ì†µÌ±•Ì†µÌ±ñ) = Ì†µÀÜÌ±¶Ì†µÌ±ñ) with true value Ì†µÌ±¶Ì†µÌ±ñ using a Error or Loss function ‚Ñí(Ì†µÌ±¶, Ì†µÀÜÌ±¶). We can the compute a score Ì†µÌ≤Æ averaging over all samples:

1

Ì†µÌ±Å
‚àëÔ∏Å

(Ô∏Å

)Ô∏Å

Ì†µÌ≤Æ(Ì†µÌ±ì ) = Ì†µÌ±Å

‚Ñí Ì†µÌ±¶Ì†µÌ±ñ, Ì†µÌ±ì (Ì†µÌ±•‚àíÌ†µÌ±ò(Ì†µÌ±ñ), Ì†µÌ±¶‚àíÌ†µÌ±ò(Ì†µÌ±ñ)) .

Ì†µÌ±ñ

Similarly we can compute a score Ì†µÌ≤Æ on each each fold Ì†µÌ±ò and average accross folds:

5.6. Resampling Methods

201

Statistics and Machine Learning in Python, Release 0.2

1

Ì†µÌ∞æ
‚àëÔ∏Å

Ì†µÌ≤Æ(Ì†µÌ±ì ) = Ì†µÌ∞æ

Ì†µÌ≤ÆÌ†µÌ±ò(Ì†µÌ±ì ).

Ì†µÌ±ò

1

Ì†µÌ∞æ
‚àëÔ∏Å

1

‚àëÔ∏Å

(Ô∏Å

)Ô∏Å

Ì†µÌ≤Æ(Ì†µÌ±ì ) = Ì†µÌ∞æ

Ì†µÌ±ò

‚Ñí Ì†µÌ±ÅÌ†µÌ±ò Ì†µÌ±ñ‚ààÌ†µÌ±ò

Ì†µÌ±¶Ì†µÌ±ñ, Ì†µÌ±ì (Ì†µÌ±•‚àíÌ†µÌ±ò(Ì†µÌ±ñ), Ì†µÌ±¶‚àíÌ†µÌ±ò(Ì†µÌ±ñ))

.

these two measures (an average of average vs. a global average) are generaly similar. They may differ slightly is folds are of different sizes.
This validation scheme is known as the K-Fold CV. Typical choices of Ì†µÌ∞æ are 5 or 10, [Kohavi 1995]. The extreme case where Ì†µÌ∞æ = Ì†µÌ±Å is known as leave-one-out cross-validation, LOO-CV.

CV for regression
Usually the error function ‚Ñí() is the r-squared score. However other function could be used.
%matplotlib inline import warnings warnings.filterwarnings(action= once )

import numpy as np from sklearn import datasets import sklearn.linear_model as lm import sklearn.metrics as metrics from sklearn.model_selection import KFold
X, y = datasets.make_regression(n_samples=100, n_features=100, n_informative=10, random_state=42)
model = lm.Ridge(alpha=10)
cv = KFold(n_splits=5, random_state=42) y_test_pred = np.zeros(len(y)) y_train_pred = np.zeros(len(y))
for train, test in cv.split(X): X_train, X_test, y_train, y_test = X[train, :], X[test, :], y[train], y[test] model.fit(X_train, y_train) y_test_pred[test] = model.predict(X_test) y_train_pred[train] = model.predict(X_train)
print("Train r2:%.2f" % metrics.r2_score(y, y_train_pred)) print("Test r2:%.2f" % metrics.r2_score(y, y_test_pred))

/home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.

Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£

Àì‚Üígot 216 from PyObject

return f(*args, **kwds)

/home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.

Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£

Àì‚Üígot 216 from PyObject

return f(*args, **kwds)

/home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.

Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£

Àì‚Üígot 216 from PyObject

(continues on next page)

202

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

(continued from previous page) return f(*args, **kwds) /home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£ Àì‚Üígot 216 from PyObject return f(*args, **kwds)
Train r2:0.99 Test r2:0.72
/home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£ Àì‚Üígot 216 from PyObject
return f(*args, **kwds)
Scikit-learn provides user-friendly function to perform CV:
from sklearn.model_selection import cross_val_score
scores = cross_val_score(estimator=model, X=X, y=y, cv=5) print("Test r2:%.2f" % scores.mean())
# provide a cv cv = KFold(n_splits=5, random_state=42) scores = cross_val_score(estimator=model, X=X, y=y, cv=cv) print("Test r2:%.2f" % scores.mean())
Test r2:0.73 Test r2:0.73

CV for classiÔ¨Åcation

With classiÔ¨Åcation problems it is essential to sample folds where each set contains approximately the same percentage of samples of each target class as the complete set. This is called stratiÔ¨Åcation. In this case, we will use StratifiedKFold with is a variation of k-fold which returns stratiÔ¨Åed folds.
Usually the error function Ì†µÌ∞ø() are, at least, the sensitivity and the speciÔ¨Åcity. However other function could be used.

import numpy as np from sklearn import datasets import sklearn.linear_model as lm import sklearn.metrics as metrics from sklearn.model_selection import StratifiedKFold

X, y = datasets.make_classification(n_samples=100, n_features=100, n_informative=10, random_state=42)

model = lm.LogisticRegression(C=1, solver= lbfgs )

cv = StratifiedKFold(n_splits=5) y_test_pred = np.zeros(len(y))

(continues on next page)

5.6. Resampling Methods

203

Statistics and Machine Learning in Python, Release 0.2

y_train_pred = np.zeros(len(y))

(continued from previous page)

for train, test in cv.split(X, y): X_train, X_test, y_train, y_test = X[train, :], X[test, :], y[train], y[test] model.fit(X_train, y_train) y_test_pred[test] = model.predict(X_test) y_train_pred[train] = model.predict(X_train)

recall_test = metrics.recall_score(y, y_test_pred, average=None) recall_train = metrics.recall_score(y, y_train_pred, average=None) acc_test = metrics.accuracy_score(y, y_test_pred)

print("Train SPC:%.2f; SEN:%.2f" % tuple(recall_train)) print("Test SPC:%.2f; SEN:%.2f" % tuple(recall_test)) print("Test ACC:%.2f" % acc_test)
Train SPC:1.00; SEN:1.00 Test SPC:0.78; SEN:0.82 Test ACC:0.80
Scikit-learn provides user-friendly function to perform CV:
from sklearn.model_selection import cross_val_score
scores = cross_val_score(estimator=model, X=X, y=y, cv=5) scores.mean()
# provide CV and score def balanced_acc(estimator, X, y, **kwargs):
Balanced acuracy scorer
return metrics.recall_score(y, estimator.predict(X), average=None).mean()
scores = cross_val_score(estimator=model, X=X, y=y, cv=5, scoring=balanced_acc) print("Test ACC:%.2f" % scores.mean())
Test ACC:0.80
Note that with Scikit-learn user-friendly function we average the scores‚Äô average obtained on individual folds which may provide slightly different results that the overall average presented earlier.

5.6.3 Parallel computation with joblib
Dataset import numpy as np from sklearn import datasets import sklearn.linear_model as lm import sklearn.metrics as metrics from sklearn.model_selection import StratifiedKFold
204

(continues on next page)
Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

(continued from previous page)
X, y = datasets.make_classification(n_samples=20, n_features=100, n_informative=10,‚ê£ Àì‚Üírandom_state=42) cv = StratifiedKFold(n_splits=5)

Use cross_validate function
from sklearn.model_selection import cross_validate
estimator = lm.LogisticRegression(C=1, solver= lbfgs ) cv_results = cross_validate(estimator, X, y, cv=cv, n_jobs=5) print(np.mean(cv_results[ test_score ]), cv_results[ test_score ])
0.65 [0.5 0.25 0.75 0.75 1. ]
### Sequential computation
If we want have full control of the operations performed within each fold (retrieve the models parameters, etc.). We would like to parallelize the folowing sequetial code:
estimator = lm.LogisticRegression(C=1, solver= lbfgs ) y_test_pred_seq = np.zeros(len(y)) # Store predictions in the original order coefs_seq = list() for train, test in cv.split(X, y):
X_train, X_test, y_train, y_test = X[train, :], X[test, :], y[train], y[test] estimator.fit(X_train, y_train) y_test_pred_seq[test] = estimator.predict(X_test) coefs_seq.append(estimator.coef_)
test_accs = [metrics.accuracy_score(y[test], y_test_pred_seq[test]) for train, test in cv. Àì‚Üísplit(X, y)] print(np.mean(test_accs), test_accs)
0.65 [0.5, 0.25, 0.75, 0.75, 1.0]

Parallel computation with joblib

from sklearn.externals.joblib import Parallel, delayed, logger from sklearn.base import is_classifier, clone

def _split_fit_predict(estimator, X, y, train, test): X_train, X_test, y_train, y_test = X[train, :], X[test, :], y[train], y[test] estimator.fit(X_train, y_train) return [estimator.predict(X_test), estimator.coef_]

estimator = lm.LogisticRegression(C=1, solver= lbfgs )

parallel = Parallel(n_jobs=5) cv_ret = parallel(
delayed(_split_fit_predict)( clone(estimator), X, y, train, test)
for train, test in cv.split(X, y))

(continues on next page)

5.6. Resampling Methods

205

Statistics and Machine Learning in Python, Release 0.2
(continued from previous page)
y_test_pred_cv, coefs_cv = zip(*cv_ret)
# Retrieve predictions in the original order y_test_pred = np.zeros(len(y)) for i, (train, test) in enumerate(cv.split(X, y)):
y_test_pred[test] = y_test_pred_cv[i]
test_accs = [metrics.accuracy_score(y[test], y_test_pred[test]) for train, test in cv. Àì‚Üísplit(X, y)] print(np.mean(test_accs), test_accs)
0.65 [0.5, 0.25, 0.75, 0.75, 1.0]
Test same predictions and same coeÔ¨Åcients
assert np.all(y_test_pred == y_test_pred_seq) assert np.allclose(np.array(coefs_cv).squeeze(), np.array(coefs_seq).squeeze())
5.6.4 CV for model selection: setting the hyper parameters
It is important to note CV may be used for two separate goals: 1. Model assessment: having chosen a Ô¨Ånal model, estimating its prediction error (generalization error) on new data. 2. Model selection: estimating the performance of different models in order to choose the best one. One special case of model selection is the selection model‚Äôs hyper parameters. Indeed remember that most of learning algorithm have a hyper parameters (typically the regularization parameter) that has to be set.
Generally we must address the two problems simultaneously. The usual approach for both problems is to randomly divide the dataset into three parts: a training set, a validation set, and a test set.
‚Ä¢ The training set (train) is used to Ô¨Åt the models; ‚Ä¢ the validation set (val) is used to estimate prediction error for model selection or to
determine the hyper parameters over a grid of possible values. ‚Ä¢ the test set (test) is used for assessment of the generalization error of the Ô¨Ånal chosen
model. ### Grid search procedure Model selection of the best hyper parameters over a grid of possible values For each possible values of hyper parameters Ì†µÌªºÌ†µÌ±ò:
1. Fit the learner on training set: Ì†µÌ±ì (Ì†µÌ±ãÌ†µÌ±°Ì†µÌ±üÌ†µÌ±éÌ†µÌ±ñÌ†µÌ±õ, Ì†µÌ±¶Ì†µÌ±°Ì†µÌ±üÌ†µÌ±éÌ†µÌ±ñÌ†µÌ±õ, Ì†µÌªºÌ†µÌ±ò) 2. Evaluate the model on the validation set and keep the parameter(s) that minimises the
error measure Ì†µÌªº* = arg min Ì†µÌ∞ø(Ì†µÌ±ì (Ì†µÌ±ãÌ†µÌ±°Ì†µÌ±üÌ†µÌ±éÌ†µÌ±ñÌ†µÌ±õ), Ì†µÌ±¶Ì†µÌ±£Ì†µÌ±éÌ†µÌ±ô, Ì†µÌªºÌ†µÌ±ò)

206

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

3. ReÔ¨Åt the learner on all training + validation data using the best hyper parameters: Ì†µÌ±ì * ‚â° Ì†µÌ±ì (Ì†µÌ±ãÌ†µÌ±°Ì†µÌ±üÌ†µÌ±éÌ†µÌ±ñÌ†µÌ±õ‚à™Ì†µÌ±£Ì†µÌ±éÌ†µÌ±ô, Ì†µÌ±¶Ì†µÌ±°Ì†µÌ±üÌ†µÌ±éÌ†µÌ±ñÌ†µÌ±õ‚à™Ì†µÌ±£Ì†µÌ±éÌ†µÌ±ô, Ì†µÌªº*)
4. ** Model assessment ** of Ì†µÌ±ì * on the test set: Ì†µÌ∞ø(Ì†µÌ±ì *(Ì†µÌ±ãÌ†µÌ±°Ì†µÌ±íÌ†µÌ±†Ì†µÌ±°), Ì†µÌ±¶Ì†µÌ±°Ì†µÌ±íÌ†µÌ±†Ì†µÌ±°)
Nested CV for model selection and assessment
Most of time, we cannot afford such three-way split. Thus, again we will use CV, but in this case we need two nested CVs.
One outer CV loop, for model assessment. This CV performs Ì†µÌ∞æ splits of the dataset into training plus validation (Ì†µÌ±ã‚àíÌ†µÌ∞æ , Ì†µÌ±¶‚àíÌ†µÌ∞æ ) set and a test set Ì†µÌ±ãÌ†µÌ∞æ , Ì†µÌ±¶Ì†µÌ∞æ One inner CV loop, for model selection. For each run of the outer loop, the inner loop loop performs Ì†µÌ∞ø splits of dataset (Ì†µÌ±ã‚àíÌ†µÌ∞æ , Ì†µÌ±¶‚àíÌ†µÌ∞æ ) into training set: (Ì†µÌ±ã‚àíÌ†µÌ∞æ,‚àíÌ†µÌ∞ø, Ì†µÌ±¶‚àíÌ†µÌ∞æ,‚àíÌ†µÌ∞ø) and a validation set: (Ì†µÌ±ã‚àíÌ†µÌ∞æ,Ì†µÌ∞ø, Ì†µÌ±¶‚àíÌ†µÌ∞æ,Ì†µÌ∞ø).
Implementation with scikit-learn
Note that the inner CV loop combined with the learner form a new learner with an automatic model (parameter) selection procedure. This new learner can be easily constructed using Scikitlearn. The learned is wrapped inside a GridSearchCV class.
Then the new learned can be plugged into the classical outer CV loop.
import numpy as np from sklearn import datasets import sklearn.linear_model as lm from sklearn.model_selection import GridSearchCV import sklearn.metrics as metrics from sklearn.model_selection import KFold
# Dataset noise_sd = 10 X, y, coef = datasets.make_regression(n_samples=50, n_features=100, noise=noise_sd,
n_informative=2, random_state=42, coef=True)
# Use this to tune the noise parameter such that snr < 5 print("SNR:", np.std(np.dot(X, coef)) / noise_sd)
# param grid over alpha & l1_ratio param_grid = { alpha : 10. ** np.arange(-3, 3), l1_ratio :[.1, .5, .9]}

# Warp model = GridSearchCV(lm.ElasticNet(max_iter=10000), param_grid, cv=5)

# 1) Biased usage: fit on all data, ommit outer CV loop model.fit(X, y) print("Train r2:%.2f" % metrics.r2_score(y, model.predict(X))) print(model.best_params_)

# 2) User made outer CV, useful to extract specific information cv = KFold(n_splits=5, random_state=42) y_test_pred = np.zeros(len(y))

(continues on next page)

5.6. Resampling Methods

207

Statistics and Machine Learning in Python, Release 0.2

y_train_pred = np.zeros(len(y)) alphas = list()

(continued from previous page)

for train, test in cv.split(X, y): X_train, X_test, y_train, y_test = X[train, :], X[test, :], y[train], y[test] model.fit(X_train, y_train) y_test_pred[test] = model.predict(X_test) y_train_pred[train] = model.predict(X_train) alphas.append(model.best_params_)

print("Train r2:%.2f" % metrics.r2_score(y, y_train_pred)) print("Test r2:%.2f" % metrics.r2_score(y, y_test_pred)) print("Selected alphas:", alphas)

# 3.) user-friendly sklearn for outer CV from sklearn.model_selection import cross_val_score scores = cross_val_score(estimator=model, X=X, y=y, cv=cv) print("Test r2:%.2f" % scores.mean())

SNR: 2.6358469446381614 Train r2:0.96 { alpha : 1.0, l1_ratio : 0.9} Train r2:1.00 Test r2:0.62 Selected alphas: [{ alpha : 0.001, l1_ratio : 0.9}, { alpha : 0.001, l1_ratio : 0.9}, { Àì‚Üí alpha : 0.001, l1_ratio : 0.9}, { alpha : 0.01, l1_ratio : 0.9}, { alpha : 0.001, Àì‚Üí l1_ratio : 0.9}] Test r2:0.55

Regression models with built-in cross-validation
Sklearn will automatically select a grid of parameters, most of time use the defaults values.
n_jobs is the number of CPUs to use during the cross validation. If -1, use all the CPUs.
from sklearn import datasets import sklearn.linear_model as lm import sklearn.metrics as metrics from sklearn.model_selection import cross_val_score
# Dataset X, y, coef = datasets.make_regression(n_samples=50, n_features=100, noise=10,
n_informative=2, random_state=42, coef=True)

print("== Ridge (L2 penalty) ==") model = lm.RidgeCV(cv=3) # Let sklearn select a list of alphas with default LOO-CV scores = cross_val_score(estimator=model, X=X, y=y, cv=5) print("Test r2:%.2f" % scores.mean())
print("== Lasso (L1 penalty) ==") model = lm.LassoCV(n_jobs=-1, cv=3) # Let sklearn select a list of alphas with default 3CV

(continues on next page)

208

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

scores = cross_val_score(estimator=model, X=X, y=y, cv=5) print("Test r2:%.2f" % scores.mean())

(continued from previous page)

print("== ElasticNet (L1 penalty) ==") model = lm.ElasticNetCV(l1_ratio=[.1, .5, .9], n_jobs=-1, cv=3) # Let sklearn select a list of alphas with default 3CV scores = cross_val_score(estimator=model, X=X, y=y, cv=5) print("Test r2:%.2f" % scores.mean())

== Ridge (L2 penalty) == Test r2:0.16 == Lasso (L1 penalty) ==
/home/edouard/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_ Àì‚Üísearch.py:841: DeprecationWarning: The default of the iid parameter will change‚ê£ Àì‚Üífrom True to False in version 0.22 and will be removed in 0.24. This will‚ê£ Àì‚Üíchange numeric results when test-set sizes are unequal.
DeprecationWarning) /home/edouard/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_ Àì‚Üísearch.py:841: DeprecationWarning: The default of the iid parameter will change‚ê£ Àì‚Üífrom True to False in version 0.22 and will be removed in 0.24. This will‚ê£ Àì‚Üíchange numeric results when test-set sizes are unequal.
DeprecationWarning) /home/edouard/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_ Àì‚Üísearch.py:841: DeprecationWarning: The default of the iid parameter will change‚ê£ Àì‚Üífrom True to False in version 0.22 and will be removed in 0.24. This will‚ê£ Àì‚Üíchange numeric results when test-set sizes are unequal.
DeprecationWarning) /home/edouard/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_ Àì‚Üísearch.py:841: DeprecationWarning: The default of the iid parameter will change‚ê£ Àì‚Üífrom True to False in version 0.22 and will be removed in 0.24. This will‚ê£ Àì‚Üíchange numeric results when test-set sizes are unequal.
DeprecationWarning) /home/edouard/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_ Àì‚Üísearch.py:841: DeprecationWarning: The default of the iid parameter will change‚ê£ Àì‚Üífrom True to False in version 0.22 and will be removed in 0.24. This will‚ê£ Àì‚Üíchange numeric results when test-set sizes are unequal.
DeprecationWarning)
Test r2:0.74 == ElasticNet (L1 penalty) == Test r2:0.58

ClassiÔ¨Åcation models with built-in cross-validation
from sklearn import datasets import sklearn.linear_model as lm import sklearn.metrics as metrics from sklearn.model_selection import cross_val_score
5.6. Resampling Methods

(continues on next page)
209

Statistics and Machine Learning in Python, Release 0.2
(continued from previous page) X, y = datasets.make_classification(n_samples=100, n_features=100,
n_informative=10, random_state=42)
# provide CV and score def balanced_acc(estimator, X, y, **kwargs):
Balanced accuracy scorer
return metrics.recall_score(y, estimator.predict(X), average=None).mean()
print("== Logistic Ridge (L2 penalty) ==") model = lm.LogisticRegressionCV(class_weight= balanced , scoring=balanced_acc, n_jobs=-1,‚ê£ Àì‚Üícv=3) # Let sklearn select a list of alphas with default LOO-CV scores = cross_val_score(estimator=model, X=X, y=y, cv=5) print("Test ACC:%.2f" % scores.mean())
== Logistic Ridge (L2 penalty) ==
/home/edouard/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic. Àì‚Üípy:1926: ChangedBehaviorWarning: The long-standing behavior to use the accuracy score‚ê£ Àì‚Üíhas changed. The scoring parameter is now used. This warning will disappear in version‚ê£ Àì‚Üí0.22.
ChangedBehaviorWarning) /home/edouard/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic. Àì‚Üípy:1926: ChangedBehaviorWarning: The long-standing behavior to use the accuracy score‚ê£ Àì‚Üíhas changed. The scoring parameter is now used. This warning will disappear in version‚ê£ Àì‚Üí0.22.
ChangedBehaviorWarning) /home/edouard/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic. Àì‚Üípy:1926: ChangedBehaviorWarning: The long-standing behavior to use the accuracy score‚ê£ Àì‚Üíhas changed. The scoring parameter is now used. This warning will disappear in version‚ê£ Àì‚Üí0.22.
ChangedBehaviorWarning)
Test ACC:0.77
/home/edouard/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic. Àì‚Üípy:1926: ChangedBehaviorWarning: The long-standing behavior to use the accuracy score‚ê£ Àì‚Üíhas changed. The scoring parameter is now used. This warning will disappear in version‚ê£ Àì‚Üí0.22.
ChangedBehaviorWarning) /home/edouard/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic. Àì‚Üípy:1926: ChangedBehaviorWarning: The long-standing behavior to use the accuracy score‚ê£ Àì‚Üíhas changed. The scoring parameter is now used. This warning will disappear in version‚ê£ Àì‚Üí0.22.
ChangedBehaviorWarning)
5.6.5 Random Permutations
A permutation test is a type of non-parametric randomization test in which the null distribution of a test statistic is estimated by randomly permuting the observations.

210

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2
Permutation tests are highly attractive because they make no assumptions other than that the observations are independent and identically distributed under the null hypothesis.
1. Compute a observed statistic Ì†µÌ±°Ì†µÌ±úÌ†µÌ±èÌ†µÌ±† on the data.
2. Use randomization to compute the distribution of Ì†µÌ±° under the null hypothesis: Perform Ì†µÌ±Å random permutation of the data. For each sample of permuted data, Ì†µÌ±ñ the data compute the statistic Ì†µÌ±°Ì†µÌ±ñ. This procedure provides the distribution of Ì†µÌ±° under the null hypothesis Ì†µÌ∞ª0: Ì†µÌ±É (Ì†µÌ±°|Ì†µÌ∞ª0)
3. Compute the p-value = Ì†µÌ±É (Ì†µÌ±° > Ì†µÌ±°Ì†µÌ±úÌ†µÌ±èÌ†µÌ±†|Ì†µÌ∞ª0) |{Ì†µÌ±°Ì†µÌ±ñ > Ì†µÌ±°Ì†µÌ±úÌ†µÌ±èÌ†µÌ±†}|, where Ì†µÌ±°Ì†µÌ±ñ‚Äôs include Ì†µÌ±°Ì†µÌ±úÌ†µÌ±èÌ†µÌ±†.
Example with a correlation
The statistic is the correlation.
import numpy as np import scipy.stats as stats import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline #%matplotlib qt
np.random.seed(42) x = np.random.normal(loc=10, scale=1, size=100) y = x + np.random.normal(loc=-3, scale=3, size=100) # snr = 1/2
# Permutation: simulate the null hypothesis nperm = 10000 perms = np.zeros(nperm + 1)
perms[0] = np.corrcoef(x, y)[0, 1]
for i in range(1, nperm): perms[i] = np.corrcoef(np.random.permutation(x), y)[0, 1]
# Plot # Re-weight to obtain distribution weights = np.ones(perms.shape[0]) / perms.shape[0] plt.hist([perms[perms >= perms[0]], perms], histtype= stepfilled ,
bins=100, label=["t>t obs (p-value)", "t<t obs"], weights=[weights[perms >= perms[0]], weights])
plt.xlabel("Statistic distribution under null hypothesis") plt.axvline(x=perms[0], color= blue , linewidth=1, label="observed statistic") _ = plt.legend(loc="upper left")
# One-tailed empirical p-value pval_perm = np.sum(perms >= perms[0]) / perms.shape[0]
# Compare with Pearson s correlation test _, pval_test = stats.pearsonr(x, y)
print("Permutation two tailed p-value=%.5f. Pearson test p-value=%.5f" % (2*pval_perm,‚ê£ Àì‚Üípval_test))

5.6. Resampling Methods

211

Statistics and Machine Learning in Python, Release 0.2
/home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£ Àì‚Üígot 216 from PyObject
return f(*args, **kwds) /home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 216, got 192
return f(*args, **kwds) /home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: ImportWarning: can t‚ê£ Àì‚Üíresolve package from __spec__ or __package__, falling back on __name__ and __path__
return f(*args, **kwds) /home/edouard/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy. Àì‚Üíufunc size changed, may indicate binary incompatibility. Expected 192 from C header,‚ê£ Àì‚Üígot 216 from PyObject
return f(*args, **kwds)
Permutation two tailed p-value=0.06959. Pearson test p-value=0.07355

Exercise
Given the logistic regression presented above and its validation given a 5 folds CV. 1. Compute the p-value associated with the prediction accuracy using a permutation test. 2. Compute the p-value associated with the prediction accuracy using a parametric test.
5.6.6 Bootstrapping
Bootstrapping is a random sampling with replacement strategy which provides an nonparametric method to assess the variability of performances scores such standard errors or conÔ¨Ådence intervals.

212

Chapter 5. Machine Learning

Statistics and Machine Learning in Python, Release 0.2

A great advantage of bootstrap is its simplicity. It is a straightforward way to derive estimates of standard errors and conÔ¨Ådence intervals for complex estimators of complex parameters of the distribution, such as percentile points, proportions, odds ratio, and correlation coefÔ¨Åcients.
1. Perform Ì†µÌ∞µ sampling, with replacement, of the dataset.
2. For each sample Ì†µÌ±ñ Ô¨Åt the model and compute the scores.
3. Assess standard errors and conÔ¨Ådence intervals of scores using the scores obtained on the Ì†µÌ∞µ resampled dataset.
import numpy as np from sklearn import datasets import sklearn.linear_model as lm import sklearn.metrics as metrics import pandas as pd
# Regression dataset n_features = 5 n_features_info = 2 n_samples = 100 X = np.random.randn(n_samples, n_features) beta = np.zeros(n_features) beta[:n_features_info] = 1 Xbeta = np.dot(X, beta) eps = np.random.randn(n_samples) y = Xbeta + eps
# Fit model on all data (!! risk of overfit) model = lm.RidgeCV() model.fit(X, y) print("Coefficients on all data:") print(model.coef_)
# Bootstrap loop nboot = 100 # !! Should be at least 1000 scores_names = ["r2"] scores_boot = np.zeros((nboot, len(scores_names))) coefs_boot = np.zeros((nboot, X.shape[1]))
orig_all = np.arange(X.shape[0]) for boot_i in range(nboot):
boot_tr = np.random.choice(orig_all, size=len(orig_all), replace=True) boot_te = np.setdiff1d(orig_all, boot_tr, assume_unique=False) Xtr, ytr = X[boot_tr, :], y[boot_tr] Xte, yte = X[boot_te, :], y[boot_te] model.fit(Xtr, ytr) y_pred = model.predict(Xte).ravel() scores_boot[boot_i, :] = metrics.r2_score(yte, y_pred) coefs_boot[boot_i, :] = model.coef_
# Compute Mean, SE, CI scores_boot = pd.DataFrame(scores_boot, columns=scores_names) scores_stat = scores_boot.describe(percentiles=[.99, .95, .5, .1, .05, 0.01])
print("r-squared: Mean=%.2f, SE=%.2f, CI=(%.2f %.2f)" %\ tuple(scores_stat.loc[["mean", "std", "5%", "95%"], "r2"]))
(continues on next page)

5.6. Resampling Methods

213

Statistics and Machine Learning in Python, Release 0.2

(continued from previous page)
coefs_boot = pd.DataFrame(coefs_boot) coefs_stat = coefs_boot.describe(percentiles=[.99, .95, .5, .1, .05, 0.01]) print("Coefficients distribution") print(coefs_stat)

Coefficients on all data:

[ 1.01094082 0.92410534 -0.07588245 0.13848944 0.19659906]

r-squared: Mean=0.59, SE=0.11, CI=(0.39 0.74)

Coefficients distribution

0

1

2

3

4

count 100.000000 100.000000 100.000000 100.000000 100.000000

mean 1.007552 0.927999 -0.072079 0.166443 0.188892

std

0.105309 0.118476 0.092189 0.109172 0.089533

min

0.710004 0.676758 -0.331354 -0.059170 -0.043120

1%

0.787546 0.695238 -0.261349 -0.058613 0.001962

5%

0.835648 0.741149 -0.219243 0.009053 0.050575

10%

0.878441 0.775851 -0.202235 0.042002 0.079449

50%

1.013152 0.921445 -0.083346 0.167349 0.190600

95%

1.211640 1.108971 0.052410 0.352416 0.345560

99%

1.254097 1.180027 0.150143 0.383904 0.399238

max

1.290094 1.264742 0.153999 0.511701 0.401858

214

Chapter 5. Machine Learning

‚Ä¢ genindex ‚Ä¢ modindex ‚Ä¢ search

CHAPTER
SIX
INDICES AND TABLES

215

