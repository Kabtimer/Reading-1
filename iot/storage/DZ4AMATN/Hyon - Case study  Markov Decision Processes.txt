Case Study: MDP
Case study : Markov Decision Processes
Emmanuel Hyon1
1Université Paris Nanterre LIP6, Sorbonne Universités
RESCOM Summer School, June 2019
1/29

Case Study: MDP
Outline
1 Introduction 2 Problem positioning 3 MDP Model 4 Case B = 1 : explicit computations 5 The Literature
2/29

Case Study: MDP Introduction
Outline
1 Introduction The Model The Problem
3/29

Case Study: MDP Introduction The Model
The Model
Arrivals

Queue

Server

Impatience

Controler

Arrival Customers arrive to an inﬁnite-buﬀer queue. Time is discrete. The distribution of arrivals in each slot At, arbitrary with mean λ (customers/slot)
4/29

Case Study: MDP Introduction The Model
The Model
Arrivals

Queue

Server

Impatience

Controler

Services Service occurs by batches of size B. Service time is one slot.
4/29

Case Study: MDP Introduction The Model
The Model
Arrivals

Queue

Server

Impatience

Controler

Deadline Customers are impatient: they may leave before service.
the individual probability of being impatient in each slot: α memoryless, geometrically distributed patience
4/29

Case Study: MDP Introduction The Model
The Model
Arrivals

Queue

Server

Impatience

Controler

Control Service is controlled. The controller knows the number of customers but not their amount of patience: just the distribution.
4/29

Case Study: MDP Introduction The Problem
The Question

What is the optimal policy π∗ of the controller, so as to minimise the θ-discounted global cost:

∞

vθπ(x ) = Eπx

θn c(xn, qn) ,

n=0

where:

xn: number of customers at step n;

qn: decision taken at step n; and c(x, q) is the cost incurred, involving:

cB : cost for serving a batch (setup cost) cH : per capita holding cost of customers cL: per capita loss cost of impatient customers.

5/29

Case Study: MDP Problem positioning
Outline
2 Problem positioning Problem positioning with discrete optimisation problem Short review of stochastic methods
6/29

Case Study: MDP Problem positioning Problem positioning with discrete optimisation problem
The problem type depend on the available information.
We deﬁne a realisation as the value taken by the random value after a draw. When all realisations are known In this case we face up a scheduling problem oﬀ-line. Selecting unitary task with release date and deadline.
⇒ Algorithm polynomial and Greedy Method [Chretienne]
Only the past realisation are known In this case we face up a scheduling problem on-line.
When we have no knowledge about the future tasks after the current slot, we face up an on-line deterministic problem. When we know only the distributions of the next tasks (customers) we face up a stochastic control problem or an on-line stochastic problem
7/29

Case Study: MDP Problem positioning Short review of stochastic methods
Methods that use the expected values

Presentation One replace the random value by its expectation value and one optimises on a deterministic problem.

Here this will be equivalent to have the same duration of impatiences and the same arrivals number by slot.
Disadvantage We can be very far from the optimum.

For example : We have 2 types of customers.

with P = 1/2 task T1 has duration m/2.

with

P

=

1/2

task

T2

has

duration

3 2

m.

But the average duration of the task is m !

8/29

Case Study: MDP Problem positioning Short review of stochastic methods
Scenario Method Description
We assume that we know the probabilities that a given number of realisations occur. We replace the realisation values (we build a scenario). We then obtain a deterministic problem that appears with a given probability that is known Pi . We optimise this deterministic problem and we get a value Vi We obtain the global cost by an average computation
V = Pi Vi
i
9/29

Case Study: MDP Problem positioning Short review of stochastic methods
Scenario Method Description
An example For example : We know that
with P = 1/3 we have scenario 1 In slot 1 : we have 1 arrival, no lost In slot 2 : we have 2 arrivals, 1 lost In slot 3 : we have 1 arrival, no lost
with P = 2/3 we have scenario 2 In slot 1 : we have 4 arrivals, no lost In slot 2 : we have 0 arrival, 3 lost In slot 3 : we have 2 arrivals, 1 lists
9/29

Case Study: MDP Problem positioning Short review of stochastic methods
Scenario Method Description (2)

Associated with scenario 1 we get a deterministic problem. We solve it and we obtain a value of the objective function V1∗.

Similarly for scenario 2, we obtain V2∗.

The global value of the problem is

V∗

=

1 3

V1∗

+

2 3

V2∗.

Disadvantages
Problem : Curse of dimensionality. Indeed, the more we want approach the optimal value the more we have scenarios.

10/29

Case Study: MDP Problem positioning Short review of stochastic methods
Scenario Method Description (3)
Disadvantages (2) We have a value but no action to take ! To ﬁnd an action one has to decompose the behaviour and the actions in a decision tree. An example (sub optimal) from van Heteynrick
1 With the solving we can get a decision. 2 On generate a sequence of scenarios.
For each of them we record the optimal decision 3 We select one with respect of one criteria
par ex. this one which appears the more.
11/29

Case Study: MDP Problem positioning Short review of stochastic methods
Sample Path Analysis Analysis of trajectories samples
The path of a stochastic process The path of a stochastic process X (t, ω) is the set of the values it takes along the time.
An analysis of paths allows to make it possible to draw observation about the optimal policy. Exchange arguments : We compare the trajectories which diﬀer at diﬀerent times with diﬀerent action. Advantage We do not need to make computation (stochastic order).
Disadvantage The passing from a property which is satisﬁed on a trajectory in a property which is satisﬁed in expectation is hard !.
12/29

Case Study: MDP MDP Model
Outline
3 MDP Model Optimal Policy
13/29

Case Study: MDP MDP Model
State action and temporal decomposition
State: xn: number of customers in the queue at time n. Action: qn is the decision at time n: if qn = 1 a service occurs, otherwise qn = 0 and no service takes place.
14/29

Case Study: MDP MDP Model
State action and temporal decomposition
Temporal Decomposition 1 Begining of slot. 2 Update the number of customers (instant operation): by counting the number of arrivals during the previous slot, by adding arrivals to the number of customers in the queue. 3 According to the number of present customers decide to launch a service or not. 4 Start of service. 5 Departures of the impatient waiting customers. 6 End of service (one time unit later). 7 Update the number of customers (instant operation): by taking into account departures. 8 End of the slot.
14/29

Case Study: MDP MDP Model
State dynamics equation

S(x): the (random) number of “survivors” after impatience, out of x customers initially present.
I (x): the number of impatient customers. =⇒ binomially distributed random variables
An+1: the number of arrived customers during slot n.

System Dynamics Equation

We can deﬁne the Equation that describes the evolution of the

state.

xn+1 = S [xn − qnB]+ + An+1 .

15/29

Case Study: MDP MDP Model
Transition probabilities

Following

xn+1 = S [xn − qnB]+ + An+1 .

We get the transition probabilities.

The transition probability from x when q is triggered, to arrive in y

is:
i =(x−qB)+

P (y |(x, q)) =

P(S = i)PA(y − i)

i =0

Idea of proof If the number of survivors is i, (y − i)+ customers should arrives to reach y .

16/29

Case Study: MDP MDP Model
Costs
The cost at step n is: cB qn + cLI ([xn − qnB]+) + cH [xn − qnB]+
Immediate Cost
c(x, q) = q cB + (cL α + cH ) (x − qB)+ = q cB + cQ (x − qB)+ .

Optimisation criterion:

∞

vθπ(x ) = Eπx

θn c(xn, qn) .

n=0

17/29

Case Study: MDP MDP Model Optimal Policy
Dynamic programming equation
The optimal value function Vθ∗(x) is solution to: The dynamic programming V (x) = min {cB q + cQ [x − Bq]+ + θE V (S([x − Bq]+) + A) }.
q∈{0,1}
18/29

Case Study: MDP MDP Model Optimal Policy
Optimal Policy
The optimal policy π∗ is stationnary Markovian and feedback: there exists a function of the state x, d(x), such that
π∗ = (d, d, . . . , d, . . .) and d(x) is given by: The optimal policy
d(x) = arg min {c(x, q) + θE V (S([x − Bq]+) + A) }.
q∈{0,1}
19/29

Case Study: MDP MDP Model Optimal Policy
Dynamic programming validity

We have unbounded costs so we can not use usual results but more speciﬁc ones. The required assumptions are given in Theorem 6.11.3 in Puterman and the following properties should hold: a) there exists a positive function on the state space, w , such that:

|c(x, q)|

sup

< +∞ ,

(1)

(x,q) w (x )

1

sup

p(y |x, q)w (y ) < + ∞ ,

(2)

(x,q) w (x ) y

b) for every µ, 0 µ < 1, there exists η, 0 η < 1 and some integer J, such that, for every J-tuple of Markov Deterministic decision rules π = (d1, . . . , dJ ), and every x,

µJ Pπ(y |x)w (y ) ηw (x) .

(3)

y

20/29

Case Study: MDP MDP Model Optimal Policy
Computation of the optimal policy
Since we exhibit the Bellman equation and all the elements we can compute the optimal policy. It suﬃces to code an algorithm for the resolution !! There were very few solvers of MDP and we had to compute the algorithm for each model. But structural results give us the optimal policy without the need for a numerical computations.
21/29

Case Study: MDP Case B = 1 : explicit computations
Outline
4 Case B = 1 : explicit computations Results Proof Computation of the threshold
22/29

Case Study: MDP Case B = 1 : explicit computations Results
Optimality Results for B = 1

Theorem The optimal policy is of threshold type: there exists a ν such that d (x ) = 1{x ν}.

Theorem

Let ψ be the number deﬁned by

ψ

=

cB

−

cQ 1 − (1 − α)θ

.

Then, 1 If ψ > 0, the optimal threshold is ν = +∞. 2 If ψ < 0, the optimal threshold is ν = 1. 3 If ψ = 0, any threshold ν 1 gives the same value.
23/29

Case Study: MDP Case B = 1 : explicit computations Proof
Method of Proof : Structured policies
Framework: propagation of properties through the dynamic programming operator (Puterman, Glasserman & Yao).
Theorem (Puterman, Theorem 6.11.3)
Let Vw be a set of functions on the state space adequately chosen. Assume that:
0. ∀ v ∈ Vw , ∃ d, Markov decision rule, such that Lv = Ld v . If, furthermore,
1 v ∈ V σ implies Lv ∈ V σ, 2 v ∈ V σ implies there exists a decision d such that
d ∈ Dσ ∩ arg mind Ld v , 3 V σ is a closed by simple convergence. Then, there exists an optimal stationary policy (d∗)∞ in Πσ with d ∗ ∈ arg mind Ld v .
24/29

Case Study: MDP Case B = 1 : explicit computations Proof
Method of Proof : One step
Property (Submodularity (Topkis, Glasserman & Yao, Puterman)) A function g is submodular if, for any x x ∈ X and any q q ∈ Q:
g (x, q) − g (x, q) g (x, q) − g (x, q).
Property (Monotone Control (Topkis, Glasserman & Yao, Puterman)) A control is said monotone if the function d x → q is monotone.
Theorem If Tv (x, q) is submodular over N × Q then x → arg minq Tv (x, q) is increasing in x.
25/29

Case Study: MDP Case B = 1 : explicit computations Proof
Propagation of structure
Theorem Let, for any function v , v˜(x) = min Tv (x, q). Then:
q
1 If v increasing, then v˜ increasing 2 If v increasing and convex then v˜ increasing convex
Theorem If v is increasing and convex, then Tv (x, q) is submodular over N × Q.
26/29

Case Study: MDP Case B = 1 : explicit computations Computation of the threshold
Optimal Threshold

The system under threshold ν evolves as: xn+1 = Rν (xn) := S [xn − 1{x ν}]+ + An+1 .

A direct computation gives:

Vν (x )

=

1

−

cQ θ(1 −

α)

θλ x+
1−θ

∞
Φ(ν, x) = θnP(Rν(n)(x) ν)

n=0

ψ = cB

−

cQ

.

1 − (1 − α)θ

+ ψ Φ(ν, x)

27/29

Case Study: MDP Case B = 1 : explicit computations Computation of the threshold
Optimal Threshold

The system under threshold ν evolves as: xn+1 = Rν (xn) := S [xn − 1{x ν}]+ + An+1 .

We can deduce the following Lemma
Lemma The function Φ(ν, x) is decreasing in ν

1, for every x.

27/29

Case Study: MDP The Literature
Outline
5 The Literature

28/29

Case Study: MDP The Literature
Related Literature
Optimal service control (without impatience):
K. Deb & R. Serfozo, Optimal control of batch service queues. Advances in Applied Probability, (1973).
K. Papadaki & W.B. Powell, Exploiting structure in adaptive dynamic programming algorithms for a stochastic batch service problem. European Journal of Operational Research (2002). Optimal admission with impatience:
Y. Kocaga & A. Ward, Admission Control for a Multi-Server Queue with Abandonment Queueing Systems (2010). Optimal control of service in presence of stochastic impatience:
E. Hyon & A. Jean-Marie Scheduling Services in a Queuing System with Impatience and Setup Costs The Computer Journal, 2012.

29/29

