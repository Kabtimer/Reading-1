Int J Data Sci Anal (2017) 3:267–283 DOI 10.1007/s41060-017-0050-5
REGULAR PAPER
The non-stationary stochastic multi-armed bandit problem
Robin Allesiardo1,2 · Raphaël Féraud1 · Odalric-Ambrym Maillard2

Received: 9 January 2017 / Accepted: 6 March 2017 / Published online: 30 March 2017 © Springer International Publishing Switzerland 2017

Abstract We consider a variant of the stochastic multiarmed bandit with K arms where the rewards are not assumed to be identically distributed, but are generated by a nonstationary stochastic process. We ﬁrst study the unique best arm setting when there exists one unique best arm. Second, we study the general switching best arm setting when a best arm switches at some unknown steps. For both settings, we target problem-dependent bounds, instead of the more conservative problem-free bounds. We consider two classical problems: (1) identify a best arm with high probability (best arm identiﬁcation), for which the performance measure by the sample complexity (number of samples before ﬁnding a near-optimal arm). To this end, we naturally extend the definition of sample complexity so that it makes sense in the switching best arm setting, which may be of independent interest. (2) Achieve the smallest cumulative regret (regret minimization) where the regret is measured with respect to the strategy pulling an arm with the best instantaneous mean at each step.
This paper extends the work presented in the DSAA’2015 Long Presentation paper “EXP3 with Drift Detection for the Switching Bandit Problem” [1]. Algorithms SER3 and SER4 are original and presented for the ﬁrst time.
B Robin Allesiardo
robin.allesiardo@gmail.com
Raphaël Féraud Raphael.Feraud@orange.com
Odalric-Ambrym Maillard odalricambrym.maillard@inria.fr
1 Orange Labs, Lannion, France 2 Team TAO, CNRS, Inria Saclay, Île de France - LRI, Orsay,
France

Keywords Multi-armed bandit · Non-stationarity · Adversarial bandit
1 Introduction
The theoretical framework of the multi-armed bandit problem formalizes the fundamental exploration/exploitation dilemma that appears in decision-making problems facing partial information. At a high level, a set of K arms is available to a player. At each turn, she has to choose one arm and receives a reward corresponding to the played arm, without knowing what would have been the received reward had she played another arm. The player faces the dilemma of exploring, that is playing an arm whose mean reward is loosely estimated in order to build a better estimate, or exploiting, that is playing a seemingly best arm based on current mean estimates in order to maximize her cumulative reward. The accuracy of the player policy at time horizon T is typically measured in terms of sample complexity or of regret. The sample complexity is the number of plays required to ﬁnd an approximation of the best arm with high probability. In that case, the player can stop playing after identifying this arm. The regret is the difference between the cumulative rewards of the player and the one that could be acquired by a policy assumed to be optimal.
The stochastic multi-armed bandit problem assumes the rewards to be generated independently from stochastic distribution associated with each arm. Stochastic algorithms usually assume distributions to be constant over time like with the Thompson Sampling (TS) [17], UCB [2] or Successive Elimination (SE) [6]. Under this assumption of stationarity, TS and UCB achieve optimal upper bounds on the cumulative regret with logarithmic dependencies on

123

268

Int J Data Sci Anal (2017) 3:267–283

T . SE algorithm achieves a near-optimal sample complexity.
In the adversarial multi-armed bandit problem, rewards are chosen by an adversary. This formulation can model any form of non-stationarity√. EXP3 algorithm [3,14] achieves an optimal regret of O( T ) against an oblivious opponent that chooses rewards before the beginning of the game, with respect to the best policy that pulls the same arm over the totality of the game. This weakness is partially overcame by EXP3.S [3], a variant of EXP3, that forgets the past adding at each time-step a proportion of the mean gain and achieves controlled regret with respect to policies that allow arm switches during the run.
The switching bandit problem introduces non-stationarity within the stochastic bandit problem by allowing means to change at some time-steps. As mean rewards stay stationary between those changes, this setting is also qualiﬁed as piecewise-stationary. Discounted UCB [13] and sliding-window UCB [8] are adaptations of UCB to the swi√tching bandit problem and achieve a regret bound of O( M T log T ), where M − 1 is the number of distribution changes. It is also worth citing Meta- Eve [10] that associates UCB with a mean change detector, resetting the algorithm when a change is detected. While no analysis is provided, it has demonstrated strong empirical performances.
Stochastic and adversarial Several variants combining stochastic and adversarial rewards have been proposed by Seldin and Slivkins [15] or Bubeck and Slivkins [5]. For instance, in the setting with contaminated rewards, rewards are mainly drawn from stationary distributions except for a minority of mean rewards chosen in advance by an adversary. In order to guarantee their proposed algorithm EXP3++ [15] achieves logarithmic guarantees, the adversary is constrained in the sense it cannot lowered the gap between arms more than a factor 1/2. They also proposed another variant called adversarial with gap [15] which assumes the existence of a round after which an arm persists to be the best. These works are motivated by the desire to create generic algorithms able to perform bandit tasks with various reward types, stationary, adversary or mainly stationary. However, despite achieving good performances on a wide range of problems, each one needs a speciﬁc parameterization (i.e., an instance of EXP3++ parameterized for stationary rewards may not perform well if rewards are chosen by an adversary).
Our contribution We consider a generalization of the stationary stochastic, piecewise-stationary and adversarial bandit problems. In this formulation, rewards are drawn from stochastic distributions of arbitrary means deﬁned before the beginning of the game. Our ﬁrst contribution is for the unique best arm setting. We introduce a deceptively simple variant of the Successive Elimination (SE) algorithm, called
Successive Elimination with Randomized Round-

Robin (SER3), and we show that the seemingly minor modiﬁcation—a randomized round-robin procedure—leads to a dramatic improvement of the performance over the original SE algorithm. We identify a notion of gap that generalizes the gap from stochastic bandits to the non-stationary case and derive gap-dependent (also known as problem-dependent) sample complexity and regret bounds, instead of the more classical and less informative problem-free bounds. We show for instance in Theorem 1 and Corollary 1 that SER3 achieves a non-trivial problem-dependent sample complexity scaling with Δ−2 and a cumulative regret in O(K log(T K /Δ)/Δ) after T steps, in situations where SE may even suffer from a linear regret, as supported by numerical experiments (see Sect. 5). This result positions, under some assumptions, SER3 as an alternative to EXP3 when the rewards are nonstationary.
Our second contribution is to manage best arm switches during the game. First, we extend the deﬁnition of the sample complexity in order to analyze the best arm identiﬁcation algorithms when best arm switches during the game. SER4 takes advantages of the low regret of SER3 by resetting the reward estimators randomly during the game and then starting a new phase of optimization. Against an optimal policy with N − 1 switches of the optimal arm (but arbitrarily many distribution switches), this new algorithm achieves an expected sample complexity of O(Δ−2 N K δ−1 log(K δ−1)), with probability 1 − δ, and an expected cumulative regret of O(Δ−1 N T K log(T K )) after T time-steps. A second algorithm for the non-stationary stochastic multi-armed bandit with switches is an alternative to the passive approach used in SER4 (the random resets). Second, algorithm EXP3.R takes advantage of the exploration factor of EXP3 to evaluate unbiased estimations of the mean rewards. Combined with a drift detector, this active approach resets the weights of EXP3 when a change of best arm is detected. We ﬁnally show that EXP3.R also obtains competitive proble√m-dependent regret minimization guarantees in O 3N C K T K log T , where C depends on Δ.
2 Setting
We consider a generalization of the stationary stochastic, piecewise-stationary and adversarial bandit problems where the adversary chooses before the beginning of the game a sequence of distributions instead of directly choosing a sequence of rewards. This formulation generalizes the adversarial setting since choosing arbitrarily a reward yk(t) is equivalent to drawing this reward from a distribution of mean yk(t) and a variance of zero. The stationary stochastic formulation of the bandit problem is a particular case, where the distributions do not change.

123

Int J Data Sci Anal (2017) 3:267–283

269

2.1 The problem

2.3 The notion of regret

Let [K ] = 1, . . . , K be a set of K arms. The reward
ykt (t) ∈ [0, 1] obtained by the player after playing the arm kt is drawn from a distribution of mean μkt (t) ∈ [0, 1]. The instantaneous gap between arms k and k at time t is:

Δk,k (t) d=ef μk (t) − μk (t).

(1)

The player competes against an optimal policy, assumed
as optimal (per example, always playing the arm with the highest mean reward). Let k∗(t) be the arm played by the
optimal policy at time t.

2.2 The notion of sample complexity

When the decision process does not lead to one ﬁnal decision, minimizing the sample complexity may not be an appropriate goal. Instead, we may want to maximize the cumulative gain obtained through the game which is equivalent to minimize the difference between the choices of an optimal policy and those of the player. We call this difference, the regret. We deﬁne the pseudocumulative regret as the difference of mean rewards between the arms chosen by the optimal policy and those chosen by the player.

Deﬁnition 2 (Pseudocumulative regret)

T

μk∗(t)(t ) − μkt (t ).

(2)

t =1

In the literature [12], the sample complexity of an algorithm is the number of samples needed by this algorithm to ﬁnd a policy achieving a speciﬁc level of performance with high probability. We denote δ ∈ (0, 1] the probability of failure. For instance, for the best arm identiﬁcation in the stochastic stationary bandit (that is when ∀k∀t, μk(t) = μk(t + 1) and k∗(t) = k∗(t + 1)), the sample complexity is the number of sample needed to ﬁnd, with a probability at least 1−δ, the arm k∗ with the maximum mean reward. Analysis in sample complexity is useful for situations where the knowledge of the optimal arm is needed to make one impact-full decision, for example to choose which one of several possible products to manufacture or for building hierarchical models of contextual bandits in a greedy way [7], reducing the exploration space.
Deﬁnition 1 (Sample complexity) Let A be an algorithm. An arm k is epsilon optimal if μk ≥ μ∗ − , with ∈ [0, 1]. The sample complexity of A performing a best arm identiﬁcation task is the number of observations needed to ﬁnd an -optimal arm with a probability of at least 1 − δ.
The usual notion of sample complexity—the minimal number of observations required to ﬁnd a near-optimal arm with high probability—is well adapted to the case when there exists a unique best arm during all the game, but makes little sense in the general scenario when the best arm can change. Indeed, after a best arm change, a learning algorithm requires some time-steps before recovering. Thus, we provide in Sect. 4 a meaningful extension of the sample complexity deﬁnition to the switching best arm scenario. This extended notion of sample complexity now takes into account not only the number of time-steps required by the algorithm to identify a near-optimal arm, but more generally the number of time-steps required before recovering a near-optimal arm after each change.

Usually, in the stochastic bandit setting, the distributions of rewards are stationary and the instantaneous gap Δk,k (t) = μk(t) − μk (t) is the same for all the time-steps.
There exists a non-reciprocate relation between the minimization of the sample complexity and the minimization of the pseudocumulative regret. For instance, algorithm UCB has an order optimal regret, but it does not minimize the sample complexity. UCB will continue to play suboptimal arms, but with a decreasing frequency as the number of plays increases. However, an algorithm with an optimal sample complexity, like Median Elimination [6], will also have an optimal pseudocumulative regret (up to some constant factors). More details on the relation between both lower bounds can be found in [4,9].
Therefore, the algorithms presented in this paper slightly differ according to the quantity to minimize, the regret or the sample complexity. For instance, when the target is the regret minimization, after identifying the best arm, the algorithms continue to sample it, whereas in the case of sample complexity minimization, the algorithms stop the sampling process when the best arm is identiﬁed. When best arm switches are considered, algorithms minimizing the sample complexity enter a waiting state after identifying the current best arm and do not sample the sequence for exploitation purposes (sampling the optimal arm still increases the sample complexity). However, they still have to parsimoniously collect samples for each action in order to detect best arm changes and face a new trade-off between the rate of sampling and the time needed to ﬁnd the new best arm after a switch.
3 Non-stationary stochastic multi-armed bandit with unique best arm
In this section, we present algorithm Successive Elimination with Randomized Round- Robin (SER3, see

123

270

Int J Data Sci Anal (2017) 3:267–283

Algorithm 1), a randomized version of Successive Elimination which tackles the best arm identiﬁcation problem when rewards are non-stationary.
3.1 A modiﬁed successive elimination algorithm
We elaborate on several notions required to understand the behavior of the algorithm and to relax the constraint of stationarity.
3.1.1 The elimination mechanism
The elimination mechanism was introduced by Successive Elimination [6]. Estimators of the rewards are built by sequentially sampling the arms. After τmin turns of roundrobin, the elimination mechanism starts to occur. A lower bound of the reward of the best empirical arm is computed and compared with an upper bound of the reward of all other arms. If the lower bound is higher than one of the upper bounds, then the associated arm is eliminated and stop being considered by the algorithm. Processes of sampling and elimination are repeated until the elimination of all arms except one.

Table 1 A sequence of mean rewards tricking a deterministic bandit algorithm

μk (t) t = 1 t = 2 t = 3 t = 4 t = 5 t = 6

k = 1 0.6

1

0.6

1

0.6

1

k = 2 0.4

0.8

0.4

0.8

0.4

0.8

stationarity and only requires independence. We remember the Hoeffding inequality:

Lemma 1 (Hoeffding’s inequality [11]) If X1, X2, . . . , Xτ are τ independent random variables and 0 ≤ Xi ≤ 1 for all (i = 1, 2, . . . , τ ), then for τ > 0

P

t i =1

Xi τ

−E

t Xi i=1 τ

≥τ

≤ 2 exp

−2

2 τ

τ

.

Thus, we can use this inequality to calculate conﬁdence bounds of empirical means computed with rewards drawn from non-identical distributions.

3.1.3 Randomization of the round-robin

Algorithm 1 Successive Elimination with Random-

ized Round- Robin (SER3)

input: δ ∈ (0, 0.5],

∈

[0,

1),

τmin

=

log

K δ

output: an -approximation of the best arm

S1 = [K ], ∀k, μˆ k (0) = 0, t = 1, τ = 1

While |Sτ | > 1

Shufﬂe Sτ For each k ∈ Sτ do

Play k

μˆ k (τ )

=

τ

−1 τ

μˆ k

(τ

− 1) +

yk (t) τ

t =t+1

End for
kmax = arg maxk∈S μˆ k (τ ) If τ ≥ τmin Remove from Sτ+1 all k such as:

μˆ max(τ ) − μˆ k (τ ) + ≥

2

4K τ 2

log

τ

δ

(3)

End if If |Sτ | = 1 and the algorithm performs a sample complexity minimization task
Return the last element of Sτ End if τ =τ +1 End while

3.1.2 Hoeffding’s inequality
Successive Elimination assumes that the rewards are drawn from stochastic distributions that are identical over time (rewards are identically distributed). However, the Hoeffding inequality used by this algorithm does not require

We illustrate the need of randomization with an example
tricking a deterministic algorithm (see Table 1). The best arm seems to be k = 1 as μ1(t) is greater than
μ2(t) at every time-step t. However, by sampling the arms with a deterministic policy playing sequentially k = 1 and then k = 2, after t = 6 the algorithm has only sampled rewards from a distribution of mean 0.6 for k = 1 and of mean 0.8 for k = 2. After enough time following this pattern, an elimination algorithm will eliminate the ﬁrst arm. Our
algorithm SER3 adds a shufﬂing of the arm set after each
round-robin cycle to Successive Elimination and avoids
this behavior.

3.1.4 Uniqueness of the best arm

The best arm identiﬁcation task assumes a criteria identifying the best arm without ambiguity. We deﬁne the optimal arm as:

T

k∗

=

arg

max
k∈[K ]

t =1

μk (t).

(4)

As an efﬁcient algorithm will ﬁnd the best arm before

the end of the run, we use Assumption 1 to ensure its

uniqueness at every time-step. First, we deﬁne some nota-

tions. A run of SER3 is a succession of round-robin. The

set [τ ] = {(t1, |S1|), . . . , (tτ , |Sτ |)} is a realization of SER3,

and ti is the time-step when the round-robin ith of size |Si |

starts (ti = 1 +

i −1 j =1

|Sj

|).

As

arms

are

only

eliminated,

123

Int J Data Sci Anal (2017) 3:267–283

271

Fig. 2 Cumulative regret of SER3, SE, UCB and EXP3 on Problem 1

Fig. 1 Two examples of sequence of mean rewards a Assumption 1 is satisﬁed as the mean gap remains positive. b Assumption 1 is not satisﬁed. This sequence involves a best arm switch as the mean gap becomes non-positive

|Si | ≥ |Si+1|. We denote T(τ ) the set containing all possible realizations of τ round-robin steps. Now, we can introduce Assumption 1 that ensures the best arm is the same at any time-step.
Assumption 1 (Positive mean gap) For any k ∈ [K ] − {k∗} and any [τ ] ∈ T(τ ) with τ ≥ τmin, we have:

Δ∗k ([τ ])

=

1 τ

τ i =1

ti +|Si |−1 j =ti

Δk∗,k ( j ) |Si |

>

0.

(5)

Assumption 1 is trivially satisﬁed when distributions are stationary, is quite weak (see, e.g., Fig. 1b) and can tolerate a large noise when τ is high. As the optimal arm must distinguish itself from others, instantaneous gaps are more constrained at the beginning of the game. It is quite similar to the assumption used by Seldin and Slivkins [15] to be able to achieve logarithmic expected regret on moderately

Fig. 3 Cumulative regret of SER3, UCB and EXP3 on Problem 2

contaminated rewards, i.e., the adversary does not lower the

averaged gap too much. Another analogy can be done with the

adversarial with gap setting [15], τmin representing the time

needed for the optimal arm to accumulate enough rewards

and to distinguish itself from the suboptimal arms.

Figure 1a illustrates Assumption 1. In this example the mean of the optimal arm k∗ is lower than the second one on

time-steps t ∈ {5, 6, 7}. Thus, even if the instantaneous gap is
negative during these time-steps, the mean gap Δ∗k ([τ ]) stays positive. The parameter τmin protects the algorithm from local

noise at the initialization of the algorithm. In order to ease

the reading of the results in the next sections, we here assume

τmin

=

log

K δ

.

Assumption 1 can be seen as a sanity-check assump-

tion ensuring that the best arm identiﬁcation problem indeed

makes sense. In Sect. 4, we consider the more general switch-

ing bandit problem. In this case, Assumption 1 may not be

veriﬁed (see Fig. 1b) and is naturally extended by dividing the

game in segments wherein Assumption 1 is satisﬁed (Figs. 2,

3 and 4).

123

272

Int J Data Sci Anal (2017) 3:267–283

Fig. 4 Cumulative regret of SER4, SW–UCB, EXP3.S, EXP3.R and Meta- Eve on Problem 3

3.2 Analysis

All theoretical results are provided for = 0 and therefore accept only k∗ as the optimal arm.

Theorem 1 (Sample complexity of SER3) For K ≥ 2, δ ∈

(0, 0.5],

and

τmin

=

log

K δ

,

the

sample

complexity

of

SER3

is upper bounded by:

K

K

O Δ2 log δΔ

where

Δ

=

min[τ ],k

1 τ

τ i =1

ti +|Si |−1 t =ti

Δk∗,k (t |Si |

)

.

The proof is given in “Proof of Theorem 1 and Theorem 2” of Appendix 2.
Guarantee on the sample complexity can be transposed in guarantee on the pseudocumulative regret. In that case, when only one arm remains in the set, the player continues to play this last arm until the end of the game.

Corollary 1 (Expected pseudocumulative regret of SER3) For K ≥ 2, and δ = 1/T , and τmin = log(K T ), the expected pseudocumulative regret of SER3 is upper bounded by:

K −1

KT

min O Δ log Δ

,O

T T K log
K

The proof is given in “Proof of Corollary 1” of Appendix 2.
These guarantees are the same as the original Successive Elimination performed with a deterministic round-robin on arms with stationary rewards. Indeed, when reward distribu-

tions are stationary, we have for all t and all [τ ]:

1 τ

τ i =1

ti +|Si |−1 t =ti

Δk∗,k (t ) |Si |

=

Δk∗,k (t )

=

Δk∗,k (t

+ 1).

(6)

However, in a non-stationary environment satisfying Assumption 1 Successive Elimination will eliminate the optimal arm if the adversary knows the order of its roundrobin before the beginning of the run and exploits this knowledge against the learner, thus resulting in a linear cumulative regret. Our modiﬁcation of SE algorithm allows SER3 to perform on near adversarial sequence of reward while achieving a gap-dependent logarithmic pseudocumulative regret.
Remark These logarithmic guarantees result from Assumption 1 that allows to stop the exploration of eliminated arms. They do not contradict the lo√wer bound for non-stationary bandit whose scaling is in Ω( T ) [8] as it is due to the cost of the constant exploration for the case where the best arm changes.

3.3 Non-stationary stochastic multi-armed bandit with budget

We study the case when the sequence from which the rewards

are drawn does not satisfy Assumption 1.

The sequence of mean rewards is build by the adversary

in two steps. First, the adversary choose the mean rewards

μk(1), . . . , μk(T ) associated with each arm in such a way

that Assumption 1 is satisﬁed. The adversary can then apply

a malus bk(t) ∈ [0, μk(t)] to each mean reward to obtain

the ﬁnal sequence. The mean reward of the arm k at time t

is μk(t) − bk(t). The budget spent by the adversary for the

arm k is Bk =

T t =1

bk (t).

We

denote

B

≥

arg

maxk

Bk

the

upper bound on the budget of the adversary.

Algorithm SER3 can be modiﬁed to perform a best arm

identiﬁcation task when Assumption 1 is not satisﬁed but

B is known. To achieve that, we replace the condition of

elimination (Inequality (3) in Algorithm 1) is replaced by

the following:

μˆ max(τ ) − μˆ k (τ ) +

≥

B τ

+2

1 2τ log

4K τ 2 δ

This new algorithm is called Successive Elimina-

tion with Round- Robin Randomized and Budget

(SER3.B).

Theorem 2

For

K

≥ 2, δ

∈ (0, 0.5], and τmin

= log

K δ

,

the

sample complexity of SER3.B is upper bounded by:

O

K Δ2

log

K δΔ

+

B

123

Int J Data Sci Anal (2017) 3:267–283

where

Δ

=

min[τ ],k

1 τ

τ i =1

ti +|Si |−1 t =ti

Δk∗,k (t |Si |

)

.

The proof is given in “Proof of Theorem 1 and Theorem 2” of Appendix 2.

4 Non-stationary stochastic multi-armed bandit with best arm switches

The switching bandit problem has been proposed by Garivier et al. [8] and assumes means to be stationary between switches. In particular, algorithm SW–UCB is built on this assumption and is a modiﬁcation of UCB using only the rewards obtained inside a sliding window. In our setting, we allow mean rewards to change at every time-steps and consider that a best arm switch occurs when the arm with the highest mean change. This setting provides an alternative to the adversarial bandit with budget, when B is very high or unknown.
The optimal policy is the sequence of couples (optimal arm, time when the switch occurred):

{(k1∗,

1),

.

.

.

,

(k

∗ N

,

TN

)},

(7)

with kn∗ = kn∗+1 and Δkn∗,k (t) > 0 for any k ∈ [K ] − {kn∗} and any t ∈ [Tn, Tn+1). The optimal policy starts playing the arm kn∗ at the time-step Tn. Time-steps Tn when switches occur are unknown to the player.

273

T ], with T1 = 1 < T2 < · · · < TN < T , is:

N Tn+1−1

max(s(t ), 1 kt =kn∗ ),

(8)

n=1 t=Tn

where s(t) is a binary variable equal to 1 if and only if the
time-step t is used by the sampling process of A, kt is the arm identiﬁed as optimal by A at time t, kn∗ is the optimal arm over the segment n and TN+1 = T + 1.

In order to clarify Deﬁnition 3, we detail the different states achievable by an algorithm of best arms identiﬁcation and their impact on the sample complexity. Two states are achievable during a task of minimization of the sample complexity:

– s(t) = 1 if the algorithm is sampling an arm during the time-step t. In the case of SER4, s(t) = 1 when |Sτ | = 1 and the sample complexity increases by one.
– s(t) = 0 if the algorithm submits an arm as the optimal one during the time-step t. In the case of SER4, s(t) = 0 when |Sτ | = 1. The sample complexity increases by one if kt = k∗(t).

In the context of SER4, the sample complexity is the number of time-steps during which the arm set does not only contain the optimal arm.

4.1.2 Algorithm

4.1 Successive Elimination with Randomized Round-Robin and Resets (SER4)
Deﬁnition 1 of the sample complexity is not adapted to the switching bandit problem. Indeed, this deﬁnition is used to measure the number of observations needed by an algorithm to ﬁnd one unique best arm. When the best arm changes during the game, this deﬁnition is too limiting. In Sect. 4.1.1 we introduce a generalization of the sample complexity for the case of switching policies.
4.1.1 The sample complexity of the best arm identiﬁcation problem with switches
A cost associated is added to the usual sample complexity. This cost is equal to the number of iterations after a switch during which the player does not know the optimal arm and does not sample.
Deﬁnition 3 (Sample complexity with switches) Let A be an algorithm. The sample complexity of A performing a best arms identiﬁcation task for a segmentation {Tn}n=1..N of [1 :

In order to allow the algorithm to choose another arm when a switch occurs, at each turn, estimators of SER3 are reseted with a probability ϕ ∈ [0, 1] and a new task of best arm identiﬁcation is started. We name this algorithm Successive Elimination with Randomized Round- Robin
and Resets (SER4).

4.1.3 Analysis.

We now provide the performance guarantees of SER4 algorithm, both in terms of sample complexity and of pseudocumulative regret.
The following results are given in expectation and in high probability. The expectations are taken with regard to the randomization of the resets. The sample complexity or the pseudocumulative regret achieved by the algorithm between each resets (given by the analysis of SER3) still results in high probability.

Theorem 3 (Expected sample complexity of SER4) For

K

≥

2,

δ

=

1/T ,

τmin

=

log

K δ

and

ϕ

∈

(0, 1],

the

expected

sample complexity of SER4 w.r.t. the randomization of resets

is upper bounded by:

123

274

Int J Data Sci Anal (2017) 3:267–283

Algorithm 2 Successive Elimination with Random-

ized Round- Robin and Resets (SER4)

input: δ ∈ (0, 1], ∈ [0, 1), ϕ ∈ [0, 1)

S1 = [K ], ∀k, μˆ k (0) = 0, t = 1, τ = 1 While t ≤ T

Shufﬂe Sτ

For each k ∈ Sτ do

If |Sτ | = 1 or If the algorithm performs a regret minimization task

Play k

μˆ k (τ )

=

τ

−1 τ

μˆ k

(τ

− 1) +

ykt (t) τ

End if

t =t+1

End for

kmax = arg maxk∈S μˆ k (τ ) Remove from Sτ+1 all k such as:

μˆ max(τ ) − μˆ k (τ ) +

≥2

1 2τ log

4K τ 2 δ

τ =τ +1 t =t+1 With a probability ϕ
St = [K ] ∀k, μˆ k (t) = 0 τ =1
End with a probability
End while

O

ϕK δΔ2 log

K δΔ

+N ϕ

with a probability of at least 1 − δ.

The proof is given in “Proof of Theorem 3” of Appendix 2. We tune ϕ in order to minimize the sample complexity.

Corollary 2

For

K

≥

2, δ

= 1/T , τmin

=

log

K δ

,

Δ

≥

1 KT

and ϕ =

K

Nδ log(

K δ

)

,

the

expected

sample

complexity

of

SER4

w.r.t. the randomization of resets is upper bounded by:

⎛

O

⎝

1 Δ2

⎞

N

K

log(

K δ

)

⎠

.

δ

Remark 2 Transposing Theorem 3 for the case where ∈

[

1 KT

,

1]

is

straightforward.

This

allows

to

tune

the

bound

by

setting ϕ = (N δ)/(K log(T K )).

This result can also be transposed in bound on the expected cumulative regret. We consider that the algorithm continues to play the last arm of the set until a reset occurs.

Corollary 3 (Expected cumulative regret of SER4) For K ≥

2, and δ

=

1/ T , τmin

=

log(K T ), Δ

≥

1 KT

and ϕ

=

T

K

N log(K

T

)

,

the

expected

cumulative

regret

of

SER4

w.r.t.

the randomization of resets is upper bounded by:

min O

N T K log(K T )

,O

T 2/3

T N K log

.

Δ

K

(9)

The proof is given in “Proof of Corollary 3” of Appendix

2.

Remark 3

A

similar

dependency

in

√ T

Δ−1

appears

also

in

SW–UCB (see Theorem 1 in [8]) and is standard in this type

of results.

4.2 EXP3 with resets

SER4 and other algorithms from the state of the art [3,8,13] use a passive approach through forgetting the past. In this subsection, we propose an active strategy which consists in resetting the reward estimations when a change of the best arm is detected. A supposed advantage of this approach is to let the algorithm converge on a longer time period, as it is reset only when a switch is detected, and thus build a more accurate estimate of the reward distributions. First, we describe the adversarial bandit algorithm EXP3 [3], which will be used by proposed algorithm EXP3.R between detections. We then describe the drift detector used to detect changes of the best arm. Finally, we combine the both to obtain EXP3.R algorithm.

Algorithm 3 EXP3
The parameter γ ∈ [0, 1] controls the exploration and the probability to choose an action k at round t is:

pk (t) = (1 − γ )

wk (t)

k i =1

wi

(t

)

+

γ K

,

(10)

where the weight wk (t) of each action k is computed from the unbiased cumulative reward estimator Xˆ k (t):

wk (t)

=

exp(

γ K

Xˆ k (t)),

(11)

with

Xˆ k (t) =

t j =tr

xk( j) pk ( j )

k = k( j)

,

where tr is the time-steps when the algorithm is initialized.

(12)

4.2.1 EXP3 algorithm
EXP3 algorithm (see Algorithm 3) minimizes the regret against the best arm using an unbiased estimation of the

123

Int J Data Sci Anal (2017) 3:267–283

275

cumulative reward at time t for computing the choice probabilities of each action. While this policy can be viewed as optimal in an actual adversarial setting, in many practical cases the non-stationarity within a time period exists but is weak and is only noticeable between different periods. If an arm performs well in a long time period but is extremely bad on the next period, EXP3 algorithm can need a number of trial equal to the ﬁrst period’s length to switch its most played arm.

4.2.2 The detection test

The detection test (see Algorithm 4) uses conﬁdence intervals

to estimate the expected reward in the previous time period.

The action distribution in EXP3 is a mixture of uniform and

Gibbs distributions. We call γ -observation an observation

selected though the uniform distribution. Parameters γ , H

and δ deﬁne the minimal number of γ -observations by arm

needed to call a test of accuracy with a probability 1 − δ.

They will be ﬁxed in the analysis (see Corollary 4), and the

correctness of the test is proven in Lemma 2. We denote μ¯ k (I )

the empirical mean of the rewards acquired from the arm k

on the interval I using only γ -observations and Γmin(I) the

smallest number of γ -observations among each action on the

interval

I.

The

detector

is

called

only

when

Γmin(I )

≥

γH K

.

The detector raises an alert when the action kmax with the

highest empirical mean μˆ k(I − 1) on the interval I − 1 is

eliminated by an other on the current interval.

count of

γH K

γ

-observations per

arm

is fulﬁlled,

the

detection

test is called. If in the corresponding interval, the empirical

mean of an arm exceeds by 2 the empirical mean of the

current best arm, then a drift detection is raised. In this case,

weights of EXP3 are reset. Then a new interval of collect

begins, preparing the next test. These steps are repeated until

the run ends (see Algorithm 5).

Algorithm 5 EXP3 with Resets

Parameters: Reals δ, γ and Integer H I =1 for each t = 1, . . . , T do

Run EXP3 on time step t

if

Γmin(I ) ≥

γH K

then

if Dri f t Detection(I ) then

Reset EXP3

end if I = I +1

end if

end for

4.2.4 Analysis

In this section we analyze the drift detector, and then, we bound the expected regret of EXP3.R algorithm.

Assumption 2 (Accuracy of the drift detector) During each

of

the

segments

S

where

k

∗ S

is

the

optimal

arm,

the

gap

between

k

∗ S

and

any

other

arm

is

of

at

least

4

with

Algorithm 4 DriftDetection(I)

Parameters: Current interval I kmax = arg maxμˆ k (I − 1)
k

=

K

log(

1 δ

)

2γ H

return ∃k, μˆ k (I ) − μˆ kmax (I ) ≥ 2

4.2.3 EXP3.R algorithm
Coupled with a detection test, EXP3 algorithm has several advantages. First in a non-stationary environment, we need a constant exploration to detect changes where a suboptimal arm becomes optimal and this exploration is naturally given by the algorithm. Second, the number of breakpoints is higher than the number of best arm changes (M ≥ N ). This means that the number of resets needed by EXP3 is lower than the one needed by a stochastic bandit algorithm such as UCB. Third, EXP3 is robust against test failures (non-detection) or local non-stationarity. We call EXP3.R algorithm obtained by combining EXP3 and the drift detector. First, one instance of EXP3 is initialized and used to select actions. When the

=

K

log(

1 δ

4γ H

)

.

(13)

Lemma 2 guarantees that when Assumption 1 holds and
the interval I is included into the interval S, then, with high
probability, the test will raise a detection if and only if the optimal action kS∗ eliminates a suboptimal action.

Lemma 2 (Arm switches are detected) When Assumption 2

holds and I ⊆ S, then, with a probability 1 − 2δ, for any

k

=

k

∗ S

:

μˆ k∗S (I ) − μˆ k (I ) ≥ 2

K

log(

1 δ

)

⇔

μk

∗ S

(

I

)

≥

μk (I ).

(14)

2γ H

The proof is given in “Proof of Lemma 2” of Appendix 2. Theorem 4 bounds the expected cumulative regret of EXP3.R.

Theorem 4 (Expected cumulative regret of EXP3.R) For

any

K

>

0, 0

<

γ

≤

1, 0

≤

δ

<

1 2

,

H

≥

K

and

N

≥

1

123

276

Int J Data Sci Anal (2017) 3:267–283

when Assumption 2 holds, the expected cumulative regret of EXP3.R is

G∗ − E[GEXP3.R] ≤ (e − 1)γ T

+

N

−1+

K δT H

+ Kδ

K log(K )

γ

+ (N − 1)H K

1

1 −

2δ

+

1

.

(15)

The proof is given in “Proof of Theorem 4” of Appendix 2.
In Corollary 4 we optimize parameters of the bound obtained in Theorem 4.
Corollary 4 (Expected cumulative regret of EXP3.R) For any K ≥ 1, T ≥ 10, N ≥ 1 and C ≥ 1 when Assumption 1 holds, the expected cumulative regret of EXP3.R run with input parameters

δ = log T , γ = K log K log T and H = C T log T

KT

T

(16)

is

G∗ − E[GEXP3.R]) ≤ (e − 1) T K log K log T

+ N T K log K

+ (C + 1)K T log K

+ 3N C K T log T .

(17)

The proof is given in “Proof of Corollary 4” of Appendix 2.
According to C, the precision is:

=

1

log

KT log T

K.

(18)

2C log T log K

Notice that, when T increases, toward a constant.

log

KT log T

log T

K log K

tends

5 Numerical experiments
We compare our algorithm with the state of the art. For each problem, K = 20 and T = 107. The instantaneous gap between the optimal arm and the others is constant, Δ = 0.05, i.e., the mean of the optimal arm is μ∗(t) =

μ(t) + Δ. During all experiments, probabilities of failure of Successive Elimination (SE), SER3 and SER4 are set to δ = 0.05. Constant explorations of all algorithms of the EXP3 family are set to γ = 0.05. Results are averaged over 50 runs. On problems 1 and 2 (Figs. 2 and 3), variances are low (in the order of 103) and thus not showed. On problem 3 (Fig. 4), variances are plotted as the gray areas under the curves.
5.1 Problem 1: sinusoidal means
The index of the optimal arm k∗ is drawn before the game and does not change. The mean of all suboptimal arm is μ(t) = cos(2π t/K )/5 + 0.5.
This problem challenges SER3 against SE, UCB and EXP3. SER3 achieves a low cumulative regret, successfully eliminating suboptimal arms at the beginning of the run. Contrarily, SE is tricked by the periodicity of the sinusoidal means and eliminates the optimal arm. The deterministic policy of UCB is not adapted to the non-stationarity of rewards, and thus, the algorithm suffers from a high regret. The unbiased estimators of EXP3 enable the algorithm to quickly converge on the best arm. However, EXP3 suffers from a linear regret due to its constant exploration until the end of the game.
5.2 Problem 2: decreasing means with positive gap
The optimal arm k∗ does not change during the game. The mean of all suboptimal arms is μ(t) = 0.95 − min(0.45, 10−7t).
On this problem, SER3 is challenged against SE, UCB and EXP3. SER3 achieves a low cumulative regret, successfully eliminating suboptimal arms at the beginning of the run. Contrarily to problem 1, mean rewards evolve slowly and Successive Elimination (SE) achieves the same level of performance than SER3. Similarly to problem 1, UCB achieves a high cumulative regret. The cumulative regret of EXP3 is low at the end of the game but would still increase linearly with time.
5.3 Problem 3: decreasing means with arm switches
At every turn, the optimal arm k∗(t) changes with a probability of 10−6. In expectation, there are 10 switches by run. The mean of all suboptimal arms is μ(t) = 0.95 − min(0.45, 10−7(t[mod 106]).
On problem 3, SER4 is challenged against SW–UCB, EXP3.S, EXP3.R and Meta- Eve. The probability of reset of SER4 is ϕ = 5−5. The size of the window of SW–UCB is 105. The historic considered by EXP3.R is H = 4 × 105, and the regularization parameter of EXP3.S is α = 10−5.
SER4 obtains the lowest cumulative regret, conﬁrming the random resets approach to overcome switches of best arm.

123

Int J Data Sci Anal (2017) 3:267–283

277

SW–UCB suffers from the same issues as UCB in previous problems and obtains a very high regret. Constant changes of mean cause Meta- Eve to reset very frequently and to obtain a lower regret than SW UCB. EXP3.S and EXP3.R achieve both low regrets but EXP3.R suffers from the large size of historic needed to detect switches with a gap of Δ. We can notice that the randomization of resets in SER4, while allowing to achieve the best performances on this problem, involves a highest variance. Indeed, on some runs, a reset may occur lately after a best arm switch, whereas the use of windows or regularization parameters will be more consistent.
6 Conclusion
We proposed a new formulation of the multi-armed bandit problem that generalize the stationary stochastic, piecewisestationary and adversarial bandit problems. This formulation allows to manage difﬁcult cases, where the means rewards and/or the best arm may change at each turn of the game. We studied the beneﬁt of random shufﬂing in the design of sequential elimination bandit algorithms. We showed that the use of random shufﬂing extends their range of application to a new class of best arm identiﬁcation problems involving non-stationary distributions, while achieving the same level of guarantees than SE with stationary distributions. We introduced SER3 and extended it to the switching bandit problem with SER4 by adding a probability of restarting the best arm identiﬁcation task. We extended the deﬁnition of the sample complexity to include switching policies. Up to our knowledge, we proved the ﬁrst sample complexitybased upper bound for the best arm identiﬁcation problem with arm switches. The upper bound over the cumulative regret of SER4 depends only of the number N − 1 of arm switches, as opposed to the number of distribution changes M − 1 in SW–UCB (M ≥ N can be of order T in our setting). Algorithm EXP3.R also achieves a competitive regret bound. The adversarial nature of EXP3 makes it robust to non-stationarity, and the detection test accelerates the switch when the optimal arm changes while allowing convergence of the bandit algorithm during periods where the best arm does not change.
Acknowledgements This work was supported by Team TAO (CNRS - Inria Saclay, Île de France - LRI), Team Proﬁling and Data-mining (Orange Labs).
Appendix 1: A summary of the contributions
We provide in Tables 2 and 3 a brief summary of the existing results regarding the performance of a few algorithms, together with the contributions of this article, that are indicated in bold.

In both tables, T is the time horizon, assumed to be known, K is the number of arms, Δ is the gap, and δ is the probability of success of the algorithm. C is quantity similar to the gap, described in Sect. 4.2.4. Finally, M is the number of breakpoints (the mean reward of an arm changes) and N the number of best arm switches.

Appendix 2: Technical results

Proof of Theorems 1 and 2

Proof Theorem 1 is a special case of Theorem 2. For Theorem 1, for every k and every t, B = 0, Bk = 0 and bk(t) = 0.
The proof consists of three main steps. The ﬁrst step makes explicit the conditions leading to the elimination of an arm from the set. The second step shows that the optimal arm will not be eliminated with high probability. Finally, the third step shows that a suboptimal arm will be eliminated after at most a critical number of steps τ ∗, which then allows to derive an upper bound on the sample complexity.
Step 1 Conditions for the elimination of an arm From Hoeffding’s inequality, for any deterministic roundrobin length τ and arm k we have:

P

|μˆ k − E[μˆ k]| ≥

τ

≤ 2 exp

−2

2 τ

τ

.

where E denotes the expectation with respect to the distribution Dy. By setting

t=

1 log

4K τ 2

, we have:

2τ

δ

P |μˆ k − E[⎛μˆ k(τ )]| ≥ t

≤ 2 exp ⎝−2

1 2τ log

4K τ 2 δ

2⎞ τ 2⎠

δ = 2Kτ2 .

Applying Hoeffding’s inequality for each round-robin size

τ ∈ N , applying a standard union bound and using that

∞ τ =1

1/τ 2

=

π 2/6,

the

following

inequality

holds

simulta-

neously

for

any

τ

with

a

probability

at

least

1

−

δπ2 12K

:

μˆ k (τ ) − τ ≤ E[μˆ k ] ≤ μˆ k (τ ) + τ .

(19)

Let Si ⊂ {1, . . . , K } be the set containing all the arms that are not eliminated by the algorithm at the start of the ith round-robin. By construction of the algorithm, an arm k

123

278
Table 2 Overview of the different bandit algorithms for policies with unique best arm

Algorithms

Regret

State of the art UCB SE EXP3 EXP3++ Our contribution SER3

O Δ−1 K log(T ) O Δ−1 K log(T K /Δ)
√ O K T log K O(Δ−1 K log3 T ) + O˜ (Δ−3)
O Δ−1 K log(T K /Δ)

Int J Data Sci Anal (2017) 3:267–283

Sample complexity

Non-stationarity

X

No

O Δ−2 K log(T K /Δ)

No

X

Yes

X

Yes

O Δ−2 K log(T K /Δ)

Yes

Table 3 Overview of the different bandit algorithms for policies with switching best arm

Algorithms

Regret

Sample complexity

State of the art SW–UCB EXP3.S Our contributions SER4 EXP3.R

O Δ−1√M T log T O N K T log(K T )
O Δ−1 N K T log(K T ) √
O 3N C K T K log T

X X
O Δ−2 N K δ−1 log(K δ−1) X

Non-stationarity between breakpoints
No Yes
Yes Yes

remains in the set of selected arms as long as for each arm k ∈ Sτ − {k }:

μˆ k (τ ) − τ < μˆ k (τ ) + τ and τ ≥ τmin

(20)

Combining (19) and the left inequality of (20), it holds on an event Ω of high probability

Taking the expectation of Eq. (23) with respect to the randomization of the round-robin we have:

⎛
E[μˆ k(τ )] = ⎝
r ∈T(τ )

r = [τ ] τ

⎞

τ ti +|Sτ |−1 μk ( j ) ⎠ − Bk .

i=1 j =ti

|Si |

τ

(24)

E[μˆ k(τ )] − 2 τ < E[μˆ k (τ )] + 2 τ .

(21)

We denote tτ , the time-step where the τ th round-robin

starts (tτ = 1+

τ −1 i =1

|Si

|).

Let

us

remind

that

T(τ

)

is

the

set

containing all possible realizations of τ sequences of round-

robin. Each arm k is played one time during each round-robin

phase, and thus, τ observations per arm are available after

τ th round-robin phases. The empirical mean reward μˆ k(τ ) of each arm k after the τ th round-robin is:

μˆ k(τ ) =

1 r=[τ ] τ

tτ +|Sτ |−1
yk ( j )1 k=k j .

r ∈T(τ )

j =1

(22)

Decomposing the second sum in round-robin phases and taking the expectation with respect to the reward distribution Dy we have:

EDy [μˆ k (τ )]
=
r ∈T(τ )

r = [τ ] τ

τ ti +|Sτ |−1
(μk ( j ) − bk (t)) k = k j .
i=1 j =ti
(23)

Now, under the event Ω for which (21) holds for k and k , we deduce by using (24) that

⎛

⎞

r = [τ ]

τ
⎝

ti +|Sτ |−1 μk ( j ) −

τ

ti +|Sτ |−1 μk ( j ) ⎠

τ
r ∈T(τ )

i=1 j =ti

|Si |

i=1 j =ti

|Si |

<4

τ

+

Bk τ

−

Bk τ

+ B. τ

(25)

Let us introduce the following mean-gap quantity

Δk,k ([τ ]) =

1 r=[τ ] τ

r∈T⎛(τ )

⎞

τ
×⎝

ti +|Sτ |−1 μk ( j ) −

τ

ti +|Sτ |−1 μk ( j )⎠.

i=1 j =ti

|Si |

i=1 j =ti

|Si |

Replacing the value of t in (25), it comes

Δk,k ([τ ]) < 4

1 2τ log

4K τ 2 δ

+ Bk − Bk + B , τ ττ

123

Int J Data Sci Anal (2017) 3:267–283

279

Δk,k

([τ ])2

<

8 τ

log

4K τ 2 δ

+ Bk − Bk + B . τ ττ

(26)

An arm will be eliminated if (26) becomes false and if τ ≥ τmin.
Step 2 The optimal arm is not eliminated For k = k∗ et k = k∗, in the worst case Bk = 0 and Bk = B. After injecting those quantities in (26), we have:

Δk,k

([τ ])2

<

8 τ

log

4K τ 2 δ

.

(27)

By assumption (Δk,k∗ ([τ ]) is negative after τmin), (27) is

always true when τ ≥ τmin,implying that the optimal arm

will always remain in the set with a probability of at least

1−

δ K

for all τ .

Step 3 The elimination of suboptimal arms
If the arm k still remains in the set, it will be eliminated if inequality (26) is not satisﬁed and if τ ∗ ≥ τmin.
Let us consider k = k∗, k = k∗, and deﬁne the quantity

+2 log log 16K , δΔk([τ ])

=

8Δk ([τ ])2

642

log

16K δΔk ([τ ])

×

log

4K δ

− 4 log Δk([τ ]) + 24 log 2

+2

log

log

16K δΔk ([τ

])

,

≤

8Δk ([τ ])2

642

log

16K δΔk ([τ ])

×

4

log

16K δΔk([τ ])

+

24

log

2

+2

log

log

16K δΔk ([τ

])

.

We remark that for X > 8 we have

24 log 2 + 2 log log X < 8 log X.

Δk([τ ]) =

1 r=[τ ] τ

r∈T⎛(τ )

⎞

τ
×⎝

ti +|Sτ |−1 μk ( j ) −

τ

ti +|Sτ |−1 μk ( j ) ⎠ .

i=1 j =ti

|Si |

i=1 j =ti

|Si |

In the worst case, Bk∗ = B et Bk = 0. Using equation (26) we obtain the condition to invalidate to eliminate the arm of index k:

Δk,k

([τ ])2

<

8 τ

log

4K τ 2 δ

+

2B τ

.

(28)

Let us also introduce for convenience the critical value

Hence, provided that for K ≥ 2, δ ∈ (0, 0.5] and

Δk([τ ])

>

0,

we

have

4K δΔk ([τ ])

>

8

and

C1(τ1∗)

≤

8Δk ([τ ])2

642

log

16K δΔk ([τ ])

16K 16 log δΔk([τ ])

≤ Δk ([τ ])2 .

(29)

512

trueAfsoCr a1l(lττ1∗)>isτs1∗tr.ictly decreasing with regard to t, (29) is When t > τ1∗, it exists C2(t) such as:

Δk([τ ])2 = C1(t) + C2(t).

τ1∗

=

642 Δk ([τ ])2

log

16K δΔk([τ ])

.

For invalidating 28, we must ﬁnd a value τ2∗ > τ1∗ such as:

Notice that τ1∗ ≥ τmin, satisfying one of the two conditions needed to eliminate an arm.
We introduce the following quantity

C1 (t )

=

8 τ

log

4K τ 2 δ

.

For τ = τ1∗, we derive the following bound

C1(τ1∗)

=

8Δk ([τ ])2

642

log

16K δΔk ([τ ])

×

log

4K δ

+ 2 log

64 Δk ([τ ])2

τ2∗

≥

4B C2(t2∗)

(30)

As C2(τ ) = Δk([τ ])2 −C1(τ ), we have C2(τ ) ≥ Δk([τ ])2 −

Δk ([τ ])2 512

and:

τ

≥

2048 B 511Δk ([τ ])2

For τ = τ2∗ with:

τ2∗

=

642 Δk ([τ ])2

log

16K δΔk([τ ])

+

5B Δk ([τ

]2

)

.

(31)

123

280

Int J Data Sci Anal (2017) 3:267–283

(30) is true, invalidating (28) and invalidating (26) and involving the elimination of the suboptimal arms k with a probability at least 1 − δ/K .
We conclude the proof by summing over all the arms, taking the union bound and lower bounding all Δk([τ ]) by

Δ = min

[τ

]∈T(τ
⎛

),k

r

∈T(τ

)

r = [τ ] τ

⎞

τ
×⎝

ti +|Sτ |−1 μk ( j ) −

τ

ti +|Sτ |−1 μk ( j ) ⎠ .

i=1 j =ti

|Si |

i=1 j =ti

|Si |

(32)

We conclude the proof of the distribution-dependent upper bound by setting δ = 1/T and:

E[R(T )] = O

K −1 Δ log

KT Δ

,

(36)

with

Δ

=

min[τ ],k

1 τ

τ i =1

ti +|Si |−1 t =ti

Δk∗,k (t |Si |

)

.

We now upper bound the regret in the worst case in order

to derive a distribution-independent bound. To this end, we

consider a sequence that ensures that, with high probability,

no suboptimal arm is eliminated by the algorithm at the end

of the T rounds, while maximizing the instantaneous regret.

According to (21) an arm is not eliminated as long as

Proof of Corollary 1

Proof We ﬁrst provide the proof of the distributiondependent upper bound.
The pseudocumulative regret of the algorithm is:

τ ti +|Si |−1

R(T ) =

Δk∗,k (t )1 k=kt .

k=k∗ i =1 t=ti

(33)

Taking in each round-robin the expectation of the corresponding random variable kt with respect to the randomization of the round-robin (denoted by Ekt ), it comes:

τ ti +|Si |−1

E[R(T )] = E

Ekt [Δk∗,k (t )1 k=kt ]

k=k∗ i =1 t=ti

=E

τ ti +|Si |−1 Δk∗,k (t )

k=k∗ i =1 t=ti

|Si |

.

E[μˆ k(τ )] − E[μˆ k (τ )] < 4 τ .

(37)

By injecting (37) in (34) and replacing τ by its value

2 τ

log

4K τ 2 δ

we obtain:

2

4K τ 2

E[R(T )] < τ 4
k =k ∗

τ log

δ

+ δT.

(38)

The

non-elimination of

suboptimal arms

involves τ

=

T K

,

and by setting δ

=

1 T

,

we

obtain

the

distribution-independent

upper bound:

E[R(T )] < (K − 1) T 4

K log

4T 3

+ 1,

(39)

KT

K

E[R(T )] = O

T T K log

.

(40)

K

E[R(T )] = E

τ 1 τ ti +|Si |−1 Δk∗,k (t )

k=k∗ τ i=1 t=ti

|Si |

Δ∗k

=E

τ Δ∗k .

k =k ∗

(34)

The penultimate step of the proof of Theorem 1 allows us to upper bound τ with the previously introduced critical value τ ∗ on an event of high probability 1 − δ, while the
cumulative regret is controlled by the trivial upper bound T on the complementary event of probability not higher than δ,
leading to:

E[ R (T

)]

≤

k =k ∗

64 Δ2k

log

4K δΔk

Δk + δT.

(35)

Proof of Theorem 3

Proof In order to prove Theorem 3, we consider the following quantities:

– The expected number of times when the estimators are

reseted: Nreset = ϕT .

– The sample complexity needed to ﬁnd the best arm

between each reset is SSER3 = O

K Δ2

log(

K δΔ

)

.

– The time before a reset that follows a negative binomial

distribution of parameters r = 1 and p = 1 − ϕ. Its

expectation is upper bounded by 1/ϕ.

– The number of arm switches: N − 1.

The sample complexity of SER4 is the total number of time-steps spent sampling an arm added to the time between each switch and reset.

123

Int J Data Sci Anal (2017) 3:267–283

281

Taking the expectation with respect to the randomization of resets, we obtain an upper bound on the expected number of suboptimal plays given by

O

ϕT K Δ2 log

K δΔ

+

N ϕ

.

(41)

The ﬁrst term is the expectation of the total number of

time-steps required by the algorithm in order to ﬁnd the best

arms at its initialization and then after each reset of the algo-

rithm. The second term is the expected total number of steps

lost by the algorithm when not resetting the algorithm after

the N − 1 arm switches.

We obtain the ﬁnal statement of theorem by setting T

=

1 δ

.

Proof of Corollary 3

N

+ E Ln + δT.

(45)

n=1

At this point, we note that {τireset}i is an i.i.d sequence of random variables and that Nreset is a random stopping time with respect to this sequence. Moreover, f is a concave
function. We can thus apply Wald’s equation followed by
Jensen’s inequality and deduce that

Nreset +1

E

f (τireset) ≤ E[Nreset + 1]E[ f (τ1reset)]

i =1

≤ E[Nreset + 1] f (E[τ1reset]).

We

upper

bound

log(

4(τireset δ

)2

)

by

log(

4T δK

2 2

)

and

set

δ

=

1 T

.

As E[Nreset]

=

ϕT , E[τ1reset]

=

1 ϕK

and E[Ln]

≤

1 ϕ

,

we

have

Proof Converting Corollary 2 into a distribution-dependent

upper bound on the cumulative regret is straightforward by

setting

δ

=

1 T

,

replacing

the

sample

complexity

in

the

proof

of Theorem 3 by the cumulative regret and using the upper

bound of Corollary 1.

E[R(T )] = O

ϕT K Δ log

KT Δ

+

N ϕ

.

(42)

Setting ϕ =

N T K log(K T )

and assuming Δ

≥

1 KT

we

obtain the ﬁnal statement of theorem:

E[R(T )] < 4(ϕT + 1)

2 ϕ K log

4T 3 K2

+

N ϕ

+ 1.

(46)

The previ√ous equation makes appear a trade-off in ϕ, and

we set ϕ

=

T

N
2/3

.

Finally we have shown that

E[R(T )] = O

T 2/3

T N K log

.

(47)

K

E[R(T )] = O

N T K log(K T )

Δ

.

(43) Proof of Lemma 2

We also derive a distribution-independent upper bound.
We introduce some notations, Nreset is the number of resets, τireset is the number of round-robin phases between the ith and the (i + 1)th resets and Ln is the number of time-steps before a reset after the nth arm switch.
When the resets are ﬁxed, the expected cumulative regret
is:

Nreset +1

E[R(T )] < E

(K − 1)τireset4

i =1

N
+ Ln + δT , ⎡n=1

2 τireset log

4(τireset)2 δ
(44) ⎤

E[R(T )]

<

E

⎢⎢⎢⎢⎢⎣

Nreset +1 i =1

(

K

−

1)4

2τireset log

4(τireset)2 δ

⎥⎥⎥⎥⎥⎦

f (τireset)

Proof We justify our detection test by considering an obser-
vation of a reward through γ -exploration as a drawing in
an urn without replacement. More speciﬁcally, when all the
necessary observations are collected, the detection test pro-
cedure is called. During the interval, rewards were drawn from m different distributions of mean μk0(I ), . . . , μkm(I ) . We denote ti the steps where the mean reward starts being μik(I ) and tm+1 the time-step of the call. When the test is called, all xk (t) have a probability (ti+1 − ti )/(tm+1 − t0) to be drawn from the distribution of mean μik(I ). The mean μk(I ) of the urn corresponding to the action k is:

μk (I )

=

m i =1

ti +1 tm+1

− −

ti t0

μik

(I

).

(48)

At each time-step, by assumption , the mean reward of the best arm is away by 4 from any suboptimal arms. Consequently, the difference between the mean reward of the urn

123

282

Int J Data Sci Anal (2017) 3:267–283

of the optimal arm k∗ and that of an another arm k is at least 4 if the best arm does not change during the interval.

m
μk (I ) ≤
i =1

ti +1 tm+1

− −

ti t0

(μik∗S

−

4

) ≤ μk∗S (I ) − 4

.

(49)

The following arguments prove the equivalence between

the

detection

and

the

optimality

of

k

∗ S

with

high

probability.

Applying the Serﬂing inequality [16], we have:

−2n 2

P(μˆ k∗S (I ) +

≥

μk

∗ S

(I

))

≤

e

1−

n−1 U

≤ e−2n

2 =δ

(50)

and

P(μˆ k(I ) − ≤ μk(I )) ≤ δ,

(51)

where

n

=

γH K

is

the

number

of

observation

and

U

the

size

of the urn.

μk

∗ S

(I

)

−

μk

(

I

)

≥

4

(52)

and with probability at least 1 − 2δ,

μˆ k

∗ S

(

I

)

+

≥ μk∗S (I )

(53)

and

μˆ k(I ) − ≤ μk(I )

(54)

Summing (52), (53) and (54) we obtain:

μˆ k

∗ S

(I

)

−

μˆ k

(

I

)

≥

2

?

(55)

This ensures, with high probability, a positive test if μˆ kmax is not the optimal arm.
Reciprocally, we also have

μˆ k

(I

)

−

μˆ k

∗ S

(

I

)

≤

−2

.

(56)

ensuring, with high probability, a negative test if μˆ kmax is the optimal arm.

Proof of Theorem 4

Proof First we obtain the main structure of the bound. In the following, L(T ) denotes the expected number of intervals after a best action change occurs before detection and F(T ) denotes the expected number of false detections up to time T . Using the same arguments as [18] we deduce the form of the bound with drift detector from the classical EXP3 bound. If there are N −1 changes of best arm. Therefore the expectation of the number of resets over an horizon T is upper bounded

by N − 1 + F(T ). The regret of EXP3 on these periods is

(e − 1)γ T

+

K log K γ

[3]. While our optimal policy plays the

arm with the highest mean, the optimal policy of EXP3 plays

the arm associated with the actual highest cumulative reward.

As

TS +1 −1

TS +1 −1

xk

∗ S

(t

)

≤

max
k

xk (t),

t =TS

t =TS

(57)

the gain of our optimal policy is upper bounded by the gain

EXP3 optimal policy. Summing over each periods we obtain

(e

− 1)γ T

+

(N −1+F(T ))K γ

log K

.

The regret also includes the delay between a best arm

change and its detection. To evaluate the expected size of the

intervals between each call of the detection test, we consider

an hypothetical algorithm that collects only the observations

of one arm and then proceeds on the next arm until collect-

ing all the observations. The γ -observation is drawn with a

probability

γ K

,

and

γH K

observations are needed per action.

The expectation of the number of failures before collecting

γH K

γ -observations

follows

a

negative

binomial distribution

of expectation

γH 1− γ K = H − γH.

(58)

K

Kγ

K

The expectation of the number of steps at the end of the collect is the number of success plus the expected number of failures:

γ H + H − γ H = H.

(59)

K

K

Summing over all arms gives a total expectation of H K . Because our algorithm collects γ -observations from any arm at any step, on a same sequence of drawings, our algorithm will collect the required observations before the hypothetical algorithm. By consequence, the expectation of the time between each query of the detection test is upper bounded by H K and lower bounded by H , the expected time of collect for one arm. There are N − 1 best action changes, and the detections occur at most L(T ) H K time-steps after the drifts. Finally, there are also at most N − 1 intervals where the optimal arm switches. In these intervals we do not have any guarantee on the test behavior due to this change. In the worst case, the test does not detect the drift and we set the instantaneous regret to 1.

G∗ − E[GEXP3.R] ≤ (e − 1)γ T

+ (N

− 1 + F(T ))K γ

log K +(N

− 1)H K (

L(T )

+1).

(60)

123

Int J Data Sci Anal (2017) 3:267–283

283

We now bound F(T ) and L(T ). Conﬁdence intervals hold

with probability 1 − δ, and they are used K times at each

detection test. The maximal number of calls of the test up to

time

horizon

T

is

T H

+ 1.

Using

the

union

bound

we

deduce

F(T )

≤

K

δ(

T H

+

1).

L(T )

is

the

ﬁrst

occurrence

of

the

event detection after a drift. When a drift occurs, Lemma 2

ensures the detection happens with a probability 1 − 2δ. We

have

L(T )

≤

1 1−2δ

.

G∗ − E[GEXP3.R] ≤ (e − 1)γ T

+

N

−1+

K δT H

+

Kδ

K log K

γ

+ (N − 1)H K

1

1 − 2δ

+

1

.

(61)

Proof of Corollary 4

Proof We set δ =

log T KT

and H

=

√ CT

log T

in Theo-

rem 4

G∗ − E[GEXP3.R] ≤ (e√− 1)γ T

+

(N

−

1

+

(C

+ 1) γ

K )K log K

+ 3(N − 1)C K T log T .

(62)

Finally, setting γ =

K log K log T T

we obtain:

G∗ − E[GEXP3.R] ≤ (e − 1) T K log K log T

+ N T K log K + (C + 1)K T log K

+ 3N C K T log T .

(63)

References

2. Auer, P., Cesa-Bianchi, N., Fischer, P.: Finite-time analysis of the multiarmed bandit problem. Mach. Learn. 47(2–3), 235–256 (2002a)
3. Auer, P., Cesa-Bianchi, N., Freund, Y., Schapire, R.E.: The nonstochastic multiarmed bandit problem. SIAM J. Comput. 32(1), 48–77 (2002b)
4. Bubeck, S., Cesa-Bianchi, N.: Regret analysis of stochastic and nonstochastic multi-armed bandit problems. In: Foundations and Trends in Machine Learning. (2012) http://dblp.uni-trier.de/rec/ bibtex/journals/ftml/BubeckC12
5. Bubeck, S., Slivkins, A.: The best of both worlds: stochastic and adversarial bandits. In: COLT 2012—the 25th Annual Conference on Learning Theory, pp. 42.1–42.23. Edinburgh, Scotland, 25–27 June 2012
6. Even-Dar, E., Mannor, S., Mansour, Y.: Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. J. Mach. Learn. Res. 7, 1079–1105 (2006)
7. Féraud, R., Allesiardo, R., Urvoy, T., Clérot, F.: Random forest for the contextual bandit problem. In: AISTATS. (2016)
8. Garivier, A., Moulines, E.: On upper-conﬁdence bound policies for non-stationary bandit problems. In: Algorithmic Learning Theory, pp. 174–188. (2011) http://dblp.uni-trier.de/rec/bibtex/conf/ alt/GarivierM11
9. Garivier, A., Kaufmann, E., Lattimore, T.: On explore-then-commit strategies. In: NIPS, vol. 30. (2016) http://dblp.uni-trier.de/rec/ bibtex/conf/nips/GarivierLK16
10. Hartland, C., Baskiotis, N., Gelly, S., Teytaud, O., Sebag, M.: Multi-armed bandit, dynamic environments and meta-bandits. In: Online Trading of Exploration and Exploitation Workshop, NIPS. (2006)
11. Hoeffding, W.: Probability inequalities for sums of bounded random variables. J. Am. Stat. Assoc. 58(301), 13–30 (1963)
12. Kaufmann, E., Cappé, O., Garivier, A.: On the complexity of best arm identiﬁcation in multi-armed bandit models. J. Mach. Learn. Res. 17(1), 1–42 (2016)
13. Kocsis, L., Szepesvári, C.: Discounted UCB. In: 2nd PASCAL Challenges Workshop. (2006)
14. Neu, G.: Explore no more: improved high-probability regret bounds for non-stochastic bandits. In: NIPS. pp. 3168–3176 (2015)
15. Seldin, Y., Slivkins, A.: One practical algorithm for both stochastic and adversarial bandits. In: 31th International Conference on Machine Learning (ICML). (2014)
16. Serﬂing, R.J.: Probability inequalities for the sum in sampling without replacement. Ann. Stat. 2(1), 39–48 (1974)
17. Thompson, W.R.: On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika 25, 285–294 (1933)
18. Yu, J.Y., Mannor, S.: Piecewise-stationary bandit problems with side observations. In: Proceedings of the 26th Annual International Conference on Machine Learning, ICML. (2009)

1. Allesiardo, R., Féraud, R.: EXP3 with drift detection for the switching bandit problem. In: 2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA 2015)

123

