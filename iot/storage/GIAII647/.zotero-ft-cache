Machine Learning

Srihari

Linear Algebra for Machine Learning
Sargur N. Srihari srihari@cedar.buffalo.edu
1

Machine Learning

Srihari

What is linear algebra?

•  Linear algebra is the branch of mathematics concerning linear equations such as

a1x1+…..+anxn=b
–  In vector notation we say aTx=b –  Called a linear transformation of x
•  Linear algebra is fundamental to geometry, for defining objects such as lines, planes, rotations

Linear equation a1x1+…..+anxn=b defines a plane in (x1,..,xn) space Straight lines define common solutions
to equations

2

Machine Learning

Srihari

Why do we need to know it?

•  Linear Algebra is used throughout engineering
–  Because it is based on continuous math rather than discrete math
•  Computer scientists have little experience with it
•  Essential for understanding ML algorithms
–  E.g., We convert input vectors (x1,..,xn) into outputs by a series of linear transformations
•  Here we discuss:
–  Concepts of linear algebra needed for ML –  Omit other aspects of linear algebra

3

Machine Learning Linear Algebra Topics

Srihari

–  Scalars, Vectors, Matrices and Tensors

–  Multiplying Matrices and Vectors

–  Identity and Inverse Matrices

–  Linear Dependence and Span

–  Norms

–  Special kinds of matrices and vectors

–  Eigendecomposition

–  Singular value decomposition

–  The Moore Penrose pseudoinverse

–  The trace operator

–  The determinant

–  Ex: principal components analysis

4

Machine Learning

Srihari

Scalar

•  Single number
–  In contrast to other objects in linear algebra, which are usually arrays of numbers
•  Represented in lower-case italic x
–  They can be real-valued or be integers
•  E.g., let x ∈! be the slope of the line
–  Defining a real-valued scalar
•  E.g., let n ∈! be the number of units
–  Defining a natural number scalar

5

Machine Learning

Srihari

Vector

•  An array of numbers arranged in order

•  Each no. identified by an index

•  Written in lower-case bold such as x

–  its elements are in italics lower case, subscripted

⎡ ⎢

x1

⎤ ⎥

⎢ ⎢

x2

⎥ ⎥

x=⎢ ⎥

⎢⎥

⎢ ⎢ ⎣⎢

xn

⎥ ⎥ ⎦⎥

•  If each element is in R then x is in Rn

•  We can think of vectors as points in space

–  Each element gives coordinate along an axis
6

Machine Learning

Srihari

Matrices

•  2-D array of numbers

–  So each element identified by two indices

•  Denoted by bold typeface A

–  Elements indicated by name in italic but not bold

•  A1,1 is the top left entry and Am,nis the bottom right entry •  We can identify nos in vertical column j by writing : for the
horizontal coordinate

• 

E.g., A

=

⎡ ⎢ ⎣⎢

A1,1 A2,1

A1,2

⎤ ⎥

A2,2 ⎦⎥

•  Ai: is ith row of A, A:j is jth column of A

•  If A has shape of height m and width n with real-values then A∈!m×n
7

Machine Learning

Srihari

Tensor

•  Sometimes need an array with more than two axes
–  E.g., an RGB color image has three axes
•  A tensor is an array of numbers arranged on a regular grid with variable number of axes
–  See figure next
•  Denote a tensor with this bold typeface: A •  Element (i,j,k) of tensor denoted by Ai,j,k
8

Machine Learning

Srihari

Shapes of Tensors

9

Machine Learning

Srihari

Transpose of a Matrix

•  An important operation on matrices •  The transpose of a matrix A is denoted as AT •  Defined as
(AT)i,j=Aj,i
–  The mirror image across a diagonal line
•  Called the main diagonal , running down to the right starting from upper left corner

⎡ ⎢

A1,1

A1,2

A1,3

⎤ ⎥

⎡ ⎢

A1,1

A2,1

A3,1

⎤ ⎥

A

=

⎢ ⎢

A2,1

A2,2

A2,3

⎥ ⎥

⇒

AT

=

⎢ ⎢

A1,2

A2,2

A3,2

⎥ ⎥

⎣⎢ A3,1 A3,2 A3,3 ⎦⎥

⎣⎢ A1,3 A2,3 A3,3 ⎦⎥

⎡ ⎢

A1,1

A1,2

A

=

⎢ ⎢

A2,1

A2,2

⎣⎢ A3,1 A3,2

⎤ ⎥

⎡ ⎢

A1,1

A2,1

A3,1

⎤ ⎥

⎥ ⎥

⇒

AT

=

⎢ ⎢

A1,2

A2,2

A3,2

⎥ ⎥

⎦⎥

⎣⎢

⎦⎥

10

Machine Learning

Srihari

Vectors as special case of matrix

•  Vectors are matrices with a single column

•  Often written in-line using transpose

x = [x1,..,xn]T

⎡ ⎢

x1

⎤ ⎥

⎢ ⎢ x=⎢ ⎢

x2

⎥ ⎥ ⎥ ⎥

⇒ x T = ⎡⎣x1 ,x2 ,..xn ⎤⎦

⎢ ⎣⎢⎢

xn

⎥ ⎦⎥⎥

•  A scalar is a matrix with one element a=aT

11

Machine Learning

Srihari

Matrix Addition

•  We can add matrices to each other if they have the same shape, by adding corresponding elements
–  If A and B have same shape (height m, width n)
C = A+B ⇒Ci,j = Ai,j +Bi,j
•  A scalar can be added to a matrix or multiplied by a scalar D = aB+c ⇒ Di,j = aBi,j +c
•  Less conventional notation used in ML:
–  Vector added to matrix C = A+b ⇒Ci,j = Ai,j +bj
•  Called broadcasting since vector b added to each row of A

12

Machine Learning

Srihari

Multiplying Matrices

•  For product C=AB to be defined, A has to have the same no. of columns as the no. of rows of B
–  If A is of shape mxn and B is of shape nxp then matrix product C is of shape mxp
∑ C = AB ⇒Ci,j = Ai,kBk,j k
–  Note that the standard product of two matrices is not just the product of two individual elements
•  Such a product does exist and is called the element-wise product or the Hadamard product A¤B
13

Machine Learning

Srihari

Multiplying Vectors

•  Dot product between two vectors x and y of same dimensionality is the matrix product xTy
•  We can think of matrix product C=AB as computing Cij the dot product of row i of A and column j of B

14

Machine Learning

Srihari

Matrix Product Properties

•  Distributivity over addition: A(B+C)=AB+AC •  Associativity: A(BC)=(AB)C
•  Not commutative: AB=BA is not always true
•  Dot product between vectors is commutative: xTy=yTx
•  Transpose of a matrix product has a simple form: (AB)T=BTAT

15

Machine Learning

Srihari

Example flow of tensors in ML

Vector x is converted into vector y by multiplying x by a matrix W

A linear classifier y= WxT+b

A linear classifier with bias eliminated y= WxT

Machine Learning

Srihari

Linear Transformation

•  Ax=b

–  where A ∈!n×n and b ∈!n

–  More explicitly A x + A x +....+ A x = b

11 1 12 2

1n n 1

A21x1

+

A22

x 2

+

....+

A2n

x n

=

b 2

An1

x 1

+

Am2

x 2

+

....+

An,n

x n

=

bn

n equations in n unknowns

⎡ ⎢

A1,1

!

A1,n

⎤ ⎥

A

=

⎢ ⎢

⎣⎢

" An,1

" !

" Ann

⎥ ⎥ ⎦⎥

nxn

⎡ ⎢

x 1

⎤ ⎥

x=⎢ " ⎥

⎢ ⎣⎢

x n

⎥ ⎦⎥

nx1

⎡ ⎢

b 1

⎤ ⎥

b=⎢ " ⎥

⎢ ⎣⎢

bn

⎥ ⎦⎥

n x1

Can view A as a linear transformation of vector x to vector b

•  Sometimes we wish to solve for the unknowns x ={x1,..,xn} when A and b provide constraints
17

Machine Learning

Srihari

Identity and Inverse Matrices

•  Matrix inversion is a powerful tool to analytically solve Ax=b

•  Needs concept of Identity matrix

•  Identity matrix does not change value of vector when we multiply the vector by identity matrix

–  Denote identity matrix that preserves n-dimensional

vectors as In

–  Formally In ∈!n×n and ∀x ∈!n ,Inx = x

–  Example of I3

⎡ ⎢ ⎢

1 0

0 1

0 0

⎤ ⎥ ⎥

⎣⎢ 0 0 1 ⎦⎥

18

Machine Learning

Srihari

Matrix Inverse

•  Inverse of square matrix A defined as
•  We can now solve Ax=b as follows:
Ax = b A−1Ax = A−1b In x = A−1b x = A−1b

A−1A = In

•  This depends on being able to find A-1
•  If A-1 exists there are several methods for finding it

19

Machine Learning

Srihari

Solving Simultaneous equations

•  Ax = b
where A is (M+1) x (M+1) x is (M+1) x 1: set of weights to be determined b is N x 1

20

Example: System of Linear Machine Learning

Srihari

Equations in Linear Regression

•  Instead of Ax=b
•  We have Φw = t
–  where Φ is m x n design matrix of m features for n samples xj, j=1,..n
–  w is weight vector of m values –  t is target values of sample, t=[t1,..tn] –  We need weight w to be used with m features to
determine output

∑m
y(x,w)= wixi

i=1

21

Machine Learning

Srihari

Closed-form solutions

•  Two closed-form solutions
1. Matrix inversion x=A-1b 2. Gaussian elimination

22

Machine Learning

Srihari

Linear Equations: Closed-Form Solutions

1. Matrix Formulation: Ax=b Solution: x=A-1b

2. Gaussian Elimination followed by back-substitution
L2-3L1àL2

L3-2L1àL3

-L2/4àL2

Machine Learning

Srihari

Disadvantage of closed-form solutions

•  If A-1 exists, the same A-1 can be used for any given b

–  But A-1 cannot be represented with sufficient precision

–  It is not used in practice

•  Gaussian elimination also has disadvantages

–  numerical instability (division by small no.)

–  O(n3) for n x n matrix

•  Software solutions use value of b in finding x

–  E.g., difference (derivative) between b and output is

used iteratively

24

Machine Learning

Srihari

How many solutions for Ax=b exist?

•  System of equations with
–  n variables and m equations is:
•  Solution is x=A-1b

A x + A x +....+ A x = b

11 1 12 2

1n n 1

A21

x 1

+

A22

x 2

+

....+

A2n

x n

=

b 2

Am1

x 1

+

Am2

x 2

+

....+

Amn

x n

=

bm

•  In order for A-1 to exist Ax=b must have exactly one solution for every value of b

–  It is also possible for the system of equations to have no solutions or an infinite no. of solutions for some values of b
•  It is not possible to have more than one but fewer than infinitely many solutions

–  If x and y are solutions then z=α x + (1-α) y is a

solution for any real α

25

Machine Learning

Srihari

Span of a set of vectors

•  Span of a set of vectors: set of points obtained

by a linear combination of those vectors

–  A linear combination of vectors {v(1),.., v(n)} with
coefficients ci is ∑civ(i) i
–  System of equations is Ax=b

•  A column of A, i.e., A:i specifies travel in direction i

•  How much we need to travel is given by xi

•  This is a linear combination of vectors

∑ Ax = xiA:,i

i

–  Thus determining whether Ax=b has a solution is

equivalent to determining whether b is in the span of

columns of A

•  This span is referred to as column space or range of A

Machine Learning

Srihari

Conditions for a solution to Ax=b

•  Matrix must be square, i.e., m=n and all columns must be linearly independent
–  Necessary condition is n ≥ m
•  For a solution to exist when A ∈!m×n we require the column space be all of !m

–  Sufficient Condition

•  If columns are linear combinations of other columns, column space is less than !m
–  Columns are linearly dependent or matrix is singular
•  For column space to encompass !m at least one set of m linearly independent columns

•  For non-square and singular matrices

–  Methods other than matrix inversion are used

Machine Learning

Srihari

Use of a Vector in Regression

•  A design matrix
–  N samples, D features

•  Feature vector has three dimensions
•  This is a regression problem
28

Machine Learning

Srihari

Norms

•  Used for measuring the size of a vector •  Norms map vectors to non-negative values •  Norm of vector x = [x1,..,xn]T is distance from
origin to x
–  It is any function f that satisfies:
( ) f x = 0⇒x = 0 ( ) ( ) f( x+y) ≤ f x + f y Triangle Inequality ∀α ∈R f (αx )= α f (x )
29

Machine Learning

LP Norm

Srihari

•  Definition:
–  L2 Norm

1

∑ x

p

=

⎛ ⎝⎜

i

xi

p⎞ ⎠⎟

p

•  Called Euclidean norm

–  Simply the Euclidean distance between the origin and the point x
–  written simply as ||x|| –  Squared Euclidean norm is same as xTx
–  L1 Norm

22 + 22 = 8 = 2 2

•  Useful when 0 and non-zero have to be distinguished
–  Note that L2 increases slowly near origin, e.g., 0.12=0.01)

–  L∞ Norm

x

= max

∞

i

xi

•  Called max norm

30

Machine Learning

Srihari

Use of norm in Regression

•  Linear Regression
x: a vector, w: weight vector
y(x,w) = w0+w1x1+..+wd xd = wTx
With nonlinear basis functions ϕj
∑M −1
y(x,w) = w0 + wjφj(x) j =1

•  Loss Function

∑ E!(w)

=

1 2

N
{y(xn,w)
n =1

−

tn }2

+

λ 2

|| w2

||

Second term is a weighted norm

called a regularizer (to prevent overfitting) 31

Machine Learning

Srihari

LP Norm and Distance

•  Norm is the length of a vector

•  We can use it to draw a unit circle from origin
–  Different P values yield different shapes
•  Euclidean norm yields a circle
•  Distance between two vectors (v,w)
–  dist(v,w)=||v-w|| = (v1 − w1)2 + .. + (vn − wn )2
Distance to origin would just be sqrt of sum of squares 32

Machine Learning

Srihari

Size of a Matrix: Frobenius Norm

•  Similar to L norm 2

1

∑ A

⎛ F = ⎝⎜

i,j

A2 i,j

⎞ ⎠⎟

2

⎡

A

=

⎢ ⎢

2 0

−1 2

5 1

⎤ ⎥ ⎥

⎣⎢ 3 1 1 ⎦⎥

A = 4 + 1 + 25 + .. + 1 = 46

•  Frobenius in ML

–  Layers of neural network involve matrix multiplication

–  Regularization:

•  minimize Frobenius of weight matrices ||W(i)|| over L layers

V matrix

W matrix

I

J

K

I1×(I+1) × V(I+1)×J=netJ

hj=f(netj) f(x)=1/(1+e-x)

33

Machine Learning

Srihari

Angle between Vectors

•  Dot product of two vectors can be written in

terms of their L2 norms and angle θ between

them

x Ty ⇒||x ||2||y ||2 cos θ

•  Cosine between two vectors is a measure of their similarity

34

Machine Learning

Srihari

Special kind of Matrix: Diagonal

•  Diagonal Matrix has mostly zeros, with nonzero entries only in diagonal
–  E.g., identity matrix, where all diagonal entries are 1

–  E.g., covariance matrix with independent features

If Cov(X,Y)=0 then E(XY)=E(X)E(Y)

N (x

|

µ, Σ)

=

1 (2π)D/2

|

1 Σ |1/2

exp

⎧⎨⎪⎪⎩⎪⎪−

1 2

(x

−

µ)T

Σ−1(x

−

µ)⎫⎬⎪⎪⎭⎪⎪

Machine Learning

Srihari

Efficiency of Diagonal Matrix

•  diag (v) denotes a square diagonal matrix with diagonal elements given by entries of vector v
•  Multiplying vector x by a diagonal matrix is efficient
–  To compute diag(v)x we only need to scale each xi
by vi diag( v)x = v ⊙x
•  Inverting a square diagonal matrix is efficient
–  Inverse exists iff every diagonal entry is nonzero, in which case diag (v)-1=diag ([1/v1,..,1/vn]T)

Machine Learning

Srihari

Special kind of Matrix: Symmetric

•  A symmetric matrix equals its transpose: A=AT
–  E.g., a distance matrix is symmetric with Aij=Aji

–  E.g., covariance matrices are symmetric

Machine Learning

Srihari

Special Kinds of Vectors

•  Unit Vector

–  A vector with unit norm

x =1 2

•  Orthogonal Vectors

–  A vector x and a vector y are orthogonal to each other if xTy=0

•  If vectors have nonzero norm, vectors at 90 degrees to each other

–  Orthonormal Vectors

•  Vectors are orthogonal & have unit norm

•  Orthogonal Matrix

–  A square matrix whose rows are mutually

orthonormal: ATA=AAT=I

Orthogonal matrices are of

–  A-1=AT

interest because their inverse is

very cheap to compute

Machine Learning

Srihari

Matrix decomposition

•  Matrices can be decomposed into factors to learn universal properties, just like integers:

–  Properties not discernible from their representation

1. Decomposition of integer into prime factors

•  From 12=2×2×3 we can discern that
–  12 is not divisible by 5 or –  any multiple of 12 is divisible by 3 –  But representations of 12 in binary or decimal are different
2. Decomposition of Matrix A as A=Vdiag(λ)V-1
•  where V is formed of eigenvectors and λ are eigenvalues, e.g,

A = ⎡⎣⎢⎢

2 1

1 2

⎤⎦⎥⎥

has eigenvalues λ=1 and λ=3 and eigenvectors V:

vλ=1 = ⎡⎣⎢⎢

1 −1

⎤⎦⎥⎥ ,vλ=3 = ⎡⎣⎢⎢

1 1

⎤⎦⎥⎥

Machine Learning

Eigenvector

•  An eigenvector of a square matrix A is a non-zero vector v such that multiplication by A only changes the scale of v
Av=λv
–  The scalar λ is known as eigenvalue
•  If v is an eigenvector of A, so is any rescaled vector sv. Moreover sv still has the same eigen value. Thus look for a unit eigenvector

Srihari
Wikipedia 40

Machine Learning

Srihari

Eigenvalue and Characteristic Polynomial

•  Consider Av=w

⎡ ⎢

A1,1

L

A1,n

⎤ ⎥

A=

⎢ ⎢

⎣⎢

M An,1

M L

M Ann

⎥ ⎥ ⎦⎥

⎡ ⎢

v1

⎤ ⎥

v=⎢ M ⎥

⎢ ⎣⎢

vn

⎥ ⎦⎥

⎡ ⎢

w1

⎤ ⎥

w= ⎢ M ⎥

⎢ ⎣⎢

wn

⎥ ⎦⎥

•  If v and w are scalar multiples, i.e., if Av=λv

•  then v is an eigenvector of the linear transformation A and the scale factor λ is the eigenvalue corresponding to the eigen vector

•  This is the eigenvalue equation of matrix A

–  Stated equivalently as (A-λI)v=0

–  This has a non-zero solution if |A-λI|=0

as

•  The polynomial of degree n can be factored as

|A-λI| = (λ1-λ)(λ2-λ)…(λn-λ)
•  The λ1, λ2…λn are roots of the polynomial and are eigenvalues of A

Machine Learning

Srihari

Example of Eigenvalue/Eigenvector

•  Consider the matrix

A = ⎡⎣⎢⎢

2 1

1 2

⎤⎦⎥⎥

•  Taking determinant of (A-λI), the char poly is

| A −λI

|=

⎡⎣⎢⎢

2−λ 1

1 2−λ

⎤⎦⎥⎥ = 3 − 4λ + λ2

•  It has roots λ=1 and λ=3 which are the two eigenvalues of A

•  The eigenvectors are found by solving for v in

Av=λv, which are

vλ=1 = ⎡⎣⎢⎢

1 −1

⎤⎦⎥⎥ ,vλ=3 = ⎡⎣⎢⎢

1 1

⎤⎦⎥⎥

42

Machine Learning

Srihari

Eigendecomposition

•  Suppose that matrix A has n linearly independent eigenvectors {v(1),..,v(n)} with eigenvalues {λ1,..,λn}
•  Concatenate eigenvectors to form matrix V
•  Concatenate eigenvalues to form vector λ=[λ1,..,λn]
•  Eigendecomposition of A is given by
A=Vdiag(λ)V-1

43

Machine Learning

Srihari

Decomposition of Symmetric Matrix

•  Every real symmetric matrix A can be decomposed into real-valued eigenvectors and eigenvalues

A=QΛQT

where Q is an orthogonal matrix composed of eigenvectors of A: {v(1),..,v(n)}
orthogonal matrix: components are orthogonal or v(i)Tv(j)=0
Λ is a diagonal matrix of eigenvalues {λ1,..,λn}
•  We can think of A as scaling space by λi in direction v(i)

–  See figure on next slide

44

Machine Learning

Srihari

Effect of Eigenvectors and Eigenvalues

•  Example of 2×2 matrix

•  Matrix A with two orthonormal eigenvectors

–  v(1) with eigenvalue λ1, v(2) with eigenvalue λ2

Plot of unit vectors u ∈!2 (circle)

Plot of vectors Au (ellipse)

with two variables x1 and x2

45

Machine Learning

Srihari

Eigendecomposition is not unique

•  Eigendecomposition is A=QΛQT
–  where Q is an orthogonal matrix composed of eigenvectors of A
•  Decomposition is not unique when two eigenvalues are the same
•  By convention order entries of Λ in descending order:
–  Under this convention, eigendecomposition is unique if all eigenvalues are unique
46

Machine Learning

Srihari

What does eigendecomposition tell us?

•  Tells us useful facts about the matrix:

1.  Matrix is singular if & only if any eigenvalue is zero

2.  Useful to optimize quadratic expressions of form

f(x)=xTAx subject to ||x||2=1
Whenever x is equal to an eigenvector, f is equal to the corresponding eigenvalue

Maximum value of f is max eigen value, minimum value is min eigen value

Example of such a quadratic form appears in multivariate

Gaussian

N (x

|

µ, Σ)

=

1 (2π)D/2

|

1 Σ |1/2

exp ⎧⎨⎪⎪⎩⎪⎪−

1 2

(x

−

µ)T

Σ−1(x

−

µ)⎫⎬⎪⎪⎭⎪⎪

47

Machine Learning

Srihari

Positive Definite Matrix

•  A matrix whose eigenvalues are all positive is called positive definite
–  Positive or zero is called positive semidefinite
•  If eigen values are all negative it is negative definite
–  Positive definite matrices guarantee that xTAx ≥ 0

48

Machine Learning

Srihari

Singular Value Decomposition (SVD)
•  Eigendecomposition has form: A=Vdiag(λ)V-1
–  If A is not square, eigendecomposition is undefined
•  SVD is a decomposition of the form A=UDVT •  SVD is more general than eigendecomposition
–  Used with any matrix rather than symmetric ones –  Every real matrix has a SVD
•  Same is not true of eigen decomposition

Machine Learning

Srihari

SVD Definition

•  Write A as a product of 3 matrices: A=UDVT
–  If A is m×n, then U is m×m, D is m×n, V is n×n
•  Each of these matrices have a special structure
•  U and V are orthogonal matrices •  D is a diagonal matrix not necessarily square
–  Elements of Diagonal of D are called singular values of A –  Columns of U are called left singular vectors –  Columns of V are called right singular vectors
•  SVD interpreted in terms of eigendecomposition
•  Left singular vectors of A are eigenvectors of AAT •  Right singular vectors of A are eigenvectors of ATA •  Nonzero singular values of A are square roots of eigen
values of ATA. Same is true of AAT

Machine Learning

Srihari

Use of SVD in ML

1.  SVD is used in generalizing matrix inversion
–  Moore-Penrose inverse (discussed next)
2.  Used in Recommendation systems
–  Collaborative filtering (CF)
•  Method to predict a rating for a user-item pair based on the history of ratings given by the user and given to the item
•  Most CF algorithms are based on user-item rating matrix where each row represents a user, each column an item
–  Entries of this matrix are ratings given by users to items
•  SVD reduces no.of features of a data set by reducing space dimensions from N to K where K < N
51

Machine Learning

Srihari

SVD in Collaborative Filtering

•  X is the utility matrix
–  Xij denotes how user i likes item j –  CF fills blank (cell) in utility matrix that has no entry
•  Scalability and sparsity is handled using SVD
–  SVD decreases dimension of utility matrix by extracting its latent factors
•  Map each user and item into latent space of dimension r
52

Machine Learning

Srihari

Moore-Penrose Pseudoinverse

•  Most useful feature of SVD is that it can be used to generalize matrix inversion to nonsquare matrices
•  Practical algorithms for computing the pseudoinverse of A are based on SVD
A+=VD+UT
–  where U,D,V are the SVD of A
•  Pseudoinverse D+ of D is obtained by taking the reciprocal of its nonzero elements when taking transpose of resulting matrix
53

Machine Learning

Srihari

Trace of a Matrix

•  Trace operator gives the sum of the elements along the diagonal
∑ Tr(A )= Ai,i i ,i
•  Frobenius norm of a matrix can be represented as
( )1
A = Tr(A) 2 F

54

Machine Learning

Srihari

Determinant of a Matrix

•  Determinant of a square matrix det(A) is a mapping to a scalar
•  It is equal to the product of all eigenvalues of the matrix
•  Measures how much multiplication by the matrix expands or contracts space

55

Machine Learning

Srihari

Example: PCA

•  A simple ML algorithm is Principal Components Analysis
•  It can be derived using only knowledge of basic linear algebra

56

Machine Learning

Srihari

PCA Problem Statement

•  Given a collection of m points {x(1),..,x(m)} in Rn represent them in a lower dimension.
–  For each point x(i) find a code vector c(i) in Rl
–  If l is smaller than n it will take less memory to store the points
–  This is lossy compression
–  Find encoding function f (x) = c and a decoding function x ≈ g ( f (x) )

57

Machine Learning

Srihari

PCA using Matrix multiplication

•  One choice of decoding function is to use matrix multiplication: g(c) =Dc where D ∈!n×l
–  D is a matrix with l columns
•  To keep encoding easy, we require columns of D to be orthogonal to each other
–  To constrain solutions we require columns of D to have unit norm
•  We need to find optimal code c* given D •  Then we need optimal D
58

Machine Learning

Srihari

Finding optimal code given D

•  To generate optimal code point c* given input

x, minimize the distance between input point x

and its reconstruction g(c*)

c* = argmin x − g(c)

c

2

–  Using squared L2 instead of L2, function being

minimized is equivalent to

(x − g(c))T (x − g(c))

•  Using g(c)=Dc optimal code can be shown to

be equivalent to c* = argmin− 2xT Dc+cTc

c

59

Machine Learning

Srihari

Optimal Encoding for PCA

•  Using vector calculus

∇c(−2xT Dc +cT c) = 0 −2DT x +2c = 0

c = DT x

•  Thus we can encode x using a matrix-vector

operation

–  To encode we use f(x)=DTx

–  For PCA reconstruction, since g(c)=Dc we use r(x)=g(f(x))=DDTx

–  Next we need to choose the encoding matrix D

60

Machine Learning

Srihari

Method for finding optimal D

•  Revisit idea of minimizing L2 distance between inputs and reconstructions

–  But cannot consider points in isolation

–  So minimize error over all points: Frobenius norm

•  subject to DTD=Il

∑( ( ) ) ⎛

D*

=

arg min
D

⎝⎜

i,j

1

x(ji) − r

x(i)

j

2⎞ 2 ⎠⎟

•  Use design matrix X, X ∈!m×n

–  Given by stacking all vectors describing the points

•  To derive algorithm for finding D* start by
considering the case l =1
61
–  In this case D is just a single vector d

Machine Learning

Srihari

Final Solution to PCA

•  For l =1, the optimization problem is solved using eigendecomposition
–  Specifically the optimal d is given by the eigenvector of XTX corresponding to the largest eigenvalue
•  More generally, matrix D is given by the l eigenvectors of X corresponding to the largest eigenvalues (Proof by induction)

62

