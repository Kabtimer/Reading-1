[1]

Mastering Python for Data Science
Explore the world of data science through Python and learn how to make sense of data
Samir Madhavan
BIRMINGHAM - MUMBAI

Mastering Python for Data Science
Copyright © 2015 Packt Publishing
All rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews. Every effort has been made in the preparation of this book to ensure the accuracy of the information presented. However, the information contained in this book is sold without warranty, either express or implied. Neither the author, nor Packt Publishing, and its dealers and distributors will be held liable for any damages caused or alleged to be caused directly or indirectly by this book. Packt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee the accuracy of this information.
First published: August 2015
Production reference: 1260815
Published by Packt Publishing Ltd. Livery Place 35 Livery Street Birmingham B3 2PB, UK. ISBN 978-1-78439-015-0 www.packtpub.com

Credits

Author Samir Madhavan
Reviewers Sébastien Celles Robert Dempsey Maurice HT Ling Ratanlal Mahanta Yingssu Tsai
Commissioning Editor Pramila Balan
Acquisition Editor Sonali Vernekar
Content Development Editor Arun Nadar
Technical Editor Chinmay S. Puranik
Copy Editor Sonia Michelle Cheema

Project Coordinator Neha Bhatnagar
Proofreader Safis Editing
Indexer Monica Ajmera Mehta
Graphics Disha Haria Jason Monteiro
Production Coordinator Arvindkumar Gupta
Cover Work Arvindkumar Gupta

About the Author
Samir Madhavan has been working in the field of data science since 2010.
He is an industry expert on machine learning and big data. He has also reviewed R Machine Learning Essentials by Packt Publishing. He was part of the ubiquitous Aadhar project of the Unique Identification Authority of India, which is in the process of helping every Indian get a unique number that is similar to a social security number in the United States. He was also the first employee of Flutura Decision Sciences and Analytics and is a part of the core team that has helped scale the number of employees in the company to 50. His company is now recognized as one of the most promising Internet of Things—Decision Sciences companies in the world.
I would like to thank my mom, Rajasree Madhavan, and dad, P Madhavan, for all their support. I would also like to thank Srikanth Muralidhara, Krishnan Raman, and Derick Jose, who gave me the opportunity to start my career in the world of data science.

About the Reviewers
Sébastien Celles is a professor of applied physics at Universite de Poitiers (working
in the thermal science department). He has used Python for numerical simulations, data plotting, data predictions, and various other tasks since the early 2000s. He is a member of PyData and was granted commit rights to the pandas DataReader project. He is also involved in several open source projects in the scientific Python ecosystem.
Sebastien is also the author of some Python packages available on PyPi, which are as follows:
• openweathermap_requests: This is a package used to fetch data from OpenWeatherMap.org using Requests and Requests-cache and to get pandas DataFrame with weather history
• pandas_degreedays: This is a package used to calculate degree days (a measure of heating or cooling) from the pandas time series of temperature
• pandas_confusion: This is a package used to manage confusion matrices, plot and binarize them, and calculate overall and class statistics
• There are some other packages authored by him, such as pyade, pandas_datareaders_unofficial, and more
He also has a personal interest in data mining, machine learning techniques, forecasting, and so on. You can find more information about him at http://www. celles.net/wiki/Contact or https://www.linkedin.com/in/sebastiencelles.

Robert Dempsey is a leader and technology professional, specializing in
delivering solutions and products to solve tough business challenges. His experience of forming and leading agile teams combined with more than 15 years of technology experience enables him to solve complex problems while always keeping the bottom line in mind.
Robert founded and built three start-ups in the tech and marketing fields, developed and sold two online applications, consulted for Fortune 500 and Inc. 500 companies, and has spoken nationally and internationally on software development and agile project management.
He's the founder of Data Wranglers DC, a group dedicated to improving the craft of data wrangling, as well as a board member of Data Community DC. He is currently the team leader of data operations at ARPC, an econometrics firm based in Washington, DC.
In addition to spending time with his growing family, Robert geeks out on Raspberry Pi's, Arduinos, and automating more of his life through hardware and software.
Maurice HT Ling has been programming in Python since 2003. Having completed
his PhD in bioinformatics and BSc (Hons) in molecular and cell biology from The University of Melbourne, he is currently a research fellow at Nanyang Technological University, Singapore. He is also an honorary fellow of The University of Melbourne, Australia. Maurice is the chief editor of Computational and Mathematical Biology and coeditor of The Python Papers. Recently, he cofounded the first synthetic biology start-up in Singapore, called AdvanceSyn Pte. Ltd., as the director and chief technology officer. His research interests lie in life itself, such as biological life and artificial life, and artificial intelligence, which use computer science and statistics as tools to understand life and its numerous aspects. In his free time, Maurice likes to read, enjoy a cup of coffee, write his personal journal, or philosophize on various aspects of life. His website and LinkedIn profile are http://maurice.vodien.com and http://www.linkedin.com/in/mauriceling, respectively.

Ratanlal Mahanta is a senior quantitative analyst. He holds an MSc degree in
computational finance and is currently working at GPSK Investment Group as a senior quantitative analyst. He has 4 years of experience in quantitative trading and strategy development for sell-side and risk consultation firms. He is an expert in high frequency and algorithmic trading.
He has expertise in the following areas:
• Quantitative trading: This includes FX, equities, futures, options, and engineering on derivatives
• Algorithms: This includes Partial Differential Equations, Stochastic Differential Equations, Finite Difference Method, Monte-Carlo, and Machine Learning
• Code: This includes R Programming, C++, Python, MATLAB, HPC, and scientific computing
• Data analysis: This includes big data analytics (EOD to TBT), Bloomberg, Quandl, and Quantopian
• Strategies: This includes Vol Arbitrage, Vanilla and Exotic Options Modeling, trend following, Mean reversion, Co-integration, Monte-Carlo Simulations, ValueatRisk, Stress Testing, Buy side trading strategies with high Sharpe ratio, Credit Risk Modeling, and Credit Rating
He has already reviewed Mastering Scientific Computing with R, Mastering R for Quantitative Finance, and Machine Learning with R Cookbook, all by Packt Publishing.
You can find out more about him at https://twitter.com/mahantaratan.
Yingssu Tsai is a data scientist. She holds degrees from the University of
California, Berkeley, and the University of California, Los Angeles.

www.PacktPub.com
Support files, eBooks, discount offers, and more
For support files and downloads related to your book, please visit www.PacktPub.com. Did you know that Packt offers eBook versions of every book published, with PDF and ePub files available? You can upgrade to the eBook version at www.PacktPub.com and as a print book customer, you are entitled to a discount on the eBook copy. Get in touch with us at service@packtpub.com for more details. At www.PacktPub.com, you can also read a collection of free technical articles, sign up for a range of free newsletters and receive exclusive discounts and offers on Packt books and eBooks.
TM
https://www2.packtpub.com/books/subscription/packtlib Do you need instant solutions to your IT questions? PacktLib is Packt's online digital book library. Here, you can search, access, and read Packt's entire library of books.
Why subscribe?
• Fully searchable across every book published by Packt • Copy and paste, print, and bookmark content • On demand and accessible via a web browser
Free access for Packt account holders
If you have an account with Packt at www.PacktPub.com, you can use this to access PacktLib today and view 9 entirely free books. Simply use your login credentials for immediate access.

Table of Contents

Preface

vii

Chapter 1: Getting Started with Raw Data

1

The world of arrays with NumPy

2

Creating an array

2

Mathematical operations

3

Array subtraction

4

Squaring an array

4

A trigonometric function performed on the array

4

Conditional operations

4

Matrix multiplication

5

Indexing and slicing

5

Shape manipulation

6

Empowering data analysis with pandas

7

The data structure of pandas

7

Series

7

DataFrame

8

Panel

9

Inserting and exporting data

10

CSV

11

XLS

11

JSON

12

Database

12

Data cleansing

12

Checking the missing data

13

Filling the missing data

14

String operations

16

Merging data

19

Data operations

20

Aggregation operations

20

[i]

Table of Contents

Joins

21

The inner join

22

The left outer join

23

The full outer join

24

The groupby function

24

Summary

25

Chapter 2: Inferential Statistics

27

Various forms of distribution

27

A normal distribution

28

A normal distribution from a binomial distribution

29

A Poisson distribution

33

A Bernoulli distribution

34

A z-score

36

A p-value

40

One-tailed and two-tailed tests

41

Type 1 and Type 2 errors

43

A confidence interval

44

Correlation

48

Z-test vs T-test

51

The F distribution

52

The chi-square distribution

53

Chi-square for the goodness of fit

54

The chi-square test of independence

55

ANOVA

56

Summary

57

Chapter 3: Finding a Needle in a Haystack

59

What is data mining?

60

Presenting an analysis

62

Studying the Titanic

64

Which passenger class has the maximum number of survivors?

65

What is the distribution of survivors based on gender among the

various classes?

68

What is the distribution of nonsurvivors among the various

classes who have family aboard the ship?

71

What was the survival percentage among different age groups?

74

Summary

76

Chapter 4: Making Sense of Data through

Advanced Visualization

77

Controlling the line properties of a chart

78

Using keyword arguments

78

Using the setter methods

79

[ ii ]

Table of Contents

Using the setp() command

80

Creating multiple plots

80

Playing with text

81

Styling your plots

83

Box plots

85

Heatmaps

88

Scatter plots with histograms

91

A scatter plot matrix

94

Area plots

96

Bubble charts

97

Hexagon bin plots

97

Trellis plots

98

A 3D plot of a surface

103

Summary

106

Chapter 5: Uncovering Machine Learning

107

Different types of machine learning

108

Supervised learning

108

Unsupervised learning

109

Reinforcement learning

110

Decision trees

111

Linear regression

112

Logistic regression

114

The naive Bayes classifier

115

The k-means clustering

117

Hierarchical clustering

118

Summary

119

Chapter 6: Performing Predictions with a Linear Regression

121

Simple linear regression

121

Multiple regression

125

Training and testing a model

132

Summary

138

Chapter 7: Estimating the Likelihood of Events

139

Logistic regression

139

Data preparation

140

Creating training and testing sets

141

Building a model

142

Model evaluation

144

Evaluating a model based on test data

148

Model building and evaluation with SciKit

152

Summary

154

[ iii ]

Table of Contents

Chapter 8: Generating Recommendations with

Collaborative Filtering

155

Recommendation data

156

User-based collaborative filtering

157

Finding similar users

157

The Euclidean distance score

157

The Pearson correlation score

160

Ranking the users

165

Recommending items

165

Item-based collaborative filtering

167

Summary

172

Chapter 9: Pushing Boundaries with Ensemble Models

173

The census income dataset

174

Exploring the census data
Hypothesis 1: People who are older earn more Hypothesis 2: Income bias based on working class Hypothesis 3: People with more education earn more Hypothesis 4: Married people tend to earn more Hypothesis 5: There is a bias in income based on race Hypothesis 6: There is a bias in the income based on occupation Hypothesis 7: Men earn more Hypothesis 8: People who clock in more hours earn more Hypothesis 9: There is a bias in income based on the country of origin

175
175 176 177 178 180 181 182 183 184

Decision trees

186

Random forests

187

Summary

192

Chapter 10: Applying Segmentation with k-means Clustering

193

The k-means algorithm and its working

194

A simple example

194

The k-means clustering with countries

199

Determining the number of clusters

201

Clustering the countries

205

Summary

210

Chapter 11: Analyzing Unstructured Data with Text Mining

211

Preprocessing data

211

Creating a wordcloud

215

Word and sentence tokenization

220

Parts of speech tagging

221

Stemming and lemmatization

223

Stemming

223

Lemmatization

226

[ iv ]

Table of Contents

The Stanford Named Entity Recognizer

227

Performing sentiment analysis on world leaders using Twitter

229

Summary

238

Chapter 12: Leveraging Python in the World of Big Data

239

What is Hadoop?

241

The programming model

241

The MapReduce architecture

242

The Hadoop DFS

242

Hadoop's DFS architecture

243

Python MapReduce

243

The basic word count

243

A sentiment score for each review

246

The overall sentiment score

247

Deploying the MapReduce code on Hadoop

250

File handling with Hadoopy

253

Pig

255

Python with Apache Spark

259

Scoring the sentiment

259

The overall sentiment

261

Summary

263

Index

265

[v]

Preface
Data science is an exciting new field that is used by various organizations to perform data-driven decisions. It is a combination of technical knowledge, mathematics, and business. Data scientists have to wear various hats to work with data and derive some value out of it. Python is one of the most popular languages among all the languages used by data scientists. It is a simple language to learn and is used for purposes, such as web development, scripting, and application development to name a few.
The ability to perform data science using Python is very powerful as it helps clean data at a raw level to create advanced machine learning algorithms that predict customer churns for a retail company. This book explains various concepts of data science in a structured manner with the application of these concepts on data to see how to interpret results. The book provides a good base for understanding the advanced topics of data science and how to apply them in a real-world scenario.
What this book covers
Chapter 1, Getting Started with Raw Data, teaches you the techniques of handling unorganized data. You'll also learn how to extract data from different sources, as well as how to clean and manipulate it.
Chapter 2, Inferential Statistics, goes beyond descriptive statistics, where you'll learn about inferential statistics concepts, such as distributions, different statistical tests, the errors in statistical tests, and confidence intervals.
Chapter 3, Finding a Needle in a Haystack, explains what data mining is and how it can be utilized. There is a lot of information in data but finding meaningful information is an art.
[ vii ]

Preface
Chapter 4, Making Sense of Data through Advanced Visualization, teaches you how to create different visualizations of data. Visualization is an integral part of data science; it helps communicate a pattern or relationship that cannot be seen by looking at raw data.
Chapter 5, Uncovering Machine Learning, introduces you to the different techniques of machine learning and how to apply them. Machine learning is the new buzzword in the industry. It's used in activities, such as Google's driverless cars and predicting the effectiveness of marketing campaigns.
Chapter 6, Performing Predictions with a Linear Regression, helps you build a simple regression model followed by multiple regression models along with methods to test the effectiveness of the models. Linear regression is one of the most popular techniques used in model building in the industry today.
Chapter 7, Estimating the Likelihood of Events, teaches you how to build a logistic regression model and the different techniques of evaluating it. With logistic regression, you'll be able learn how to estimate the likelihood of an event taking place.
Chapter 8, Generating Recommendations with Collaborative Filtering, teaches you to create a recommendation model and apply it. It is similar to websites, such as Amazon, which are able to suggest items that you would probably buy on their page.
Chapter 9, Pushing Boundaries with Ensemble Models, familiarizes you with ensemble techniques, which are used to combine the power of multiple models to enhance the accuracy of predictions. This is done because sometimes a single model is not enough to estimate the outcome.
Chapter 10, Applying Segmentation with k-means Clustering, teaches you about k-means clustering and how to use it. Segmentation is widely used in the industry to group similar customers together.
Chapter 11, Analyzing Unstructured Data with Text Mining, teaches you to process unstructured data and make sense of it. There is more unstructured data in the world than structured data.
Chapter 12, Leveraging Python in the World of Big Data, teaches you to use Hadoop and Spark with Python to handle data in this chapter. With the ever increasing size of data, big data technologies have been brought into existence to handle such data.
[ viii ]

What you need for this book
The following softwares are required for this book:
• Ubuntu OS, preferably 14.04 • Python 2.7 • The pandas 0.16.2 library • The NumPy 1.9.2 library • The SciPy 0.16 library • IPython 4.0 • The SciKit 0.16.1 module • The statsmodels 0.6.1 module • The matplotlib 1.4.3 library • Apache Hadoop CDH4 (Cloudera Hadoop 4) with MRv1
(MapReduce version 1) • Apache Spark 1.4.0

Preface

Who this book is for
If you are a Python developer who wants to master the world of data science,
then this book is for you. It is assumed that you already have some knowledge
of data science.

Conventions
In this book, you will find a number of styles of text that distinguish between different kinds of information. Here are some examples of these styles, and an explanation of their meaning.
Code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles are shown as follows: "The json.load() function loads the data into Python."
Any command-line input or output is written as follows:
$ pig ./BigData/pig_sentiment.pig

[ ix ]

Preface
New terms and important words are shown in bold.
Warnings or important notes appear in a box like this.
Tips and tricks appear like this.
Reader feedback
Feedback from our readers is always welcome. Let us know what you think about this book—what you liked or may have disliked. Reader feedback is important for us to develop titles that you really get the most out of. To send us general feedback, simply send an e-mail to feedback@packtpub.com, and mention the book title via the subject of your message. If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, see our author guide on www.packtpub.com/authors.
Customer support
Now that you are the proud owner of a Packt book, we have a number of things to help you to get the most from your purchase.
Downloading the example code
You can download the example code files for all Packt books you have purchased from your account at http://www.packtpub.com. If you purchased this book elsewhere, you can visit http://www.packtpub.com/support and register to have the files e-mailed directly to you. The codes provided in the code bundle are for both IPython notebook and Python 2.7. In the chapters, Python conventions have been followed.
[x]

Preface
Downloading the color images of this book
We also provide you a PDF file that has color images of the screenshots/diagrams used in this book. The color images will help you better understand the changes in the output. You can download this file from: http://www.packtpub.com/sites/ default/files/downloads/0150OS_ColorImage.pdf.
Errata
Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you find a mistake in one of our books—maybe a mistake in the text or the code—we would be grateful if you would report this to us. By doing so, you can save other readers from frustration and help us improve subsequent versions of this book. If you find any errata, please report them by visiting http://www.packtpub.com/ submit-errata, selecting your book, clicking on the errata submission form link, and entering the details of your errata. Once your errata are verified, your submission will be accepted and the errata will be uploaded on our website, or added to any list of existing errata, under the Errata section of that title. Any existing errata can be viewed by selecting your title from http://www.packtpub.com/support.
Piracy
Piracy of copyright material on the Internet is an ongoing problem across all media. At Packt, we take the protection of our copyright and licenses very seriously. If you come across any illegal copies of our works, in any form, on the Internet, please provide us with the location address or website name immediately so that we can pursue a remedy.
Please contact us at copyright@packtpub.com with a link to the suspected pirated material.
We appreciate your help in protecting our authors, and our ability to bring you valuable content.
Questions
You can contact us at questions@packtpub.com if you are having a problem with any aspect of the book, and we will do our best to address it.
[ xi ]

Getting Started with Raw Data
In the world of data science, raw data comes in many forms and sizes. There is a lot of information that can be extracted from this raw data. To give an example, Amazon collects click stream data that records each and every click of the user on the website. This data can be utilized to understand if a user is a price-sensitive customer or prefer more popularly rated products. You must have noticed recommended products in Amazon; they are derived using such data. The first step towards such an analysis would be to parse raw data. The parsing of the data involves the following steps:
• Extracting data from the source: Data can come in many forms, such as Excel, CSV, JSON, databases, and so on. Python makes it very easy to read data from these sources with the help of some useful packages, which will be covered in this chapter.
• Cleaning the data: Once a sanity check has been done, one needs to clean the data appropriately so that it can be utilized for analysis. You may have a dataset about students of a class and details about their height, weight, and marks. There may also be certain rows with the height or weight missing. Depending on the analysis being performed, these rows with missing values can either be ignored or replaced with the average height or weight.
[1]

Getting Started with Raw Data
In this chapter we will cover the following topics: • Exploring arrays with NumPy • Handling data with pandas • Reading and writing data from various formats • Handling missing data • Manipulating data
The world of arrays with NumPy
Python, by default, comes with a data structure, such as List, which can be utilized for array operations, but a Python list on its own is not suitable to perform heavy mathematical operations, as it is not optimized for it. NumPy is a wonderful Python package produced by Travis Oliphant, which has been created fundamentally for scientific computing. It helps handle large multidimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays. A NumPy array would require much less memory to store the same amount of data compared to a Python list, which helps in reading and writing from the array in a faster manner.
Creating an array
A list of numbers can be passed to the following array function to create a NumPy array object:
>>> import numpy as np
>>> n_array = np.array([[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]])
[2]

Chapter 1
A NumPy array object has a number of attributes, which help in giving information about the array. Here are its important attributes:
• ndim: This gives the number of dimensions of the array. The following shows that the array that we defined had two dimensions:
>>> n_array.ndim 2
n_array has a rank of 2, which is a 2D array. • shape: This gives the size of each dimension of the array:
>>> n_array.shape (3, 4)
The first dimension of n_array has a size of 3 and the second dimension has a size of 4. This can be also visualized as three rows and four columns. • size: This gives the number of elements:
>>> n_array.size 12
The total number of elements in n_array is 12. • dtype: This gives the datatype of the elements in the array:
>>> n_array.dtype.name int64
The number is stored as int64 in n_array.
Mathematical operations
When you have an array of data, you would like to perform certain mathematical operations on it. We will now discuss a few of the important ones in the following sections.
[3]

Getting Started with Raw Data
Array subtraction
The following commands subtract the a array from the b array to get the resultant c array. The subtraction happens element by element:
>>> a = np.array( [11, 12, 13, 14]) >>> b = np.array( [ 1, 2, 3, 4]) >>> c = a - b >>> c Array[10 10 10 10]
Do note that when you subtract two arrays, they should be of equal dimensions.
Squaring an array
The following command raises each element to the power of 2 to obtain this result:
>>> b**2 [1 4 9 16]
A trigonometric function performed on the array
The following command applies cosine to each of the values in the b array to obtain the following result:
>>> np.cos(b) [ 0.54030231 -0.41614684 -0.9899925 -0.65364362]
Conditional operations
The following command will apply a conditional operation to each of the elements of the b array, in order to generate the respective Boolean values:
>>> b<2 [ True False False False]
[4]

Chapter 1
Matrix multiplication
Two matrices can be multiplied element by element or in a dot product. The following commands will perform the element-by-element multiplication:
>>> A1 = np.array([[1, 1], [0, 1]])
>>> A2 = np.array([[2, 0], [3, 4]])
>>> A1 * A2 [[2 0] [0 4]]
The dot product can be performed with the following command:
>>> np.dot(A1, A2) [[5 4] [3 4]]
Indexing and slicing
If you want to select a particular element of an array, it can be achieved using indexes:
>>> n_array[0,1] 1
The preceding command will select the first array and then select the second value in the array. It can also be seen as an intersection of the first row and the second column of the matrix. If a range of values has to be selected on a row, then we can use the following command:
>>> n_array[ 0 , 0:3 ] [0 1 2]
[5]

Getting Started with Raw Data
The 0:3 value selects the first three values of the first row. The whole row of values can be selected with the following command:
>>> n_array[ 0 , : ] [0 1 2 3]
Using the following command, an entire column of values need to be selected:
>>> n_array[ : , 1 ] [1 5 9]
Shape manipulation
Once the array has been created, we can change the shape of it too. The following command flattens the array:
>>> n_array.ravel() [ 0 1 2 3 4 5 6 7 8 9 10 11]
The following command reshapes the array in to a six rows and two columns format. Also, note that when reshaping, the new shape should have the same number of elements as the previous one:
>>> n_array.shape = (6,2) >>> n_array [[ 0 1] [ 2 3] [ 4 5] [ 6 7] [ 8 9] [10 11]]
The array can be transposed too:
>>> n_array.transpose() [[ 0 2 4 6 8 10] [ 1 3 5 7 9 11]]
[6]

Chapter 1
Empowering data analysis with pandas
The pandas library was developed by Wes McKinny when he was working at AQR Capital Management. He wanted a tool that was flexible enough to perform quantitative analysis on financial data. Later, Chang She joined him and helped develop the package further. The pandas library is an open source Python library, specially designed for data analysis. It has been built on NumPy and makes it easy to handle data. NumPy is a fairly low-level tool that handles matrices really well. The pandas library brings the richness of R in the world of Python to handle data. It's has efficient data structures to process data, perform fast joins, and read data from various sources, to name a few.
The data structure of pandas
The pandas library essentially has three data structures: 1. Series 2. DataFrame 3. Panel
Series
Series is a one-dimensional array, which can hold any type of data, such as integers, floats, strings, and Python objects too. A series can be created by calling the following:
>>> import pandas as pd >>> pd.Series(np.random.randn(5))
0 0.733810 1 -1.274658 2 -1.602298 3 0.460944 4 -0.632756 dtype: float64
[7]

Getting Started with Raw Data
The random.randn parameter is part of the NumPy package and it generates random numbers. The series function creates a pandas series that consists of an index, which is the first column, and the second column consists of random values. At the bottom of the output is the datatype of the series. The index of the series can be customized by calling the following:
>>> pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e'])
a -0.929494 b -0.571423 c -1.197866 d 0.081107 e -0.035091 dtype: float64
A series can be derived from a Python dict too:
>>> d = {'A': 10, 'B': 20, 'C': 30} >>> pd.Series(d)
A 10 B 20 C 30 dtype: int64
DataFrame
DataFrame is a 2D data structure with columns that can be of different datatypes. It can be seen as a table. A DataFrame can be formed from the following data structures:
• A NumPy array • Lists • Dicts • Series • A 2D NumPy array
[8]

Chapter 1
A DataFrame can be created from a dict of series by calling the following commands:
>>> d = {'c1': pd.Series(['A', 'B', 'C']), 'c2': pd.Series([1, 2., 3., 4.])}
>>> df = pd.DataFrame(d) >>> df
c1 c2 0 A1 1 B2 2 C3 3 NaN 4
The DataFrame can be created using a dict of lists too:
>>> d = {'c1': ['A', 'B', 'C', 'D'], 'c2': [1, 2.0, 3.0, 4.0]}
>>> df = pd.DataFrame(d) >>> print df
c1 c2 0A 1 1B 2 2C 3 3D 4
Panel
A Panel is a data structure that handles 3D data. The following command is an example of panel data:
>>> d = {'Item1': pd.DataFrame(np.random.randn(4, 3)), 'Item2': pd.DataFrame(np.random.randn(4, 2))}
>>> pd.Panel(d)
<class 'pandas.core.panel.Panel'> Dimensions: 2 (items) x 4 (major_axis) x 3 (minor_axis) Items axis: Item1 to Item2 Major_axis axis: 0 to 3 Minor_axis axis: 0 to 2
[9]

Getting Started with Raw Data
The preceding command shows that there are 2 DataFrames represented by two items. There are four rows represented by four major axes and three columns represented by three minor axes.

Inserting and exporting data
The data is stored in various forms, such as CSV, TSV, databases, and so on. The pandas library makes it convenient to read data from these formats or to export to these formats. We'll use a dataset that contains the weight statistics of the school students from the U.S..
We'll be using a file with the following structure:

Column LOCATION CODE COUNTY AREA NAME REGION SCHOOL YEARS NO. OVERWEIGHT PCT OVERWEIGHT NO. OBESE PCT OBESE NO. OVERWEIGHT OR OBESE
PCT OVERWEIGHT OR OBESE GRADE LEVEL AREA TYPE STREET ADDRESS CITY STATE ZIP CODE Location 1

Description Unique location code The county the school belongs to The district the school belongs to The region the school belongs to The school year the data is addressing The number of overweight students The percentage of overweight students The number of obese students The percentage of obese students The number of students who are overweight or obese The percentage of students who are overweight or obese Whether they belong to elementary or high school The type of area The address of the school The city the school belongs to The state the school belongs to The zip code of the school The address with longitude and latitude

[ 10 ]

Chapter 1
CSV
To read data from a .csv file, the following read_csv function can be used:
>>> d = pd.read_csv('Data/Student_Weight_Status_Category_ Reporting_Results__Beginning_2010.csv')
>>> d[0:5]['AREA NAME']

0 RAVENA COEYMANS SELKIRK CENTRAL SCHOOL DISTRICT

1 RAVENA COEYMANS SELKIRK CENTRAL SCHOOL DISTRICT

2 RAVENA COEYMANS SELKIRK CENTRAL SCHOOL DISTRICT

3

COHOES CITY SCHOOL DISTRICT

4

COHOES CITY SCHOOL DISTRICT

The read_csv function takes the path of the .csv file to input the data. The command after this prints the first five rows of the Location column in the data.

To write a data to the .csv file, the following to_csv function can be used:

>>> d = {'c1': pd.Series(['A', 'B', 'C']), 'c2': pd.Series([1, 2., 3., 4.])}
>>> df = pd.DataFrame(d) >>> df.to_csv('sample_data.csv')

The DataFrame is written to a .csv file by using the to_csv method. The path and the filename where the file needs to be created should be mentioned.

XLS
In addition to the pandas package, the xlrd package needs to be installed for pandas to read the data from an Excel file:
>>> d=pd.read_excel('Data/Student_Weight_Status_Category _Reporting_Results__Beginning_2010.xls')
The preceding function is similar to the CSV reading command. To write to an Excel file, the xlwt package needs to be installed:
>>> df.to_excel('sample_data.xls')

[ 11 ]

Getting Started with Raw Data
JSON
To read the data from a JSON file, Python's standard json package can be used. The following commands help in reading the file:
>>> import json >>> json_data = open('Data/Student_Weight_Status_Category
_Reporting_Results__Beginning_2010.json') >>> data = json.load(json_data) >>> json_data.close()
In the preceding command, the open() function opens a connection to the file. The json.load() function loads the data into Python. The json_data.close() function closes the connection to the file.
The pandas library also provides a function to read the JSON file, which can be accessed using pd.read_json().
Database
To read data from a database, the following function can be used:
>>> pd.read_sql_table(table_name, con)
The preceding command generates a DataFrame. If a table name and an SQLAlchemy engine are given, they return a DataFrame. This function does not support the DBAPI connection. The following are the description of the parameters used:
• table_name: This refers to the name of the SQL table in a database • con: This refers to the SQLAlchemy engine
The following command reads SQL query into a DataFrame:
>>> pd.read_sql_query(sql, con)
The following are the description of the parameters used:
• sql: This refers to the SQL query that is to be executed • con: This refers to the SQLAlchemy engine
Data cleansing
The data in its raw form generally requires some cleaning so that it can be analyzed or a dashboard can be created on it. There are many reasons that data might have issues. For example, the Point of Sale system at a retail shop might have malfunctioned and inputted some data with missing values. We'll be learning how to handle such data in the following section.
[ 12 ]

Chapter 1

Checking the missing data
Generally, most data will have some missing values. There could be various reasons for this: the source system which collects the data might not have collected the values or the values may never have existed. Once you have the data loaded, it is essential to check the missing elements in the data. Depending on the requirements, the missing data needs to be handled. It can be handled by removing a row or replacing a missing value with an alternative value.

In the Student Weight data, to check if the location column has missing value, the following command can be utilized:

>>> d['Location 1'].isnull()

0

False

1

False

2

False

3

False

4

False

5

False

6

False

The notnull() method will output each row of the value as TRUE or FALSE. If it's False, then there is a missing value. This data can be aggregated to find the number of instances of the missing value:

>>> d['Location 1'].isnull().value_counts()

False 3246

True

24

dtype: int64

The preceding command shows that the Location 1 column has 24 instances of missing values. These missing values can be handled by either removing the rows with the missing values or replacing it with some values. To remove the rows, execute the following command:

>>> d = d['Location 1'].dropna()

To remove all the rows with an instance of missing values, use the following command:

>>> d = d.dropna(how='any')

[ 13 ]

Getting Started with Raw Data

Filling the missing data
Let's define some DataFrames to work with:

>>> df = pd.DataFrame(np.random.randn(5, 3), index=['a0', 'a10', 'a20', 'a30', 'a40'],

columns=['X', 'Y', 'Z'])

>>> df

X

Y

Z

a0 -0.854269 0.117540 1.515373

a10 -0.483923 -0.379934 0.484155

a20 -0.038317 0.196770 -0.564176

a30 0.752686 1.329661 -0.056649

a40 -1.383379 0.632615 1.274481

We'll now add some extra row indexes, which will create null values in our DataFrame:

>>> df2 = df2.reindex(['a0', 'a1', 'a10', 'a11', 'a20', 'a21', 'a30', 'a31', 'a40', 'a41'])
>>> df2

X

Y

Z

a0 -1.193371 0.912654 -0.780461

a1

NaN

NaN

NaN

a10 1.413044 0.615997 0.947334

a11

NaN

NaN

NaN

a20 1.583516 1.388921 0.458771

a21

NaN

NaN

NaN

a30 0.479579 1.427625 1.407924

a31

NaN

NaN

NaN

a40 0.455510 -0.880937 1.375555

a41

NaN

NaN

NaN

If you want to replace the null values in the df2 DataFrame with a value of zero in the following case, execute the following command:

>>> df2.fillna(0)

X

Y

Z

a0 -1.193371 0.912654 -0.780461

[ 14 ]

Chapter 1
a1 0.000000 0.000000 0.000000 a10 1.413044 0.615997 0.947334 a11 0.000000 0.000000 0.000000 a20 1.583516 1.388921 0.458771 a21 0.000000 0.000000 0.000000 a30 0.479579 1.427625 1.407924 a31 0.000000 0.000000 0.000000 a40 0.455510 -0.880937 1.375555 a41 0.000000 0.000000 0.000000
If you want to fill the value with forward propagation, which means that the value previous to the null value in the column will be used to fill the null value, the following command can be used:
>>> df2.fillna(method='pad') #filling with forward propagation

X

Y

Z

a0 -1.193371 0.912654 -0.780461

a1 -1.193371 0.912654 -0.780461

a10 1.413044 0.615997 0.947334

a11 1.413044 0.615997 0.947334

a20 1.583516 1.388921 0.458771

a21 1.583516 1.388921 0.458771

a30 0.479579 1.427625 1.407924

a31 0.479579 1.427625 1.407924

a40 0.455510 -0.880937 1.375555

a41 0.455510 -0.880937 1.375555

If you want to fill the null values of the column with the column mean, then the following command can be utilized:

>>> df2.fillna(df2.mean())

X a0 -1.193371 a1 0.547655 a10 1.413044 a11 0.547655 a20 1.583516

Y

Z

0.912654 -0.780461

0.692852 0.681825

0.615997 0.947334

0.692852 0.681825

1.388921 0.458771

[ 15 ]

Getting Started with Raw Data
a21 0.547655 0.692852 0.681825 a30 0.479579 1.427625 1.407924 a31 0.547655 0.692852 0.681825 a40 0.455510 -0.880937 1.375555 a41 0.547655 0.692852 0.681825

String operations
Sometimes, you would want to modify the string field column in your data. The following technique explains some of the string operations:
• Substring: Let's start by choosing the first five rows of the AREA NAME column in the data as our sample data to modify:
>>> df = pd.read_csv('Data/Student_Weight_Status_Category_ Reporting_Results__Beginning_2010.csv')
>>> df['AREA NAME'][0:5]

0 RAVENA COEYMANS SELKIRK CENTRAL SCHOOL DISTRICT

1 RAVENA COEYMANS SELKIRK CENTRAL SCHOOL DISTRICT

2 RAVENA COEYMANS SELKIRK CENTRAL SCHOOL DISTRICT

3

COHOES CITY SCHOOL DISTRICT

4

COHOES CITY SCHOOL DISTRICT

Name: AREA NAME, dtype: object

In order to extract the first word from the Area Name column, we'll use the extract function as shown in the following command:
>>> df['AREA NAME'][0:5].str.extract('(\w+)')

0 RAVENA 1 RAVENA 2 RAVENA 3 COHOES 4 COHOES Name: AREA NAME, dtype: object

[ 16 ]

Chapter 1

In the preceding command, the str attribute of the series is utilized. The str class contains an extract method, where a regular expression could be fed to extract data, which is very powerful. It is also possible to extract a second word in AREA NAME as a separate column:

>>> df['AREA NAME'][0:5].str.extract('(\w+)\s(\w+)')

0

1

0 RAVENA COEYMANS

1 RAVENA COEYMANS

2 RAVENA COEYMANS

3 COHOES

CITY

4 COHOES

CITY

To extract data in different columns, the respective regular expression needs to be enclosed in separate parentheses.

• Filtering: If we want to filter rows with data on ELEMENTARY school, then the following command can be used:
>>> df[df['GRADE LEVEL'] == 'ELEMENTARY']

• Uppercase: To convert the area name to uppercase, we'll use the following command:

>>> df['AREA NAME'][0:5].str.upper()

0 RAVENA COEYMANS SELKIRK CENTRAL SCHOOL DISTRICT

1 RAVENA COEYMANS SELKIRK CENTRAL SCHOOL DISTRICT

2 RAVENA COEYMANS SELKIRK CENTRAL SCHOOL DISTRICT

3

COHOES CITY SCHOOL DISTRICT

4

COHOES CITY SCHOOL DISTRICT

Name: AREA NAME, dtype: object

Since the data strings are in uppercase already, there won't be any difference seen.

• Lowercase: To convert Area Name to lowercase, we'll use the following command:

>>> df['AREA NAME'][0:5].str.lower()

0 ravena coeymans selkirk central school district

1 ravena coeymans selkirk central school district

2 ravena coeymans selkirk central school district

3

cohoes city school district

4

cohoes city school district

Name: AREA NAME, dtype: object

[ 17 ]

Getting Started with Raw Data
• Length: To find the length of each element of the Area Name column, we'll use the following command:
>>> df['AREA NAME'][0:5].str.len() 0 47 1 47 2 47 3 27 4 27 Name: AREA NAME, dtype: int64
• Split: To split Area Name based on a whitespace, we'll use the following command:
>>> df['AREA NAME'][0:5].str.split(' ')

0 [RAVENA, COEYMANS, SELKIRK, CENTRAL, SCHOOL, D...

1 [RAVENA, COEYMANS, SELKIRK, CENTRAL, SCHOOL, D...

2 [RAVENA, COEYMANS, SELKIRK, CENTRAL, SCHOOL, D...

3

[COHOES, CITY, SCHOOL, DISTRICT]

4

[COHOES, CITY, SCHOOL, DISTRICT]

Name: AREA NAME, dtype: object

• Replace: If we want to replace all the area names ending with DISTRICT to DIST, then the following command can be used:
>>> df['AREA NAME'][0:5].str.replace('DISTRICT$', 'DIST')

0 RAVENA COEYMANS SELKIRK CENTRAL SCHOOL DIST

1 RAVENA COEYMANS SELKIRK CENTRAL SCHOOL DIST

2 RAVENA COEYMANS SELKIRK CENTRAL SCHOOL DIST

3

COHOES CITY SCHOOL DIST

4

COHOES CITY SCHOOL DIST

Name: AREA NAME, dtype: object

The first argument in the replace method is the regular expression used to identify the portion of the string to replace. The second argument is the value for it to be replaced with.

[ 18 ]

Chapter 1
Merging data
To combine datasets together, the concat function of pandas can be utilized. Let's take the Area Name and the County columns with its first five rows:
>>> d[['AREA NAME', 'COUNTY']][0:5]

AREA NAME

0 RAVENA COEYMANS SELKIRK CENTRAL SCHOOL DISTRICT

1 RAVENA COEYMANS SELKIRK CENTRAL SCHOOL DISTRICT

2 RAVENA COEYMANS SELKIRK CENTRAL SCHOOL DISTRICT

3

COHOES CITY SCHOOL DISTRICT

4

COHOES CITY SCHOOL DISTRICT

COUNTY ALBANY ALBANY ALBANY ALBANY ALBANY

We can divide the data as follows:

>>> p1 = d[['AREA NAME', 'COUNTY']][0:2] >>> p2 = d[['AREA NAME', 'COUNTY']][2:5]

The first two rows of the data are in p1 and the last three rows are in p2. These pieces can be combined using the concat() function:

>>> pd.concat([p1,p2])

AREA NAME

0 RAVENA COEYMANS SELKIRK CENTRAL SCHOOL DISTRICT

1 RAVENA COEYMANS SELKIRK CENTRAL SCHOOL DISTRICT

2 RAVENA COEYMANS SELKIRK CENTRAL SCHOOL DISTRICT

3

COHOES CITY SCHOOL DISTRICT

4

COHOES CITY SCHOOL DISTRICT

COUNTY ALBANY ALBANY ALBANY ALBANY ALBANY

The combined pieces can be identified by assigning a key:

>>> concatenated = pd.concat([p1,p2], keys = ['p1','p2'])

>>> concatenated

AREA NAME

COUNTY

p1 0 RAVENA COEYMANS SELKIRK CENTRAL SCHOOL DISTRICT

ALBANY

1 RAVENA COEYMANS SELKIRK CENTRAL SCHOOL DISTRICT

ALBANY

p2 2 RAVENA COEYMANS SELKIRK CENTRAL SCHOOL DISTRICT

ALBANY

3

COHOES CITY SCHOOL DISTRICT ALBANY

4

COHOES CITY SCHOOL DISTRICT ALBANY

[ 19 ]

Getting Started with Raw Data
Using the keys, the pieces can be extracted back from the concatenated data:
>>> concatenated.ix['p1']

AREA NAME 0 RAVENA COEYMANS SELKIRK CENTRAL SCHOOL DISTRICT 1 RAVENA COEYMANS SELKIRK CENTRAL SCHOOL DISTRICT

COUNTY ALBANY ALBANY

Data operations
Once the missing data is handled, various operations can be performed on the data.
Aggregation operations
There are a number of aggregation operations, such as average, sum, and so on, which you would like to perform on a numerical field. These are the methods used to perform it:
• Average: To find out the average number of students in the ELEMENTARY school who are obese, we'll first filter the ELEMENTARY data with the following command:
>>> data = d[d['GRADE LEVEL'] == 'ELEMENTARY'] 213.41593780369291
Now, we'll find the mean using the following command:
>>> data['NO. OBESE'].mean()
The elementary grade level data is filtered and stored in the data object. The NO. OBESE column is selected, which contains the number of obese students and using the mean() method, the average is taken out.
• SUM: To find out the total number of elementary students who are obese across all the school, use the following command:
>>> data['NO. OBESE'].sum() 219605.0
• MAX: To get the maximum number of students that are obese in an elementary school, use the following command:
>>> data['NO. OBESE'].max() 48843.0

[ 20 ]

Chapter 1
• MIN: To get the minimum number of students that are obese in an elementary school, use the following command:
>>> data['NO. OBESE'].min() 5.0
• STD: To get the standard deviation of the number of obese students, use the following command:
>>> data['NO. OBESE'].std() 1690.3831128098113
• COUNT: To count the total number of schools with the ELEMENTARY grade in the DELAWARE county, use the following command:
>>> data = df[(d['GRADE LEVEL'] == 'ELEMENTARY') & (d['COUNTY'] == 'DELAWARE')]
>>> data['COUNTY'].count() 19
The table is filtered for the ELEMENTARY grade and the DELAWARE county. Notice that the conditions are enclosed in parentheses. This is to ensure that individual conditions are evaluated and if the parentheses are not provided, then Python will throw an error.
Joins
SQL-like joins can be performed on the DataFrame using pandas. Let's define a lookup DataFrame, which assigns levels to each of the grades using the following command:
>>> grade_lookup = {'GRADE LEVEL': pd.Series(['ELEMENTARY', 'MIDDLE/HIGH', 'MISC']), 'LEVEL': pd.Series([1, 2, 3])}
>>> grade_lookup = DataFrame(grade_lookup)
[ 21 ]

Getting Started with Raw Data

Let's take the first five rows of the GRADE data column as an example for performing the joins:

>>> df[['GRADE LEVEL']][0:5]

GRADE LEVEL

0 DISTRICT TOTAL

1

ELEMENTARY

2

MIDDLE/HIGH

3 DISTRICT TOTAL

4

ELEMENTARY

The inner join
The following image is a sample of an inner join:

An inner join can be performed with the following command:
>>> d_sub = df[0:5].join(grade_lookup.set_index(['GRADE LEVEL']), on=['GRADE LEVEL'], how='inner')
>>> d_sub[['GRADE LEVEL', 'LEVEL']]

GRADE LEVEL LEVEL

1 ELEMENTARY

1

4 ELEMENTARY

1

2 MIDDLE/HIGH

2

[ 22 ]

Chapter 1
The join takes place with the join() method. The first argument takes the DataFrame on which the lookup takes place. Note that the grade_lookup DataFrame's index is being set by the set_index() method. This is essential for a join, as without it, the join method won't know on which column to join the DataFrame to.
The second argument takes a column of the d DataFrame to join the data. The third argument defines the join as an inner join.
The left outer join
The following image is a sample of a left outer join:

A left outer join can be performed with the following commands:
>>> d_sub = df[0:5].join(grade_lookup.set_index(['GRADE LEVEL']), on=['GRADE LEVEL'], how='left') >>> d_sub[['GRADE LEVEL', 'LEVEL']]

GRADE LEVEL LEVEL

0 DISTRICT TOTAL NaN

1

ELEMENTARY

1

2

MIDDLE/HIGH

2

3 DISTRICT TOTAL NaN

4

ELEMENTARY

1

You can notice that DISTRICT TOTAL has missing values for a level column, as the grade_lookup DataFrame does not have an instance for DISTRICT TOTAL.

[ 23 ]

Getting Started with Raw Data
The full outer join
The following image is a sample of a full outer join:

The full outer join can be performed with the following commands:
>>> d_sub = df[0:5].join(grade_lookup.set_index(['GRADE LEVEL']), on=['GRADE LEVEL'], how='outer')
>>> d_sub[['GRADE LEVEL', 'LEVEL']]

GRADE LEVEL LEVEL

0 DISTRICT TOTAL NaN

3 DISTRICT TOTAL NaN

1

ELEMENTARY

1

4

ELEMENTARY

1

2

MIDDLE/HIGH

2

4

MISC

3

The groupby function
It's easy to do an SQL-like group by operation with pandas. Let's say, if you want to find the sum of the number of obese students in each of the grades, then you can use the following command:
>>> df['NO. OBESE'].groupby(d['GRADE LEVEL']).sum()
GRADE LEVEL

[ 24 ]

Chapter 1

DISTRICT TOTAL ELEMENTARY MIDDLE/HIGH

127101 72880 53089

This command chooses the number of obese students column, then uses the group by method to group the data-based group level, and finally, the sum method sums up the number. The same can be achieved by the following function too:

>>> d['NO. OBESE'].groupby(d['GRADE LEVEL']).aggregate(sum)

Here, the aggregate method is utilized. The sum function is passed to obtain the required results.

It's also possible to obtain multiple kinds of aggregations on the same metric. This can be achieved by the following command:

>>> df['NO. OBESE'].groupby(d['GRADE LEVEL']).aggregate([sum, mean, std])

sum

mean

std

GRADE LEVEL

DISTRICT TOTAL 127101 128.384848 158.933263

ELEMENTARY

72880 76.958817 100.289578

MIDDLE/HIGH

53089 59.251116 65.905591

Summary
In this chapter, we got familiarized with the NumPy and pandas packages. We understood the different datatypes in pandas and how to utilize them. We learned how to perform data cleansing and manipulation, in which we handled missing values and performed string operations. This chapter gives us a foundation for data science and you can dive deeper into NumPy and pandas by clicking on the following links:
• NumPy documentation: http://docs.scipy.org/doc/
• pandas documentation: http://pandas.pydata.org/
In the next chapter, we'll learn about the meaning of inferential statistics and what they do, and also how to make sense of the different concepts in inferential statistics.

[ 25 ]

Inferential Statistics
Before getting understanding the inferential statistics, let's look at what descriptive statistics is about.
Descriptive statistics is a term given to data analysis that summarizes data in a meaningful way such that patterns emerge from it. It is a simple way to describe data, but it does not help us to reach a conclusion on the hypothesis that we have made. Let's say you have collected the height of 1,000 people living in Hong Kong. The mean of their height would be descriptive statistics, but their mean height does not indicate that it's the average height of whole of Hong Kong. Here, inferential statistics will help us in determining what the average height of whole of Hong Kong would be, which is described in depth in this chapter.
Inferential statistics is all about describing the larger picture of the analysis with a limited set of data and deriving conclusions from it.
In this chapter, we will cover the following topics:
• The different kinds of distributions • Different statistical tests that can be utilized to test a hypothesis • How to make inferences about the population of a sample from the data given • Different kinds of errors that can occur during hypothesis testing • Defining the confidence interval at which the population mean lies • The significance of p-value and how it can be utilized to interpret results
Various forms of distribution
There are various kinds of probability distributions, and each distribution shows the probability of different outcomes for a random experiment. In this section, we'll explore the various kinds of probability distributions.
[ 27 ]

Inferential Statistics
A normal distribution
A normal distribution is the most common and widely used distribution in statistics. It is also called a "bell curve" and "Gaussian curve" after the mathematician Karl Friedrich Gauss. A normal distribution occurs commonly in nature. Let's take the height example we saw previously. If you have data for the height of all the people of a particular gender in Hong Kong city, and you plot a bar chart where each bar represents the number of people at this particular height, then the curve that is obtained will look very similar to the following graph. The numbers in the plot are the standard deviation numbers from the mean, which is zero. The concept will become clearer as we proceed through the chapter.
Also, if you take an hourglass and observe the way sand stacks up when the hour glass is inverted, it forms a normal distribution. This is a good example that shows how normal distribution exists in nature.
[ 28 ]

Chapter 2
Take the following figure: it shows three curves with normal distribution. The curve A has a standard deviation of 1, curve C has a standard deviation of 2, and curve B has a standard deviation of 3, which means that the curve B has the maximum spread of values, whereas curve A has the least spread of values. One more way of looking at it is if curve B represented the height of people of a country, then this country has a lot of people with diverse heights, whereas the country with the curve A distribution will have people whose heights are similar to each other.

A normal distribution from a binomial distribution
Let's take a coin and flip it. The probability of getting a head or a tail is 50%. If you take the same coin and flip it six times, the probability of getting a head three times can be computed using the following formula:

P(x)

=

n!
x!(n −

x)!

pxqn−x

and x is the number of successes desired

In the preceding formula, n is the number of times the coin is flipped, p is the probability of success, and q is (1– p), which is the probability of failure.

[ 29 ]

Inferential Statistics
The SciPy package of Python provides useful functions to perform statistical computations. You can install it from http://www.scipy.org/. The following commands helps in plotting the binomial distribution:
>>> from scipy.stats import binom >>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1) >>> x = [0, 1, 2, 3, 4, 5, 6] >>> n, p = 6, 0.5 >>> rv = binom(n, p) >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
label='Probablity')
>>> ax.legend(loc='best', frameon=False) >>> plt.show()
The binom function in the SciPy package helps generate binomial distributions and the necessary statistics related to it. If you observe the preceding commands, there are parts of it that are from the matplotlib, which we'll use right now to plot the binomial distribution. The matplotlib library will be covered in detail in later chapters. The plt.subplots function helps in generating multiple plots on a screen. The binom function takes in the number of attempts and the probability of success. The ax.vlines function is used to plot vertical lines and rv.pmf within it helps in calculating the probability at various values of x. The ax.legend function adds a legend to the graph, and finally, plt.show displays the graph. The result is as follows:
[ 30 ]

Chapter 2
As you can see in the graph, if the coin is flipped six times, then getting three heads has the maximum probability, whereas getting a single head or five heads has the least probability. Now, let's increase the number of attempts and see the distribution:
>>> fig, ax = plt.subplots(1, 1) >>> x = range(101) >>> n, p = 100, 0.5 >>> rv = binom(n, p) >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
label='Probablity') >>> ax.legend(loc='best', frameon=False) >>> plt.show()
Here, we try to flip the coin 100 times and see the distribution:
[ 31 ]

Inferential Statistics
When the probability of success is changed to 0.4, this is what you see:
When the probability is 0.6, this is what you see:
[ 32 ]

When you flip the coin 1000 times at 0.5 probability:

Chapter 2

As you can see, the binomial distribution has started to resemble a normal distribution.
A Poisson distribution
A Poisson distribution is the probability distribution of independent interval occurrences in an interval. A binomial distribution is used to determine the probability of binary occurrences, whereas, a Poisson distribution is used for count-based distributions. If lambda is the mean occurrence of the events per interval, then the probability of having a k occurrence within a given interval is given by the following formula:
f (k;λ ) = Pr ( X = k ) = λke−λ
k!
Here, e is the Euler's number, k is the number of occurrences for which the probability is going to be determined, and lambda is the mean number of occurrences. Let's understand this with an example. The number of cars that pass through a bridge in an hour is 20. What would be the probability of 23 cars passing through the bridge in an hour?
[ 33 ]

Inferential Statistics
For this, we'll use the poisson function from SciPy:
>>> from scipy.stats import poisson >>> rv = poisson(20) >>> rv.pmf(23)

0.066881473662401172
With the Poisson function, we define the mean value, which is 20 cars. The rv.pmf function gives the probability, which is around 6%, that 23 cars will pass the bridge.

A Bernoulli distribution
You can perform an experiment with two possible outcomes: success or failure. Success has a probability of p, and failure has a probability of 1 - p. A random variable that takes a 1 value in case of a success and 0 in case of failure is called a Bernoulli distribution. The probability distribution function can be written as:

P

(

n)

=

1 −

 

p

p

for n = 0 for n = 1

It can also be written like this:
P (n) = pn (1− )p 1−n

The distribution function can be written like this:

D

(n

)

=

1 − 1

p

for n = 0 for n = 1

[ 34 ]

Following plot shows a Bernoulli distribution:

Chapter 2

Voting in an election is a good example of the Bernoulli distribution.
A Bernoulli distribution can be generated using the bernoulli.rvs() function of the SciPy package. The following function generates a Bernoulli distribution with a probability of 0.7:
>>> from scipy import stats >>> stats.bernoulli.rvs(0.7, size=100) array([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
1, 1, 1, 0, 1, 1, 1, 0, 1, 1])])
If the preceding output is the number of votes for a candidate by people, then the candidate has 70% of the votes.
[ 35 ]

Inferential Statistics
A z-score
A z-score, in simple terms, is a score that expresses the value of a distribution in standard deviation with respect to the mean. Let's take a look at the following formula that calculates the z-score:
z = (X-µ)/ σ
Here, X is the value in the distribution, µ is the mean of the distribution, and σ is the standard deviation of the distribution Let's try to understand this concept from the perspective of a school classroom. A classroom has 60 students in it and they have just got their mathematics examination score. We simulate the score of these 60 students with a normal distribution using the following command:
>>> classscore >>> classscore = np.random.normal(50, 10, 60).round()
[ 56. 52. 60. 65. 39. 49. 41. 51. 48. 52. 47. 41. 60. 54. 41.
46. 37. 50. 50. 55. 47. 53. 38. 42. 42. 57. 40. 45. 35. 39.
67. 56. 35. 45. 47. 52. 48. 53. 53. 50. 61. 60. 57. 53. 56.
68. 43. 35. 45. 42. 33. 43. 49. 54. 45. 54. 48. 55. 56. 30.]
The NumPy package has a random module that has a normal function, where 50 is given as the mean of the distribution, 10 is the standard deviation of the distribution, and 60 is the number of values to be generated. You can plot the normal distribution with the following commands:
>>> plt.hist(classscore, 30, normed=True) #Number of breaks is 30 >>> plt.show()
[ 36 ]

Chapter 2
The score of each student can be converted to a z-score using the following functions:
>>> stats.zscore(classscore)
[ 0.86008868 0.38555699 1.33462036 1.92778497 -1.15667098 0.02965823
-0.91940514 0.26692407 -0.08897469 0.38555699 -0.20760761 0.91940514 1.33462036 0.62282284 -0.91940514 -0.32624053 -1.39393683 0.14829115 0.14829115 0.74145576 -0.20760761 0.50418992 -1.2753039 0.80077222
-0.80077222 0.9787216 -1.03803806 -0.44487345 -1.63120267 1.15667098 2.16505081 0.86008868 -1.63120267 -0.44487345 -0.20760761 0.38555699
-0.08897469 0.50418992 0.50418992 0.14829115 1.45325329 1.33462036 0.9787216 0.50418992 0.86008868 2.28368373 -0.6821393 1.63120267
-0.44487345 -0.80077222 -1.86846851 -0.6821393 0.02965823 0.62282284
-0.44487345 0.62282284 -0.08897469 0.74145576 0.86008868 2.22436727]
[ 37 ]

Inferential Statistics
So, a student with a score of 60 out of 100 has a z-score of 1.334. To make more sense of the z-score, we'll use the standard normal table. This table helps in determining the probability of a score. We would like to know what the probability of getting a score above 60 would be.
The standard normal table can help us in determining the probability of the occurrence of the score, but we do not have to perform the cumbersome task of finding the value by looking through the table and finding the probability. This task is made simple by the cdf function, which is the cumulative distribution function:
>>> prob = 1 - stats.norm.cdf(1.334) >>> prob 0.091101928265359899
The cdf function gives the probability of getting values up to the z-score of 1.334, and doing a minus one of it will give us the probability of getting a z-score, which is above it. In other words, 0.09 is the probability of getting marks above 60. Let's ask another question, "how many students made it to the top 20% of the class?"
[ 38 ]

Chapter 2
Here, we'll have to work backwards to determine the marks at which all the students above it are in the top 20% of the class:
Now, to get the z-score at which the top 20% score marks, we can use the ppf function in SciPy:
>>> stats.norm.ppf(0.80) 0.8416212335729143
The z-score for the preceding output that determines whether the top 20% marks are at 0.84 is as follows:
>>> (0.84 * classscore.std()) + classscore.mean() 55.942594176524267
We multiply the z-score with the standard deviation and then add the result with the mean of the distribution. This helps in converting the z-score to a value in the distribution. The 55.83 marks means that students who have marks more than this are in the top 20% of the distribution. The z-score is an essential concept in statistics, which is widely used. Now you can understand that it is basically used in standardizing any distribution so that it can be compared or inferences can be derived from it.
[ 39 ]

Inferential Statistics
A p-value
A p-value is the probability of rejecting a null-hypothesis when the hypothesis is proven true. The null hypothesis is a statement that says that there is no difference between two measures. If the hypothesis is that people who clock in 4 hours of study everyday score more that 90 marks out of 100. The null hypothesis here would be that there is no relation between the number of hours clocked in and the marks scored. If the p-value is equal to or less than the significance level (α), then the null hypothesis is inconsistent and it needs to be rejected.
Let's understand this concept with an example where the null hypothesis is that it is common for students to score 68 marks in mathematics. Let's define the significance level at 5%. If the p-value is less than 5%, then the null hypothesis is rejected and it is not common to score 68 marks in mathematics. Let's get the z-score of 68 marks:
>>> zscore = ( 68 - classscore.mean() ) / classscore.std() >>> zscore 2.283
[ 40 ]

Now, let's get the value:
>>> prob = 1 - stats.norm.cdf(zscore) >>> prob 0.032835182628040638

Chapter 2

So, you can see that the p-value is at 3.2%, which is lower than the significance level. This means that the null hypothesis can be rejected, and it can be said that it's not common to get 68 marks in mathematics.
One-tailed and two-tailed tests
The example in the previous section was an instance of a one-tailed test where the null hypothesis is rejected or accepted based on one direction of the normal distribution.
[ 41 ]

Inferential Statistics
In a two-tailed test, both the tails of the null hypothesis are used to test the hypothesis.
In a two-tailed test, when a significance level of 5% is used, then it is distributed equally in the both directions, that is, 2.5% of it in one direction and 2.5% in the other direction. Let's understand this with an example. The mean score of the mathematics exam at a national level is 60 marks and the standard deviation is 3 marks. The mean marks of a class are 53. The null hypothesis is that the mean marks of the class are similar to the national average. Let's test this hypothesis by first getting the z-score 60:
>>> zscore = ( 53 - 60 ) / 3.0 >>> zscore -2.3333333333333335
The p-value would be:
>>> prob = stats.norm.cdf(zscore) >>> prob 0.0098153286286453336
[ 42 ]

Chapter 2
So, the p-value is 0.98%. The null hypothesis is to be rejected, and the p-value should be less than 2.5% in either direction of the bell curve. Since the p-value is less than 2.5%, we can reject the null hypothesis and clearly state that the average marks of the class are significantly different from the national average.
Type 1 and Type 2 errors
Type 1 error is a type of error that occurs when there is a rejection of the null hypothesis when it is actually true. This kind of error is also called an error of the first kind and is equivalent to false positives.
Let's understand this concept using an example. There is a new drug that is being developed and it needs to be tested on whether it is effective in combating diseases. The null hypothesis is that it is not effective in combating diseases. The significance level is kept at 5% so that the null hypothesis can be accepted confidently 95% of the time. However, 5% of the time, we'll accept the rejecttion of the hypothesis although it had to be accepted, which means that even though the drug is ineffective, it is assumed to be effective. The Type 1 error is controlled by controlling the significance level, which is alpha. Alpha is the highest probability to have a Type 1 error. The lower the alpha, the lower will be the Type 1 error. The Type 2 error is the kind of error that occurs when we do not reject a null hypothesis that is false. This error is also called the error of the second kind and is equivalent to a false negative. This kind of error occurs in a drug scenario when the drug is assumed to be ineffective but is actually it is effective.
[ 43 ]

Inferential Statistics
These errors can be controlled one at a time. If one of the errors is lowered, then the other one increases. It depends on the use case and the problem statement that the analysis is trying to address, and depending on it, the appropriate error should reduce. In the case of this drug scenario, typically, a Type 1 error should be lowered because it is better to ship a drug that is confidently effective.
A confidence interval
A confidence interval is a type of interval statistics for a population parameter. The confidence interval helps in determining the interval at which the population mean can be defined.
Let's try to understand this concept by using an example. Let's take the height of every man in Kenya and determine with 95% confidence interval the average of height of Kenyan men at a national level. Let's take 50 men and their height in centimeters:
>>> height_data = np.array([ 186.0, 180.0, 195.0, 189.0, 191.0, 177.0, 161.0, 177.0, 192.0, 182.0, 185.0, 192.0,
173.0, 172.0, 191.0, 184.0, 193.0, 182.0, 190.0, 185.0, 181.0, 188.0, 179.0, 188.0,
170.0, 179.0, 180.0, 189.0, 188.0, 185.0, 170.0, 197.0, 187.0, 182.0, 173.0, 179.0,
184.0, 177.0, 190.0, 174.0, 203.0, 206.0, 173.0, 169.0, 178.0, 201.0, 198.0, 166.0,
171.0, 180.0])
[ 44 ]

Plotting the distribution, it has a normal distribution:
>>> plt.hist(height_data, 30, normed=True) >>> plt.show()

Chapter 2

The mean of the distribution is as follows:
>>> height_data.mean()

183.24000000000001
So, the average height of a man from the sample is 183.4 cm. To determine the confidence interval, we'll now define the standard error of the mean. The standard error of the mean is the deviation of the sample mean from the population mean. It is defined using the following formula:

SEx =

s n

Here, s is the standard deviation of the sample, and n is the number of elements of the sample.

[ 45 ]

Inferential Statistics
This can be calculated using the sem() function of the SciPy package:
>>> stats.sem(height_data) 1.3787187190005252
So, there is a standard error of the mean of 1.38 cm. The lower and upper limit of the confidence interval can be determined by using the following formula:
Upper/Lower limit = mean(height) + / - sigma * SEmean(x) For lower limit:
183.24 + (1.96 * 1.38) = 185.94 For upper limit:
183.24 - (1.96*1.38) = 180.53 A 1.96 standard deviation covers 95% of area in the normal distribution. We can confidently say that the population mean lies between 180.53 cm and 185.94 cm of height.
[ 46 ]

Chapter 2
Let's assume we take a sample of 50 people, record their height, and then repeat this process 30 times. We can then plot the averages of each sample and observe the distribution.
The commands that simulated the preceding plot is as follows:
>>> average_height = [] >>> for i in xrange(30): >>> sample50 = np.random.normal(183, 10, 50).round() >>> average_height.append(sample50.mean()) >>> plt.hist(average_height, 20, normed=True) >>> plt.show()
You can observe that the mean ranges from 180 to 187 cm when we simulated the average height of 50 sample men, which was taken 30 times. Let's see what happens when we sample 1000 men and repeat the process 30 times:
>>> average_height = [] >>> for i in xrange(30): >>> sample1000 = np.random.normal(183, 10, 1000).round()
[ 47 ]

Inferential Statistics >>> average_height.append(sample1000.mean())
>>> plt.hist(average_height, 10, normed=True) >>> plt.show()

As you can see, the height varies from 182.4 cm and to 183.4 cm. What does this mean?
It means that as the sample size increases, the standard error of the mean decreases, which also means that the confidence interval becomes narrower, and we can tell with certainty the interval that the population mean would lie on.

Correlation
In statistics, correlation defines the similarity between two random variables.
The most commonly used correlation is the Pearson correlation and it is defined
by the following:

ρ X ,Y

=

cov ( X ,Y )
σ XσY

=

E ( X

− µX )(Y
σ XσY

− µY

)

[ 48 ]

Chapter 2
The preceding formula defines the Pearson correlation as the covariance between X and Y, which is divided by the standard deviation of X and Y, or it can also be defined as the expected mean of the sum of multiplied difference of random variables with respect to the mean divided by the standard deviation of X and Y. Let's understand this with an example. Let's take the mileage and horsepower of various cars and see if there is a relation between the two. This can be achieved using the pearsonr function in the SciPy package:
>>> mpg = [21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8, 16.4, 17.3, 15.2, 10.4, 10.4, 14.7, 32.4, 30.4, 33.9, 21.5, 15.5, 15.2, 13.3, 19.2, 27.3, 26.0, 30.4, 15.8, 19.7, 15.0, 21.4]
>>> hp = [110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180, 205, 215, 230, 66, 52, 65, 97, 150, 150, 245, 175, 66, 91, 113, 264, 175, 335, 109]
>>> stats.pearsonr(mpg,hp)
(-0.77616837182658638, 1.7878352541210661e-07)
The first value of the output gives the correlation between the horsepower and the mileage and the second value gives the p-value. So, the first value tells us that it is highly negatively correlated and the p-value tells us that there is significant correlation between them:
>>> plt.scatter(mpg, hp) >>> plt.show()
[ 49 ]

Inferential Statistics
From the plot, we can see that as the mpg increases, the horsepower decreases. Let's look into another correlation called the Spearman correlation. The Spearman correlation applies to the rank order of the values and so it provides a monotonic relation between the two distributions. It is useful for ordinal data (data that has an order, such as movie ratings or grades in class) and is not affected by outliers. Let's get the Spearman correlation between the miles per gallon and horsepower. This can be achieved using the spearmanr() function in the SciPy package:
>>> stats.spearmanr(mpg,hp)
(-0.89466464574996252, 5.085969430924539e-12)
We can see that the Spearman correlation is -0.89 and the p-value is significant. Let's do an experiment in which we introduce a few outlier values in the data and see how the Pearson and Spearman correlation gets affected:
>>> mpg = [21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8, 16.4, 17.3, 15.2, 10.4, 10.4, 14.7, 32.4, 30.4, 33.9, 21.5, 15.5, 15.2, 13.3, 19.2, 27.3, 26.0, 30.4, 15.8, 19.7, 15.0, 21.4, 120, 3]
>>> hp = [110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180, 205, 215, 230, 66, 52, 65, 97, 150, 150, 245, 175, 66, 91, 113, 264, 175, 335, 109, 30, 600]
>>> plt.scatter(mpg, hp) >>> plt.show()
[ 50 ]

Chapter 2
From the plot, you can clearly make out the outlier values. Lets see how the correlations get affected for both the Pearson and Spearman correlation The following commands show you the Pearson correlation:
>>> stats.pearsonr(mpg, hp) >>> (-0.47415304891435484, 0.0046122167947348462)
Here is the Spearman correlation:
>>> stats.spearmanr(mpg, hp) >>> (-0.91222184337265655, 6.0551681657984803e-14)
We can clearly see that the Pearson correlation has been drastically affected due to the outliers, which are from a correlation of 0.89 to 0.47. The Spearman correlation did not get affected much as it is based on the order rather than the actual value in the data.
Z-test vs T-test
We have already done a few Z-tests before where we validated our null hypothesis.
A T-distribution is similar to a Z-distribution—it is centered at zero and has a basic bell shape, but its shorter and flatter around the center than the Z-distribution. The T-distributions' standard deviation is usually proportionally larger than the Z, because of which you see the fatter tails on each side.
[ 51 ]

Inferential Statistics
The t distribution is usually used to analyze the population when the sample is small. The Z-test is used to compare the population mean against a sample or compare the population mean of two distributions with a sample size greater than 30. An example of a Z-test would be comparing the heights of men from different ethnicity groups. The T-test is used to compare the population mean against a sample, or compare the population mean of two distributions with a sample size less than 30, and when you don't know the population's standard deviation. Let's do a T-test on two classes that are given a mathematics test and have 10 students in each class:
>>> class1_score = np.array([45.0, 40.0, 49.0, 52.0, 54.0, 64.0, 36.0, 41.0, 42.0, 34.0])
>>> class2_score = np.array([75.0, 85.0, 53.0, 70.0, 72.0, 93.0, 61.0, 65.0, 65.0, 72.0])
To perform the T-test, we can use the ttest_ind() function in the SciPy package:
>>> stats.ttest_ind(class1_score,class2_score)
(array(-5.458195056848407), 3.4820722850153292e-05)
The first value in the output is the calculated t-statistics, whereas the second value is the p-value and p-value shows that the two distributions are not identical.
The F distribution
The F distribution is also known as Snedecor's F distribution or the Fisher–Snedecor distribution. An f statistic is given by the following formula:
f = s12 / σ12  / s22 / σ22 
Here, s1 is the standard deviation of a sample 1 with an n1 size, s2 is the standard deviation of a sample 2, where the size n2σ1 is the population standard deviation of a sample 1σ2 is the population standard deviation of a sample 12.
[ 52 ]

Chapter 2
The distribution of all the possible values of f statistics is called F distribution. The d1 and d2 represent the degrees of freedom in the following chart:
The chi-square distribution
The chi-square statistics are defined by the following formula:
X2 = (n -1)*s2  /σ2
Here, n is the size of the sample, s is the standard deviation of the sample, and σ is the standard deviation of the population. If we repeatedly take samples and define the chi-square statistics, then we can form a chi-square distribution, which is defined by the following probability density function:
( ) Y=Y0 ∗ X2 ∗ e (v/2-1) −X2/2
Here, Y0 is a constant that depends on the number of degrees of freedom, Χ2 is the chi-square statistic, v = n - 1 is the number of degrees of freedom, and e is a constant equal to the base of the natural logarithm system.
[ 53 ]

Inferential Statistics
Y0 is defined so that the area under the chi-square curve is equal to one.

Chi-square for the goodness of fit
The Chi-square test can be used to test whether the observed data differs significantly from the expected data. Let's take the example of a dice. The dice is rolled 36 times and the probability that each face should turn upwards is 1/6. So, the expected distribution is as follows:

Expected Frequency 6 6 6 6 6 6

Outcome 1 2 3 4 5 6

>>> expected = np.array([6,6,6,6,6,6])

[ 54 ]

The observed distribution is as follows:

Chapter 2

Observed Frequency 7 5 3 9 6 6

Outcome 1 2 3 4 5 6

>>> observed = observed = np.array([7, 5, 3, 9, 6, 6])
The null hypothesis in the chi-square test is that the observed value is similar to the expected value.
The chi-square can be performed using the chisquare function in the SciPy package:
>>> stats.chisquare(observed,expected) (3.333333333333333, 0.64874235866759344)
The first value is the chi-square value and the second value is the p-value, which is very high. This means that the null hypothesis is valid and the observed value is similar to the expected value.

The chi-square test of independence
The chi-square test of independence is a statistical test used to determine whether two categorical variables are independent of each other or not.
Let's take the following example to see whether there is a preference for a book based on the gender of people reading it:

Total 280 640 920

Biography 60 90 150

Flavour Suspense 120 200 320

Romance 100 350 450

Gender Men Women

[ 55 ]

Inferential Statistics
The Chi-Square test of independence can be performed using the chi2_contingency function in the SciPy package:
>>> men_women = np.array([[100, 120, 60],[350, 200, 90]]) >>> stats.chi2_contingency(men_women) (28.362103174603167, 6.9382117170577439e-07, 2, array([[
136.95652174, 97.39130435, 45.65217391], [ 313.04347826, 222.60869565, 104.34782609]]))
The first value is the chi-square value:
The second value is the p-value, which is very small, and means that there is an association between the gender of people and the genre of the book they read. The third value is the degrees of freedom. The fourth value, which is an array, is the expected frequencies.
ANOVA
Analysis of Variance (ANOVA) is a statistical method used to test differences between two or more means.
This test basically compares the means between groups and determines whether any of these means are significantly different from each other:
H0 : µ1 = µ2 = µ3 = = µk
ANOVA is a test that can tell you which group is significantly different from each other. Let's take the height of men who are from three different countries and see if their heights are significantly different from others:
>>> country1 = np.array([ 176., 179., 180., 188., 187., 184., 171., 201., 172., 181., 192., 187., 178., 178., 180., 199., 185., 176., 207., 177., 160., 174., 176., 192., 189., 187., 183., 180., 181., 200., 190., 187., 175., 179., 181., 183., 171., 181., 190., 186., 185., 188., 201., 192., 188., 181., 172., 191., 201., 170., 170., 192., 185., 167., 178., 179., 167., 183., 200., 185.])
>>> country2 = np.array([ 177., 165., 175., 172., 179., 192., 169., 185., 187.,
[ 56 ]

Chapter 2

167., 197., 178., 166., 167., 171.,

162., 172., 191., 189., 184., 184.,

165., 175., 192., 196., 179., 156.,

188., 185., 175., 192., 178., 180.,

194., 176., 189., 189., 193., 181.,

187., 175., 171., 172., 178., 181., 171., 185., 179., 177., 187.])

163., 186., 170., 198., 181.,

178., 168., 182., 181., 174.,

>>> country3 = np.array([ 191., 190., 191., 185., 190., 184., 173., 175., 200., 190., 191., 184., 167., 194., 195., 174., 171., 191., 174., 177., 182., 184., 176., 180., 181., 186., 179., 176., 186., 176., 184., 194., 179., 171., 174., 174., 182., 198., 180., 178., 200., 200., 174., 202., 176., 180., 163., 159., 194., 192., 163., 194., 183., 190., 186., 178., 182., 174., 178., 182.])
To perform the one-way ANOVA, we can use the f_oneway() function of the SciPy package:
>>> stats.f_oneway(country1,country2,country3) (2.9852039682631375, 0.053079678812747652)
The first value of the output gives the F-value and the second value gives the p-value. Since the p-value is greater than 5% by a small margin, we can tell that the mean of the heights in the three countries is not significantly different from each other.
Summary
In this chapter, you learned about the various probability distributions. You also learned about how to use z-score, p-value, Type 1, and Type 2 errors. You gained an insight into the Z-test and T-test followed by the chi-square distribution and saw how it can be used to test a hypothesis.
In the next chapter, you'll learn about data mining and how to execute it.

[ 57 ]

Finding a Needle in a Haystack
Analyzing a dataset to find patterns is an art as much as it is a science. There can be a lot of metrics associated with a dataset and you would like to find the needle in this haystack. For us, a needle is the insight that we look for within data that we weren't aware of earlier. Here, insight could refer to important information about people who buy milk of a particular brand and also buy cereals of another brand, for instance. The retail store can then stack the products near each other. Whenever you try to analyze a dataset, you should have a detailed understanding of it and also of the domain that it is associated with. If it's a simple dataset that can be understood very easily, then the analysis can be performed directly, but if the dataset relates to the sensor data of a turbine, then domain understanding of how turbines work and what is critical to their functioning will add richness to your analysis. The understanding of a domain is like the North Star: it helps you navigate your analysis.
[ 59 ]

Finding a Needle in a Haystack
In this chapter, you'll learn the following topics: • How to structure your analysis for data mining • How to present your analysis • How to perform data mining on a Titanic survivors dataset
What is data mining?
Data mining is the process of exploring data and finding patterns in it using machine learning, statistics, and database systems. The end goal of data mining is to derive useful information from data, which can be utilized to increase revenue, reduce costs, or even save lives through some of its applications. When you have a dataset that needs to be mined, it is not feasible to use all the data-mining techniques that are available on every column field of the data to derive insights. This will be a cumbersome task and will take a long time to derive any useful insights. To speed up the process of mining data, knowledge of domains is a great help. With this knowledge, one can understand what the data represents and how to analyze it to gain insights.
[ 60 ]

Chapter 3
The best way to start data mining is to derive themes on which the data needs to be mined. If you have the sales data of a Fast Moving Consumer Goods (FMCG) company, then themes could be as follows:
• Brand behavior • Outlet behavior • Growth of products • Seasonal effect on products The themes help by giving a direction to explore data and finding patterns in it.
Once you have the themes, you need to put questions under them to streamline the analysis:
• Brand behavior: The following are the questions used to streamline the analysis: °° Which are the top brands? °° Which brands have the maximum coverage? °° Which brands are cannibalizing the sales of the other brands?
• Outlet behavior: The following are the questions used to streamline the analysis: °° What percentage of outlets takes up 80% of revenue? °° What kind of outlets have the highest number of sales? °° What kind of outlets sell primarily premium products?
• Growth of products: The following are the questions used to streamline the analysis: °° Which are the fastest growing brands in terms of sale? °° Which are the fastest growing brands in terms of volume? °° Which brand's growth has flattened out?
• Seasonal effect of the products: The following are the questions used to streamline the analysis: °° How many brands are seasonal? °° What is the difference in terms of sales during seasonal and nonseasonal periods? °° Which holiday brings in the maximum amount of sales for a particular brand?
[ 61 ]

Finding a Needle in a Haystack
The preceding questions under these themes give pinpointed directions to find patterns and perform an analysis that gives some quality results. The process of exploring data can be summarized by the following flow chart:
Presenting an analysis
After performing the analysis, you would need to present some observations. The most commonly used medium for doing this is through Microsoft PowerPoint presentations. The result of your analysis could be a construct in the form of a chart or table. When presenting these constructs, there is certain information that should be added to your slides. This is one of the most common templates used:
[ 62 ]

Chapter 3
Here are the different sections of the preceding image: • Question: The topmost part of the template should describe the problem statement that the particular analysis is trying to address. • Observation: Here, the observations from the construct are listed in a vertical column. Sometimes, the observations can be marked over the construct using arrow marks or dialog boxes. • Key Takeaway: Toward the bottom of the image, you can describe what is concluded from the chart.
[ 63 ]

Finding a Needle in a Haystack
Studying the Titanic
To perform the data analysis, we'll be using the Titanic dataset from Kaggle.
This dataset is simple to understand and does not require any domain understanding to derive insights.
This dataset contains the details of each passenger on the Titanic and also whether they survived or not.
The following are the field descriptions:

Field survival pclass name sex age sibsp parch ticket fare cabin embarked

Descriptions Survival(0 = No, 1 = Yes) Passenger class(1 = 1st, 2 = 2nd, 3 = 3rd) Name of the passenger Gender of the passenger Age of the passenger Number of siblings/spouses aboard Number of parents/children aboard Ticket number Passenger fare Cabin Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)

Since the data is quite simple to understand, we'll keep the survival analysis as the main theme that can be used for the analysis of the data. We'll attach questions to these themes.
These are the questions that we'll answer:
• Which passenger class has the maximum number of survivors? • What is the distribution, based on gender, of the survivors among the
different classes? • What is the distribution of the nonsurvivors among classes that have
relatives aboard the ship? • What is the survival percentage among different age groups?

[ 64 ]

Chapter 3
Which passenger class has the maximum number of survivors?
To answer this question, we'll construct a simple bar plot of the number of survivors and the percentage of survivors in each class, respectively. You can do this using the following command:
>>> import pandas as pd >>> import pylab as plt >>> import numpy as np
>>> df = pd.read_csv('Data/titanic data.csv')
>>> df['Pclass'].isnull().value_counts() >>> False 891 >>> dtype: int64
>>> df['Survived'].isnull().value_counts() >>> False 891 >>> dtype: int64
>>> #Passengers survived in each class >>> survivors = df.groupby('Pclass')['Survived'].agg(sum)
>>> #Total Passengers in each class >>> total_passengers = df.groupby('Pclass')['PassengerId'].count() >>> survivor_percentage = survivors / total_passengers
>>> #Plotting the Total number of survivors >>> fig = plt.figure() >>> ax = fig.add_subplot(111) >>> rect = ax.bar(survivors.index.values.tolist(),
survivors, color='blue', width=0.5) >>> ax.set_ylabel('No. of survivors') >>> ax.set_title('Total number of survivors based on class') >>> xTickMarks = survivors.index.values.tolist() >>> ax.set_xticks(survivors.index.values.tolist())
[ 65 ]

Finding a Needle in a Haystack >>> xtickNames = ax.set_xticklabels(xTickMarks) >>> plt.setp(xtickNames, fontsize=20) >>> plt.show()
>>> #Plotting the percentage of survivors in each class >>> fig = plt.figure() >>> ax = fig.add_subplot(111) >>> rect = ax.bar(survivor_percentage.index.values.tolist(),
survivor_percentage, color='blue', width=0.5) >>> ax.set_ylabel('Survivor Percentage') >>> ax.set_title('Percentage of survivors based on class') >>> xTickMarks = survivors.index.values.tolist() >>> ax.set_xticks(survivors.index.values.tolist()) >>> xtickNames = ax.set_xticklabels(xTickMarks) >>> plt.setp(xtickNames, fontsize=20) >>> plt.show()
[ 66 ]

Chapter 3
In the preceding code, we performed a preliminary check for null values on the fields that are utilized. After this, we calculated the number of survivors and the percentage of survivors in each class. Then, we plotted two bar charts for the total number of survivors and the percentage of survivors. These are our observations:
• The maximum number of survivors are in the first and third class, respectively • With respect to the total number of passengers in each class, first class has the
maximum survivors at around 61% • With respect to the total number of passengers in each class, third class has
the minimum number of survivors at around 25% This is our key takeaway:
• There was clearly a preference toward saving those from the first class as the ship was drowning. It also had the maximum percentage of survivors
[ 67 ]

Finding a Needle in a Haystack
What is the distribution of survivors based on gender among the various classes?
To answer this question, we'll use the following code to plot a side-by-side bar chart to compare the survival rate and percentage among men and women with respect to the class they were in.
>>> #Checking for any null values >>> df['Sex'].isnull().value_counts() >>> False 891 >>> dtype: int64
>>> # Male Passengers survived in each class >>> male_survivors = df[df['Sex'] == 'male']
.groupby('Pclass')['Survived'].agg(sum)
>>> #Total Male Passengers in each class >>> male_total_passengers = df[df['Sex'] == 'male']
.groupby('Pclass')['PassengerId'].count() >>> male_survivor_percentage = male_survivors / male_total_passengers
>>> # Female Passengers survived in each class >>> female_survivors = df[df['Sex'] == 'female']
.groupby('Pclass')['Survived'].agg(sum)
>>> #Total Female Passengers in each class >>> female_total_passengers = df[df['Sex'] == 'female']
.groupby('Pclass')['PassengerId'].count() >>> female_survivor_percentage = female_survivors /
female_total_passengers
>>> #Plotting the total passengers who survived based on Gender >>> fig = plt.figure() >>> ax = fig.add_subplot(111) >>> index = np.arange(male_survivors.count()) >>> bar_width = 0.35 >>> rect1 = ax.bar(index, male_survivors, bar_width, color='blue',
label='Men') >>> rect2 = ax.bar(index + bar_width, female_survivors, bar_width,
color='y', label='Women')
[ 68 ]

>>> ax.set_ylabel('Survivor Numbers') >>> ax.set_title('Male and Female survivors based on class') >>> xTickMarks = male_survivors.index.values.tolist() >>> ax.set_xticks(index + bar_width) >>> xtickNames = ax.set_xticklabels(xTickMarks) >>> plt.setp(xtickNames, fontsize=20) >>> plt.legend() >>> plt.tight_layout() >>> plt.show()

Chapter 3

>>> #Plotting the percentage of passengers who survived based on Gender >>> fig = plt.figure() >>> ax = fig.add_subplot(111) >>> index = np.arange(male_survivor_percentage.count()) >>> bar_width = 0.35 >>> rect1 = ax.bar(index, male_survivor_percentage, bar_width,
color='blue', label='Men') >>> rect2 = ax.bar(index + bar_width, female_survivor_percentage,
bar_width, color='y', label='Women') >>> ax.set_ylabel('Survivor Percentage')
[ 69 ]

Finding a Needle in a Haystack >>> ax.set_title('Percentage Male and Female of
survivors based on class') >>> xTickMarks = male_survivor_percentage.index.values.tolist() >>> ax.set_xticks(index + bar_width) >>> xtickNames = ax.set_xticklabels(xTickMarks) >>> plt.setp(xtickNames, fontsize=20) >>> plt.legend() >>> plt.tight_layout() >>> plt.show()
In the preceding code, the number of male and female survivors is calculated and then a side-by-side bar plot is plotted. After this, the percentage of male and female survivors with respect to the total number of males and females in their respective classes are taken and then plotted.
[ 70 ]

Chapter 3
These are our observations: • The majority of survivors are females in all the classes • More than 90% of female passengers in first and second class survived • The percentage of male passengers who survived in first and third class, respectively, are comparable
This is our key takeaway: • Female passengers were given preference for lifeboats and the majority were saved.
What is the distribution of nonsurvivors among the various classes who have family aboard the ship?
To answer this question, we'll use the following code to plot bar charts again using the total number of nonsurvivors in each class who each had family aboard, and the percentage with respect to the total number of passengers:
>>> #Checking for the null values >>> df['SibSp'].isnull().value_counts() >>> False 891 >>> dtype: int64
>>> #Checking for the null values >>> df['Parch'].isnull().value_counts() >>> False 891 >>> dtype: int64
>>> #Total number of non-survivors in each class >>> non_survivors = df[(df['SibSp'] > 0) | (df['Parch'] > 0) &
(df['Survived'] == 0)].groupby('Pclass')['Survived'].agg('count') >>> #Total passengers in each class >>> total_passengers = df.groupby('Pclass')['PassengerId'].count() >>> non_survivor_percentage = non_survivors / total_passengers >>> #Total number of non survivors with family based on class >>> fig = plt.figure()
[ 71 ]

Finding a Needle in a Haystack >>> ax = fig.add_subplot(111) >>> rect = ax.bar(non_survivors.index.values.tolist(), non_survivors,
color='blue', width=0.5) >>> ax.set_ylabel('No. of non survivors') >>> ax.set_title('Total number of non survivors with
family based on class') >>> xTickMarks = non_survivors.index.values.tolist() >>> ax.set_xticks(non_survivors.index.values.tolist()) >>> xtickNames = ax.set_xticklabels(xTickMarks) >>> plt.setp(xtickNames, fontsize=20) >>> plt.show()
>>> #Plot of percentage of non survivors with family based on class >>> fig = plt.figure() >>> ax = fig.add_subplot(111) >>> rect = ax.bar(non_survivor_percentage.index.values.tolist(),
non_survivor_percentage, color='blue', width=0.5) >>> ax.set_ylabel('Non Survivor Percentage')
[ 72 ]

>>> ax.set_title('Percentage of non survivors with family based on class')
>>> xTickMarks = non_survivor_percentage.index.values.tolist() >>> ax.set_xticks(non_survivor_percentage.index.values.tolist()) >>> xtickNames = ax.set_xticklabels(xTickMarks) >>> plt.setp(xtickNames, fontsize=20) >>> plt.show()

Chapter 3

The code here is pretty similar to the code used in the previous questions. Here, we can get the number of the nonsurvivors who have a family and then perform the usual bar plots.
These are our observations:
• There are lot of nonsurvivors in the third class • Second class has the least number of nonsurvivors with relatives • With respect to the total number of passengers, the first class, who had
relatives aboard, has the maximum nonsurvivor percentage and the third class has the least
[ 73 ]

Finding a Needle in a Haystack
This is our key takeaway:
• Even though third class has the highest number of nonsurvivors with relatives aboard, it primarily had passengers who did not have relatives on the ship, whereas in first class, most of the people had relatives aboard the ship

What was the survival percentage among different age groups?
For this question, we'll use the following code to plot pie charts to compare the proportion of survivors in terms of number and percentage with respect to the different age groups:

>>> #Checking for null values

>>> df['Age'].isnull().value_counts()

>>> False 714

>>> True

177

>>> dtype: int64

>>> #Defining the age binning interval >>> age_bin = [0, 18, 25, 40, 60, 100] >>> #Creating the bins >>> df['AgeBin'] = pd.cut(df.Age, bins=age_bin) >>> #Removing the null rows >>> d_temp = df[np.isfinite(df['Age'])] # removing all na instances >>> #Number of survivors based on Age bin >>> survivors = d_temp.groupby('AgeBin')['Survived'].agg(sum) >>> #Total passengers in each bin >>> total_passengers = d_temp.groupby('AgeBin')['Survived'].agg('count') >>> #Plotting the pie chart of total passengers in each bin >>> plt.pie(total_passengers,
labels=total_passengers.index.values.tolist(), autopct='%1.1f%%', shadow=True, startangle=90) >>> plt.title('Total Passengers in different age groups') >>> plt.show()

[ 74 ]

Chapter 3
>>> #Plotting the pie chart of percentage passengers in each bin >>> plt.pie(survivors, labels=survivors.index.values.tolist(),
autopct='%1.1f%%', shadow=True, startangle=90) >>> plt.title('Survivors in different age groups') >>> plt.show()
[ 75 ]

Finding a Needle in a Haystack
In the code, we defined the bin with the age_bin variable and then added a column called AgeBin, where bin values are filled using the cut function. After this, we filtered out all the rows with the age set as null. After this, we created two pie charts: one for the total number of passengers in each age group and another for the number of survivors in each age group.
These are our observations:
• The 25-40 age group has the maximum number of passengers, and 0-18 has the second highest number of passengers
• Among the people who survived, the 18-25 age group has the second highest number of survivors
• The 60-100 age group has a lower proportion among the survivors This is our key takeaway:
• The 25-40 age group had the maximum number of survivors compared to any other age group, and people who were old were either not lucky enough or made way for the younger people to the lifeboats.
Summary
In this chapter, we learned the meaning of data mining. We learned the importance of domain knowledge in performing analysis and how to perform data mining in a systematic manner. We also learned how to present the results of data mining. Toward the end, we took an example and performed a few analyses to extract useful information.
In the next chapter, you'll learn about how to create visualizations on data and where to apply different kinds of visualizations.
Downloading the example code You can download the example code files for all Packt books you have purchased from your account at http://www.packtpub.com. If you purchased this book elsewhere, you can visit http://www.packtpub. com/support and register to have the files e-mailed directly to you. The codes provided in the code bundle are for both IPython notebook and Python 2.7. In the chapters, Python conventions have been followed.
[ 76 ]

Making Sense of Data through Advanced Visualization
Visualization is a very integral part of data science. It helps in communicating a pattern or a relationship that cannot be seen by looking at raw data. It's easier for a person to remember a picture and recollect it as compared to lines of text. This holds true for data too. In this chapter, we'll cover the following topics:
• Controlling the properties of a plot • Combining multiple plots • Styling your plots • Creating various advanced visualizations
[ 77 ]

Making Sense of Data through Advanced Visualization
Controlling the line properties of a chart
There are many properties of a line that can be set, such as the color, dashes, and several others. There are essentially three ways of doing this. Let's take a simple line chart as an example:
>>> plt.plot([1,2,3,4], [1,4,9,16]) >>> plt.show()
After the preceding code is executed we'll get the following output:

Using keyword arguments
We can use arguments within the plot function to set the property of the line:
>>> import numpy as np >>> import pandas as pd >>> import matplotlib.pyplot as plt >>> import pandas.tools.rplot as rplot

>>> plt.plot([1, 2, 3, 4], [1, 4, 9, 16], linewidth=4.0) # the line width
>>> plt.show()

# increasing

[ 78 ]

After the preceding code is executed we'll get the following output:

Chapter 4

Using the setter methods
The plot function returns the list of line objects, for example line1, line2 = plot(x1,y1,x2,y2). Using the line setter method of line objects we can define the property that needs to be set:
>>> line, = plt.plot([1, 2, 3, 4], [1, 4, 9, 16]) >>> line.set_linestyle('--') # Setting dashed lines >>> plt.show()
After the preceding code is executed we'll get the following output:

[ 79 ]

