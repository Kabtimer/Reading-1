pandas
#pandas

Table des matières

À propos

1

Chapitre 1: Commencer avec les pandas

2

Remarques

2

Versions

2

Examples

3

Installation ou configuration

3

Installer via anaconda

5

Bonjour le monde

5

Statistiques descriptives

6

Chapitre 2: Ajout à DataFrame

8

Examples

8

Ajout d'une nouvelle ligne à DataFrame

8

Ajouter un DataFrame à un autre DataFrame

9

Chapitre 3: Analyse: tout rassembler et prendre des décisions

11

Examples

11

Analyse de quintile: avec des données aléatoires

11

Qu'est-ce qu'un facteur

11

Initialisation

11

pd.qcut - Create Quintile Buckets

12

Une analyse

12

Retours de parcelles

12

Visualiser la corrélation de quintile avec scatter_matrix

13

Calculer et visualiser Maximum Draw Down

14

Calculer des statistiques

16

Chapitre 4: Calendriers de vacances

18

Examples

18

Créer un calendrier personnalisé

18

Utiliser un calendrier personnalisé

18

Obtenez les vacances entre deux dates

18

Compter le nombre de jours ouvrables entre deux dates

19

Chapitre 5: Création de DataFrames

20

Introduction

20

Examples

20

Créer un exemple de DataFrame

20

Créer un exemple de DataFrame en utilisant Numpy

21

Créer un exemple de DataFrame à partir de plusieurs collections à l'aide d'un dictionnaire

22

Créer un DataFrame à partir d'une liste de tuples

22

Créer un DataFrame à partir d'un dictionnaire de listes

23

Créer un exemple de DataFrame avec datetime

23

Créer un exemple de DataFrame avec MultiIndex

25

Enregistrer et charger un DataFrame au format pickle (.plk)

26

Créer un DataFrame à partir d'une liste de dictionnaires

26

Chapitre 6: Données catégoriques

27

Introduction

27

Examples

27

Création d'objet

27

Création de jeux de données aléatoires volumineux

27

Chapitre 7: Données décalées et décalées

29

Examples

29

Décalage ou décalage de valeurs dans un dataframe

29

Chapitre 8: Données dupliquées

30

Examples

30

Sélectionnez dupliqué

30

Drop dupliqué

30

Compter et obtenir des éléments uniques

31

Obtenez des valeurs uniques dans une colonne.

32

Chapitre 9: Données manquantes

34

Remarques

34

Examples

34

Remplir les valeurs manquantes

34

Remplir les valeurs manquantes avec une seule valeur:

34

Remplissez les valeurs manquantes avec les précédentes:

34

Remplissez avec les suivants:

34

Remplir à l'aide d'un autre DataFrame:

35

Supprimer les valeurs manquantes

35

Supprimer des lignes si au moins une colonne a une valeur manquante

35

Supprimer des lignes si toutes les valeurs de cette ligne sont manquantes

36

Supprimez les colonnes qui n'ont pas au moins 3 valeurs non manquantes

36

Interpolation

36

Vérification des valeurs manquantes

36

Chapitre 10: Enregistrer les données pandas dans un fichier csv

38

Paramètres

38

Examples

39

Créez un DataFrame aléatoire et écrivez dans .csv

39

Enregistrer Pandas DataFrame de la liste aux dicts à csv sans index et avec encodage des d

40

Chapitre 11: Faire jouer les Pandas avec les types de données Python natifs

42

Examples

42

Déplacement de données hors de pandas vers des structures de données natives Python et Num

42

Chapitre 12: Fusionner, rejoindre et concaténer

44

Syntaxe

44

Paramètres

44

Examples

45

Fusionner

45

Fusion de deux DataFrames

46

Jointure interne:

46

Jointure externe:

47

Joint gauche:

47

Droit rejoindre

47

Fusion / concaténation / jonction de plusieurs blocs de données (horizontalement et vertic

48

Fusionner, rejoindre et concat

49

Quelle est la différence entre rejoindre et fusionner

50

Chapitre 13: Gotchas de pandas

52

Remarques

52

Examples

52

Détecter les valeurs manquantes avec np.nan

52

Entier et NA

52

Alignement automatique des données (comportement indexé)

53

Chapitre 14: Graphes et Visualisations

54

Examples

54

Graphiques de données de base

54

Styling l'intrigue

56

Tracer sur un axe matplotlib existant

56

Chapitre 15: Indexation booléenne des dataframes

57

Introduction

57

Examples

57

Accéder à un DataFrame avec un index booléen

57

Application d'un masque booléen à un dataframe

58

Masquage des données en fonction de la valeur de la colonne

58

Masquage des données en fonction de la valeur d'index

59

Chapitre 16: Indexation et sélection de données

60

Examples

60

Sélectionnez colonne par étiquette

60

Sélectionner par position

60

Trancher avec des étiquettes

61

Sélection mixte et sélection basée sur une étiquette

62

Indexation booléenne

63

Filtrage des colonnes (en sélectionnant "intéressant", en supprimant des données inutiles,

64

générer un échantillon DF

64

affiche les colonnes contenant la lettre 'a'

64

affiche les colonnes à l'aide du filtre RegEx (b|c|d) - b ou c ou d :

64

afficher toutes les colonnes sauf celles commençant par a (en d'autres termes, supprimer / 65

Filtrage / sélection de lignes en utilisant la méthode `.query ()`

65

générer des DF aléatoires

65

sélectionnez les lignes où les valeurs de la colonne A > 2 et les valeurs de la colonne B

65

utiliser la méthode .query() avec des variables pour le filtrage

66

Tranchage dépendant du chemin

66

Récupère les premières / dernières n lignes d'un dataframe

68

Sélectionnez des lignes distinctes sur l'ensemble des données

69

Filtrer les lignes avec les données manquantes (NaN, None, NaT)

70

Chapitre 17: IO pour Google BigQuery

72

Examples

72

Lecture des données de BigQuery avec les informations d'identification du compte utilisate

72

Lecture des données de BigQuery avec les informations d'identification du compte de servic

73

Chapitre 18: JSON

74

Examples

74

Lire JSON

74

peut soit transmettre une chaîne de json, soit un chemin de fichier à un fichier avec json

74

Dataframe dans JSON imbriqué comme dans les fichiers flare.js utilisés dans D3.js

74

Lire JSON à partir du fichier

75

Chapitre 19: Lecture de fichiers dans des pandas DataFrame

76

Examples

76

Lire la table dans DataFrame

76

Fichier de table avec en-tête, pied de page, noms de ligne et colonne d'index:

76

Fichier de table sans noms de lignes ou index:

76

Lire un fichier CSV

77

Données avec en-tête, séparées par des points-virgules au lieu de virgules

77

Table sans noms de lignes ou index et virgules comme séparateurs

77

Recueillez les données de la feuille de calcul google dans les données pandas

78

Chapitre 20: Lire MySQL sur DataFrame

79

Examples

79

Utiliser sqlalchemy et PyMySQL

79

Pour lire mysql sur dataframe, en cas de grande quantité de données

79

Chapitre 21: Lire SQL Server vers Dataframe

80

Examples

80

Utiliser pyodbc

80

Utiliser pyodbc avec boucle de connexion

80

Chapitre 22: Manipulation de cordes

82

Examples

82

Expressions régulières

82

Ficelle

82

Vérification du contenu d'une chaîne

84

Capitalisation de chaînes

84

Chapitre 23: Manipulation simple de DataFrames

87

Examples

87

Supprimer une colonne dans un DataFrame

87

Renommer une colonne

88

Ajouter une nouvelle colonne

89

Directement attribuer

89

Ajouter une colonne constante

89

Colonne comme expression dans les autres colonnes

89

Créez-le à la volée

90

ajouter plusieurs colonnes

90

ajouter plusieurs colonnes à la volée

90

Localiser et remplacer des données dans une colonne

91

Ajout d'une nouvelle ligne à DataFrame

91

Supprimer / supprimer des lignes de DataFrame

92

Réorganiser les colonnes

93

Chapitre 24: Meta: Guide de documentation

94

Remarques

94

Examples

94

Affichage des extraits de code et sortie

94

style

95

Prise en charge de la version Pandas

95

imprimer des relevés

95

Préférez le support de python 2 et 3:

95

Chapitre 25: MultiIndex

96

Examples

96

Sélectionnez MultiIndex par niveau

96

Itérer sur DataFrame avec MultiIndex

97

Définition et tri d'un MultiIndex

98

Comment changer les colonnes MultiIndex en colonnes standard

100

Comment changer les colonnes standard en MultiIndex

100

Colonnes MultiIndex

100

Afficher tous les éléments de l'index

101

Chapitre 26: Obtenir des informations sur les DataFrames

102

Examples

102

Obtenir des informations DataFrame et l'utilisation de la mémoire

102

Liste des noms de colonnes DataFrame

102

Les différentes statistiques du Dataframe.

103

Chapitre 27: Outils de calcul

104

Examples

104

Rechercher la corrélation entre les colonnes

104

Chapitre 28: Outils Pandas IO (lecture et sauvegarde de fichiers)

105

Remarques

105

Examples

105

Lecture du fichier csv dans DataFrame

105

Fichier:

105

Code:

105

Sortie:

105

Quelques arguments utiles:

105

Enregistrement de base dans un fichier csv

107

Analyse des dates lors de la lecture de csv

107

Feuille de calcul à dictée de DataFrames

107

Lire une fiche spécifique

107

Test read_csv

107

Compréhension de la liste

108

Lire en morceaux

109

Enregistrer dans un fichier CSV

109

Analyse des colonnes de date avec read_csv

110

Lire et fusionner plusieurs fichiers CSV (avec la même structure) en un seul fichier DF

110

Lire le fichier cvs dans un bloc de données pandas lorsqu'il n'y a pas de ligne d'en-tête

111

Utiliser HDFStore

111

générer un échantillon DF avec différents types de dtypes

111

faire un plus grand DF (10 * 100.000 = 1.000.000 lignes)

112

créer (ou ouvrir un fichier HDFStore existant)

112

enregistrer notre bloc de données dans le fichier h5 (HDFStore), en indexant les colonnes 112

afficher les détails du HDFStore

112

afficher les colonnes indexées

112

close (flush to disk) notre fichier de magasin

113

Lire le journal d'accès Nginx (plusieurs guillemets)

113

Chapitre 29: Pandas Datareader

114

Remarques

114

Examples

114

Exemple de base de Datareader (Yahoo Finance)

114

Lecture de données financières (pour plusieurs tickers) dans un panel de pandas - démo

115

Chapitre 30: pd.DataFrame.apply

117

Examples

117

pandas.DataFrame.apply Utilisation de base

117

Chapitre 31: Rééchantillonnage

119

Examples

119

Sous-échantillonnage et suréchantillonnage

119

Chapitre 32: Regroupement des données

121

Examples

121

Groupement de base

121

Grouper par une colonne

121

Grouper par plusieurs colonnes

121

Regroupement des numéros

122

Sélection de colonne d'un groupe

123

Agrégation par taille par rapport au nombre

124

Groupes d'agrégation

124

Exporter des groupes dans des fichiers différents

125

utiliser transformation pour obtenir des statistiques au niveau du groupe tout en préserva

125

Chapitre 33: Regroupement des données de séries chronologiques

127

Examples

127

Générer des séries chronologiques de nombres aléatoires puis d'échantillon inférieur

127

Chapitre 34: Remodelage et pivotement

129

Examples

129

Pivotement simple

129

Pivoter avec agréger

130

Empilage et dépilage

133

Tabulation croisée

134

Les pandas fondent pour passer du long au long

136

Fractionner (remodeler) les chaînes CSV dans des colonnes en plusieurs lignes, avec un élé

137

Chapitre 35: Sections transversales de différents axes avec MultiIndex

139

Examples

139

Sélection de sections en utilisant .xs

139

Utilisation de .loc et de trancheurs

140

Chapitre 36: Séries

142

Examples

142

Exemples de création de séries simples

142

Série avec datetime

142

Quelques astuces sur Series in Pandas

143

Application d'une fonction à une série

145

Chapitre 37: Traiter les variables catégorielles

147

Examples

147

Codage à chaud avec `get_dummies ()`

147

Chapitre 38: Travailler avec des séries chronologiques

148

Examples

148

Création de séries chronologiques

148

Indexation partielle des chaînes

148

Obtenir des données

148

Sous-location

148

Chapitre 39: Types de données

150

Remarques

150

Examples

151

Vérification des types de colonnes

151

Changer de type

151

Changer le type en numérique

152

Changer le type en datetime

153

Changer le type en timedelta

153

Sélection de colonnes basées sur dtype

153

Résumé des types

154

Chapitre 40: Utiliser .ix, .iloc, .loc, .at et .iat pour accéder à un DataFrame

155

Examples

155

Utiliser .iloc

155

Utiliser .loc

156

Chapitre 41: Valeurs de la carte

158

Remarques

158

Examples

158

Carte du dictionnaire

158

Crédits

159

À propos
You can share this PDF with anyone you feel could benefit from it, downloaded the latest version from: pandas
It is an unofficial and free pandas ebook created for educational purposes. All the content is extracted from Stack Overflow Documentation, which is written by many hardworking individuals at Stack Overflow. It is neither affiliated with Stack Overflow nor official pandas.
The content is released under Creative Commons BY-SA, and the list of contributors to each chapter are provided in the credits section at the end of this book. Images may be copyright of their respective owners unless otherwise specified. All trademarks and registered trademarks are the property of their respective company owners.
Use the content presented in this book at your own risk; it is not guaranteed to be correct nor accurate, please send your feedback and corrections to info@zzzprojects.com

https://riptutorial.com/fr/home

1

Chapitre 1: Commencer avec les pandas

Remarques

Pandas est un package Python fournissant des structures de données rapides, flexibles et expressives conçues pour rendre le travail avec des données «relationnelles» ou «étiquetées» à la fois simples et intuitives. Il se veut être le composant fondamental de haut niveau pour effectuer des analyses de données pratiques dans le monde réel en Python.
La documentation officielle Pandas peut être trouvée ici .

Versions

Les pandas

Version Date de sortie 0.19.1 2016-11-03 0.19.0 2016-10-02 0.18.1 2016-05-03 0.18.0 2016-03-13 0,17,1 2015-11-21 0,17,0 2015-10-09 0,16,2 2015-06-12 0.16.1 2015-05-11 0,16,0 2015-03-22 0.15.2 2014-12-12 0.15.1 2014-11-09 0,15,0 2014-10-18 0.14.1 2014-07-11 0,14,0 2014-05-31 0.13.1 2014-02-03 0.13.0 2014-01-03

https://riptutorial.com/fr/home

2

Version Date de sortie 0.12.0 2013-07-23

Examples
Installation ou configuration
Des instructions détaillées sur la mise en place ou l'installation de pandas sont disponibles dans la documentation officielle .
Installer des pandas avec Anaconda
Installer des pandas et le reste de la pile NumPy et SciPy peut être un peu difficile pour les utilisateurs inexpérimentés.
La manière la plus simple d’installer non seulement les pandas, mais aussi Python et les paquets les plus populaires constituant la pile SciPy (IPython, NumPy, Matplotlib, ...) est avec Anaconda , une plateforme multi-plateforme (Linux, Mac OS X, Windows) Distribution Python pour l'analyse de données et le calcul scientifique.
Après avoir exécuté un programme d'installation simple, l'utilisateur aura accès aux pandas et au reste de la pile SciPy sans avoir besoin d'installer autre chose, et sans avoir à attendre que des logiciels soient compilés.
Vous trouverez les instructions d'installation d'Anaconda ici .
Une liste complète des paquets disponibles dans le cadre de la distribution d'Anaconda peut être trouvée ici .
Un avantage supplémentaire de l'installation avec Anaconda est que vous n'avez pas besoin des droits d'administrateur pour l'installer, il sera installé dans le répertoire personnel de l'utilisateur, et cela rend également inutile de supprimer Anaconda ultérieurement (il suffit de supprimer ce dossier).
Installer des pandas avec Miniconda
La section précédente décrivait comment installer les pandas dans le cadre de la distribution Anaconda. Cependant, cette approche signifie que vous allez installer plus de cent paquets et que vous devrez télécharger le programme d’installation de quelques centaines de mégaoctets.
Si vous voulez avoir plus de contrôle sur les paquets, ou si vous avez une bande passante Internet limitée, installer des pandas avec Miniconda peut être une meilleure solution.
Conda est le gestionnaire de paquetages sur lequel la distribution Anaconda est construite. C'est un gestionnaire de paquets à la fois multi-plateforme et indépendant du langage (il peut jouer un rôle similaire à une combinaison pip et virtualenv).
Miniconda vous permet de créer une installation Python autonome minimale, puis d'utiliser la

https://riptutorial.com/fr/home

3

commande Conda pour installer des packages supplémentaires. Tout d'abord, vous aurez besoin de Conda pour être installé et télécharger et exécuter le Miniconda le fera pour vous. L'installateur peut être trouvé ici . L'étape suivante consiste à créer un nouvel environnement de conda (ceux-ci sont analogues à virtualenv, mais ils vous permettent également de spécifier précisément la version de Python à installer). Exécutez les commandes suivantes à partir d'une fenêtre de terminal:
conda create -n name_of_my_env python
Cela créera un environnement minimal avec seulement Python installé. Pour vous mettre dans cet environnement, exécutez:
source activate name_of_my_env
Sous Windows, la commande est la suivante:
activate name_of_my_env
La dernière étape requise consiste à installer des pandas. Cela peut être fait avec la commande suivante:
conda install pandas
Pour installer une version de pandas spécifique:
conda install pandas=0.13.1
Pour installer d'autres packages, IPython par exemple:
conda install ipython
Pour installer la distribution complète d'Anaconda:
conda install anaconda
Si vous avez besoin de paquets disponibles pour pip mais pas conda, installez simplement pip et utilisez pip pour installer ces paquets:
conda install pip pip install django
Généralement, vous installez des pandas avec l'un des gestionnaires de paquets. exemple pip:

https://riptutorial.com/fr/home

4

pip install pandas
Cela nécessitera probablement l'installation d'un certain nombre de dépendances, y compris NumPy, nécessitera un compilateur pour compiler les bits de code requis, et cela peut prendre quelques minutes.
Installer via anaconda
Commencez par télécharger anaconda du site Continuum. Soit via l'installateur graphique (Windows / OSX) ou en exécutant un script shell (OSX / Linux). Cela comprend les pandas!
Si vous ne voulez pas que les 150 paquets soient regroupés dans anaconda, vous pouvez installer miniconda . Soit via l'installateur graphique (Windows) ou le script shell (OSX / Linux). Installez les pandas sur miniconda en utilisant:
conda install pandas
Pour mettre à jour les pandas à la dernière version en utilisation anaconda ou miniconda:
conda update pandas
Bonjour le monde
Une fois Pandas installé, vous pouvez vérifier s'il fonctionne correctement en créant un ensemble de données réparties de manière aléatoire et en traçant son histogramme.
import pandas as pd # This is always assumed but is included here as an introduction. import numpy as np import matplotlib.pyplot as plt np.random.seed(0) values = np.random.randn(100) # array of normally distributed random numbers s = pd.Series(values) # generate a pandas series s.plot(kind='hist', title='Normally distributed random values') # hist computes distribution plt.show()

https://riptutorial.com/fr/home

5

Vérifiez certaines des statistiques des données (moyenne, écart-type, etc.)

s.describe()

# Output: count 100.000000

# mean

0.059808

# std

1.012960

# min

-2.552990

# 25%

-0.643857

# 50%

0.094096

# 75%

0.737077

# max

2.269755

# dtype: float64

Statistiques descriptives
Les statistiques descriptives (moyenne, écart-type, nombre d’observations, minimum, maximum et quartiles) des colonnes numériques peuvent être calculées à l’aide de la méthode .describe() , qui renvoie un ensemble de données descriptives de pandas.

In [1]: df = pd.DataFrame({'A': [1, 2, 1, 4, 3, 5, 2, 3, 4, 1], 'B': [12, 14, 11, 16, 18, 18, 22, 13, 21, 17], 'C': ['a', 'a', 'b', 'a', 'b', 'c', 'b', 'a', 'b', 'a']})
In [2]: df Out[2]:
A BC 0 1 12 a

https://riptutorial.com/fr/home

6

1 2 14 a 2 1 11 b 3 4 16 a 4 3 18 b 5 5 18 c 6 2 22 b 7 3 13 a 8 4 21 b 9 1 17 a

In [3]: df.describe()

Out[3]:

A

B

count 10.000000 10.000000

mean 2.600000 16.200000

std

1.429841 3.705851

min

1.000000 11.000000

25%

1.250000 13.250000

50%

2.500000 16.500000

75%

3.750000 18.000000

max

5.000000 22.000000

Notez que puisque C n'est pas une colonne numérique, il est exclu de la sortie.

In [4]: df['C'].describe()

Out[4]:

count

10

unique

3

freq

5

Name: C, dtype: object

Dans ce cas, la méthode résume les données catégorielles par le nombre d'observations, le nombre d'éléments uniques, le mode et la fréquence du mode.
Lire Commencer avec les pandas en ligne: https://riptutorial.com/fr/pandas/topic/796/commenceravec-les-pandas

https://riptutorial.com/fr/home

7

Chapitre 2: Ajout à DataFrame
Examples
Ajout d'une nouvelle ligne à DataFrame
In [1]: import pandas as pd
In [2]: df = pd.DataFrame(columns = ['A', 'B', 'C'])
In [3]: df Out[3]: Empty DataFrame Columns: [A, B, C] Index: []
Ajout d'une ligne par une valeur de colonne unique:
In [4]: df.loc[0, 'A'] = 1
In [5]: df Out[5]:
ABC 0 1 NaN NaN
Ajout d'une ligne, liste de valeurs donnée:
In [6]: df.loc[1] = [2, 3, 4]
In [7]: df Out[7]:
ABC 0 1 NaN NaN 12 3 4
Ajouter une ligne à un dictionnaire:
In [8]: df.loc[2] = {'A': 3, 'C': 9, 'B': 9}
In [9]: df Out[9]:
ABC 0 1 NaN NaN 12 3 4 23 9 9
La première entrée de .loc [] est l'index. Si vous utilisez un index existant, vous écraserez les valeurs de cette ligne:
In [17]: df.loc[1] = [5, 6, 7]

https://riptutorial.com/fr/home

8

In [18]: df Out[18]:
ABC 0 1 NaN NaN 15 6 7 23 9 9
In [19]: df.loc[0, 'B'] = 8
In [20]: df Out[20]:
AB C 0 1 8 NaN 156 7 239 9
Ajouter un DataFrame à un autre DataFrame
Supposons que nous ayons les deux DataFrames suivants:
In [7]: df1 Out[7]:
AB 0 a1 b1 1 a2 b2
In [8]: df2 Out[8]:
BC 0 b1 c1
Les deux DataFrames ne doivent pas nécessairement avoir le même ensemble de colonnes. La méthode append ne modifie aucun des DataFrames d'origine. Au lieu de cela, il renvoie un nouveau DataFrame en ajoutant les deux originaux. Ajouter un DataFrame à un autre est assez simple:
In [9]: df1.append(df2) Out[9]:
AB C 0 a1 b1 NaN 1 a2 b2 NaN 0 NaN b1 c1
Comme vous pouvez le voir, il est possible d'avoir des index en double (0 dans cet exemple). Pour éviter ce problème, vous pouvez demander à Pandas de réindexer le nouveau DataFrame pour vous:
In [10]: df1.append(df2, ignore_index = True) Out[10]:
AB C 0 a1 b1 NaN 1 a2 b2 NaN 2 NaN b1 c1

https://riptutorial.com/fr/home

9

Lire Ajout à DataFrame en ligne: https://riptutorial.com/fr/pandas/topic/6456/ajout-a-dataframe

https://riptutorial.com/fr/home

10

Chapitre 3: Analyse: tout rassembler et prendre des décisions
Examples
Analyse de quintile: avec des données aléatoires
L'analyse de quintile est un cadre commun pour évaluer l'efficacité des facteurs de sécurité.
Qu'est-ce qu'un facteur
Un facteur est une méthode de notation / classement des ensembles de titres. Pour un moment donné et pour un ensemble particulier de titres, un facteur peut être représenté comme une série de pandas dans laquelle l’index est un tableau des identificateurs de sécurité et les valeurs sont les scores ou les rangs.
Si nous prenons des scores factoriels au fil du temps, nous pouvons, à chaque instant, diviser l'ensemble des titres en 5 compartiments ou quintiles égaux, en fonction de l'ordre des scores factoriels. Il n'y a rien de particulièrement sacré dans le nombre 5. Nous aurions pu utiliser 3 ou 10. Mais nous utilisons 5 souvent. Enfin, nous suivons la performance de chacun des cinq compartiments pour déterminer s’il ya une différence significative dans les rendements. Nous avons tendance à nous concentrer davantage sur la différence de rendement du seau avec le rang le plus élevé par rapport à celui du rang le plus bas.
Commençons par définir certains paramètres et générer des données aléatoires.
Pour faciliter l'expérimentation avec la mécanique, nous fournissons un code simple pour créer des données aléatoires afin de nous donner une idée de son fonctionnement.
Les données aléatoires incluent
• Retours : générer des retours aléatoires pour un nombre spécifié de titres et de périodes. • Signaux : génèrent des signaux aléatoires pour un nombre spécifié de titres et de périodes
et avec le niveau de corrélation prescrit avec les retours . Pour qu'un facteur soit utile, il doit y avoir des informations ou une corrélation entre les scores / rangs et les retours ultérieurs. S'il n'y avait pas de corrélation, nous le verrions. Ce serait un bon exercice pour le lecteur, dupliquer cette analyse avec des données aléatoires générées avec 0 corrélation.
Initialisation
import pandas as pd import numpy as np

https://riptutorial.com/fr/home

11

num_securities = 1000 num_periods = 1000 period_frequency = 'W' start_date = '2000-12-31'
np.random.seed([3,1415])
means = [0, 0] covariance = [[ 1., 5e-3],
[5e-3, 1.]]
# generates to sets of data m[0] and m[1] with ~0.005 correlation m = np.random.multivariate_normal(means, covariance,
(num_periods, num_securities)).T
Générons maintenant un index de séries temporelles et un index représentant les identifiants de sécurité. Ensuite, utilisez-les pour créer des diagrammes de données pour les retours et les signaux
ids = pd.Index(['s{:05d}'.format(s) for s in range(num_securities)], 'ID') tidx = pd.date_range(start=start_date, periods=num_periods, freq=period_frequency)
Je divise m[0] par 25 pour réduire à quelque chose qui ressemble à des rendements boursiers. J'ajoute également 1e-7 pour donner un rendement moyen positif modeste.
security_returns = pd.DataFrame(m[0] / 25 + 1e-7, tidx, ids) security_signals = pd.DataFrame(m[1], tidx, ids)
pd.qcut - Create Quintile Buckets
Utilisons pd.qcut pour diviser mes signaux en pd.qcut quintiles pour chaque période.
def qcut(s, q=5): labels = ['q{}'.format(i) for i in range(1, 6)] return pd.qcut(s, q, labels=labels)
cut = security_signals.stack().groupby(level=0).apply(qcut)
Utilisez ces coupes comme index sur nos retours
returns_cut = security_returns.stack().rename('returns') \ .to_frame().set_index(cut, append=True) \ .swaplevel(2, 1).sort_index().squeeze() \ .groupby(level=[0, 1]).mean().unstack()
Une analyse
Retours de parcelles

https://riptutorial.com/fr/home

12

import matplotlib.pyplot as plt
fig = plt.figure(figsize=(15, 5)) ax1 = plt.subplot2grid((1,3), (0,0)) ax2 = plt.subplot2grid((1,3), (0,1)) ax3 = plt.subplot2grid((1,3), (0,2))
# Cumulative Returns returns_cut.add(1).cumprod() \
.plot(colormap='jet', ax=ax1, title="Cumulative Returns") leg1 = ax1.legend(loc='upper left', ncol=2, prop={'size': 10}, fancybox=True) leg1.get_frame().set_alpha(.8)
# Rolling 50 Week Return returns_cut.add(1).rolling(50).apply(lambda x: x.prod()) \
.plot(colormap='jet', ax=ax2, title="Rolling 50 Week Return") leg2 = ax2.legend(loc='upper left', ncol=2, prop={'size': 10}, fancybox=True) leg2.get_frame().set_alpha(.8)
# Return Distribution returns_cut.plot.box(vert=False, ax=ax3, title="Return Distribution")
fig.autofmt_xdate()
plt.show()

Visualiser la corrélation de quintile avec scatter_matrix
from pandas.tools.plotting import scatter_matrix scatter_matrix(returns_cut, alpha=0.5, figsize=(8, 8), diagonal='hist') plt.show()

https://riptutorial.com/fr/home

13

Calculer et visualiser Maximum Draw Down
def max_dd(returns): """returns is a series""" r = returns.add(1).cumprod() dd = r.div(r.cummax()).sub(1) mdd = dd.min() end = dd.argmin() start = r.loc[:end].argmax() return mdd, start, end
def max_dd_df(returns): """returns is a dataframe""" series = lambda x: pd.Series(x, ['Draw Down', 'Start', 'End']) return returns.apply(max_dd).apply(series)
A quoi ça ressemble
max_dd_df(returns_cut)

https://riptutorial.com/fr/home

14

Plaquons-le
draw_downs = max_dd_df(returns_cut)
fig, axes = plt.subplots(5, 1, figsize=(10, 8)) for i, ax in enumerate(axes[::-1]):
returns_cut.iloc[:, i].add(1).cumprod().plot(ax=ax) sd, ed = draw_downs[['Start', 'End']].iloc[i] ax.axvspan(sd, ed, alpha=0.1, color='r') ax.set_ylabel(returns_cut.columns[i])
fig.suptitle('Maximum Draw Down', fontsize=18) fig.tight_layout() plt.subplots_adjust(top=.95)

https://riptutorial.com/fr/home

15

Calculer des statistiques
Il existe de nombreuses statistiques potentielles que nous pouvons inclure. En voici quelques-uns, mais montrez comment nous pouvons simplement intégrer de nouvelles statistiques dans notre résumé.
def frequency_of_time_series(df): start, end = df.index.min(), df.index.max() delta = end - start return round((len(df) - 1.) * 365.25 / delta.days, 2)
def annualized_return(df): freq = frequency_of_time_series(df) return df.add(1).prod() ** (1 / freq) - 1
def annualized_volatility(df): freq = frequency_of_time_series(df) return df.std().mul(freq ** .5)
def sharpe_ratio(df): return annualized_return(df) / annualized_volatility(df)

https://riptutorial.com/fr/home

16

def describe(df): r = annualized_return(df).rename('Return') v = annualized_volatility(df).rename('Volatility') s = sharpe_ratio(df).rename('Sharpe') skew = df.skew().rename('Skew') kurt = df.kurt().rename('Kurtosis') desc = df.describe().T
return pd.concat([r, v, s, skew, kurt, desc], axis=1).T.drop('count')
Nous finirons par utiliser uniquement la fonction de describe car elle rassemble tous les autres.
describe(returns_cut)

Ceci n'est pas censé être complet. Il est destiné à rassembler de nombreuses fonctionnalités des pandas et à démontrer comment vous pouvez les utiliser pour répondre à des questions importantes pour vous. C'est un sous-ensemble des types de paramètres que j'utilise pour évaluer l'efficacité des facteurs quantitatifs.
Lire Analyse: tout rassembler et prendre des décisions en ligne: https://riptutorial.com/fr/pandas/topic/5238/analyse--tout-rassembler-et-prendre-des-decisions

https://riptutorial.com/fr/home

17

Chapitre 4: Calendriers de vacances
Examples
Créer un calendrier personnalisé
Voici comment créer un calendrier personnalisé. L'exemple donné est un calendrier français - il fournit donc de nombreux exemples.
from pandas.tseries.holiday import AbstractHolidayCalendar, Holiday, EasterMonday, Easter from pandas.tseries.offsets import Day, CustomBusinessDay
class FrBusinessCalendar(AbstractHolidayCalendar): """ Custom Holiday calendar for France based on https://en.wikipedia.org/wiki/Public_holidays_in_France - 1 January: New Year's Day - Moveable: Easter Monday (Monday after Easter Sunday) - 1 May: Labour Day - 8 May: Victory in Europe Day - Moveable Ascension Day (Thursday, 39 days after Easter Sunday) - 14 July: Bastille Day - 15 August: Assumption of Mary to Heaven - 1 November: All Saints' Day - 11 November: Armistice Day - 25 December: Christmas Day """ rules = [ Holiday('New Years Day', month=1, day=1), EasterMonday, Holiday('Labour Day', month=5, day=1), Holiday('Victory in Europe Day', month=5, day=8), Holiday('Ascension Day', month=1, day=1, offset=[Easter(), Day(39)]), Holiday('Bastille Day', month=7, day=14), Holiday('Assumption of Mary to Heaven', month=8, day=15), Holiday('All Saints Day', month=11, day=1), Holiday('Armistice Day', month=11, day=11), Holiday('Christmas Day', month=12, day=25) ]
Utiliser un calendrier personnalisé
Voici comment utiliser le calendrier personnalisé.
Obtenez les vacances entre deux dates

import pandas as pd from datetime import date
# Creating some boundaries year = 2016 start = date(year, 1, 1)

https://riptutorial.com/fr/home

18

end = start + pd.offsets.MonthEnd(12)

# Creating a custom calendar cal = FrBusinessCalendar() # Getting the holidays (off-days) between two dates cal.holidays(start=start, end=end)

# DatetimeIndex(['2016-01-01', '2016-03-28', '2016-05-01', '2016-05-05',

#

'2016-05-08', '2016-07-14', '2016-08-15', '2016-11-01',

#

'2016-11-11', '2016-12-25'],

#

dtype='datetime64[ns]', freq=None)

Compter le nombre de jours ouvrables entre deux dates
Il est parfois utile d’obtenir le nombre de jours de travail par mois, quelle que soit l’année ou le passé. Voici comment procéder avec un calendrier personnalisé.
from pandas.tseries.offsets import CDay
# Creating a series of dates between the boundaries # by using the custom calendar se = pd.bdate_range(start=start,
end=end, freq=CDay(calendar=cal)).to_series() # Counting the number of working days by month se.groupby(se.dt.month).count().head()
# 1 20 # 2 21 # 3 22 # 4 21 # 5 21
Lire Calendriers de vacances en ligne: https://riptutorial.com/fr/pandas/topic/7976/calendriers-devacances

https://riptutorial.com/fr/home

19

Chapitre 5: Création de DataFrames

Introduction

DataFrame est une structure de données fournie par la bibliothèque pandas, à l'exception de Series & Panel . C'est une structure à deux dimensions et peut être comparée à une table de lignes et de colonnes.
Chaque ligne peut être identifiée par un index entier (0..N) ou une étiquette explicitement définie lors de la création d'un objet DataFrame. Chaque colonne peut être de type distinct et identifiée par une étiquette.
Cette rubrique couvre différentes façons de créer / créer un objet DataFrame. Ex. des tableaux Numpy, de la liste des tuples, du dictionnaire.
Examples

Créer un exemple de DataFrame

import pandas as pd

Créez un DataFrame à partir d'un dictionnaire, contenant deux colonnes: des numbers et des colors . Chaque clé représente un nom de colonne et la valeur est une série de données, le contenu de la colonne:

df = pd.DataFrame({'numbers': [1, 2, 3], 'colors': ['red', 'white', 'blue']})

Afficher le contenu du dataframe:

print(df) # Output: # colors # 0 red # 1 white # 2 blue

numbers 1 2 3

Commandes de Pandas colonnes par ordre alphabétique comme dict ne sont pas ordonnés. Pour spécifier la commande, utilisez le paramètre columns .

df = pd.DataFrame({'numbers': [1, 2, 3], 'colors': ['red', 'white', 'blue']}, columns=['numbers', 'colors'])

print(df)

# Output:

# numbers colors

#0

1 red

#1

2 white

https://riptutorial.com/fr/home

20

#2

3 blue

Créer un exemple de DataFrame en utilisant Numpy

Créez un DataFrame de nombres aléatoires:

import numpy as np import pandas as pd

# Set the seed for a reproducible sample np.random.seed(0)

df = pd.DataFrame(np.random.randn(5, 3), columns=list('ABC'))

print(df)

# Output:

#

A

B

C

# 0 1.764052 0.400157 0.978738

# 1 2.240893 1.867558 -0.977278

# 2 0.950088 -0.151357 -0.103219

# 3 0.410599 0.144044 1.454274

# 4 0.761038 0.121675 0.443863

Créez un DataFrame avec des entiers:

df = pd.DataFrame(np.arange(15).reshape(5,3),columns=list('ABC'))

print(df)

# Output:

#

ABC

#0 0 1 2

#1 3 4 5

#2 6 7 8

# 3 9 10 11

# 4 12 13 14

Créez un DataFrame et incluez les nans ( NaT, NaN, 'nan', None ) entre les colonnes et les lignes:

df = pd.DataFrame(np.arange(48).reshape(8,6),columns=list('ABCDEF'))

print(df)

# Output:

#

ABCDEF

#0 0 1 2 3 4 5

# 1 6 7 8 9 10 11

# 2 12 13 14 15 16 17

# 3 18 19 20 21 22 23

# 4 24 25 26 27 28 29

# 5 30 31 32 33 34 35

# 6 36 37 38 39 40 41

# 7 42 43 44 45 46 47

df.ix[::2,0] = np.nan # in column 0, set elements with indices 0,2,4, ... to NaN

df.ix[::4,1] = pd.NaT # in column 1, set elements with indices 0,4, ... to np.NaT

df.ix[:3,2] = 'nan' # in column 2, set elements with index from 0 to 3 to 'nan'

df.ix[:,5] = None

# in column 5, set all elements to None

https://riptutorial.com/fr/home

21

df.ix[5,:] = None

# in row 5, set all elements to None

df.ix[7,:] = np.nan # in row 7, set all elements to NaN

print(df)

# Output:

#

A

B

# 0 NaN NaT

#1 6

7

# 2 NaN 13

# 3 18 19

# 4 NaN NaT

# 5 NaN None

# 6 NaN 37

# 7 NaN NaN

CDE nan 3 4 nan 9 10 nan 15 16 nan 21 22
26 27 28 None NaN NaN
38 39 40 NaN NaN NaN

F None None None None None None None
NaN

Créer un exemple de DataFrame à partir de plusieurs collections à l'aide d'un dictionnaire

import pandas as pd import numpy as np
np.random.seed(123) x = np.random.standard_normal(4) y = range(4) df = pd.DataFrame({'X':x, 'Y':y}) >>> df
XY 0 -1.085631 0 1 0.997345 1 2 0.282978 2 3 -1.506295 3

Créer un DataFrame à partir d'une liste de tuples
Vous pouvez créer un DataFrame à partir d'une liste de tuples simples, et même choisir les éléments spécifiques des tuples à utiliser. Ici, nous allons créer un DataFrame en utilisant toutes les données de chaque tuple, à l'exception du dernier élément.

import pandas as pd

data = [ ('p1', 't1', 1, 2), ('p1', 't2', 3, 4), ('p2', 't1', 5, 6), ('p2', 't2', 7, 8), ('p2', 't3', 2, 8) ]

df = pd.DataFrame(data)

print(df)

#

0 123

# 0 p1 t1 1 2

# 1 p1 t2 3 4

# 2 p2 t1 5 6

# 3 p2 t2 7 8

https://riptutorial.com/fr/home

22

# 4 p2 t3 2 8

Créer un DataFrame à partir d'un dictionnaire de listes
Créez un DataFrame à partir de plusieurs listes en transmettant un dict dont les listes de valeurs. Les clés du dictionnaire sont utilisées comme étiquettes de colonne. Les listes peuvent aussi être des ndarrays. Les listes / ndarrays doivent tous avoir la même longueur.

import pandas as pd

# Create DF from dict of lists/ndarrays

df = pd.DataFrame({'A' : [1, 2, 3, 4],

'B' : [4, 3, 2, 1]})

df

# Output:

#

AB

# 014

# 123

# 232

# 341

Si les tableaux n'ont pas la même longueur, une erreur est générée

df = pd.DataFrame({'A' : [1, 2, 3, 4], 'B' : [5, 5, 5]}) # a ValueError is raised

Utiliser ndarrays

import pandas as pd import numpy as np

np.random.seed(123)

x = np.random.standard_normal(4)

y = range(4)

df = pd.DataFrame({'X':x, 'Y':y})

df

# Output:

XY

#

0 -1.085631 0

#

1 0.997345 1

#

2 0.282978 2

#

3 -1.506295 3

Voir les détails supplémentaires sur: http://pandas.pydata.org/pandasdocs/stable/dsintro.html#from-dict-of-ndarrays-lists
Créer un exemple de DataFrame avec datetime

import pandas as pd import numpy as np
np.random.seed(0) # create an array of 5 dates starting at '2015-02-24', one per minute rng = pd.date_range('2015-02-24', periods=5, freq='T') df = pd.DataFrame({ 'Date': rng, 'Val': np.random.randn(len(rng)) })

https://riptutorial.com/fr/home

23

print (df)

# Output:

#

Date

# 0 2015-02-24 00:00:00

# 1 2015-02-24 00:01:00

# 2 2015-02-24 00:02:00

# 3 2015-02-24 00:03:00

# 4 2015-02-24 00:04:00

Val 1.764052 0.400157 0.978738 2.240893 1.867558

# create an array of 5 dates starting at '2015-02-24', one per day rng = pd.date_range('2015-02-24', periods=5, freq='D') df = pd.DataFrame({ 'Date': rng, 'Val' : np.random.randn(len(rng))})

print (df)

# Output:

#

Date

Val

# 0 2015-02-24 -0.977278

# 1 2015-02-25 0.950088

# 2 2015-02-26 -0.151357

# 3 2015-02-27 -0.103219

# 4 2015-02-28 0.410599

# create an array of 5 dates starting at '2015-02-24', one every 3 years rng = pd.date_range('2015-02-24', periods=5, freq='3A') df = pd.DataFrame({ 'Date': rng, 'Val' : np.random.randn(len(rng))})

print (df)

# Output:

#

Date

# 0 2015-12-31

# 1 2018-12-31

# 2 2021-12-31

# 3 2024-12-31

# 4 2027-12-31

Val 0.144044 1.454274 0.761038 0.121675 0.443863

DataFrame avec DatetimeIndex :

import pandas as pd import numpy as np

np.random.seed(0) rng = pd.date_range('2015-02-24', periods=5, freq='T') df = pd.DataFrame({ 'Val' : np.random.randn(len(rng)) }, index=rng)

print (df) # Output: # # 2015-02-24 00:00:00 # 2015-02-24 00:01:00 # 2015-02-24 00:02:00 # 2015-02-24 00:03:00 # 2015-02-24 00:04:00

Val 1.764052 0.400157 0.978738 2.240893 1.867558

Offset-aliases pour le paramètre freq dans date_range :

Alias B C

Description business day frequency custom business day frequency (experimental)

https://riptutorial.com/fr/home

24

D W M BM CBM MS BMS CBMS Q BQ QS BQS A BA AS BAS BH H T, min S L, ms U, us N

calendar day frequency weekly frequency month end frequency business month end frequency custom business month end frequency month start frequency business month start frequency custom business month start frequency quarter end frequency business quarter endfrequency quarter start frequency business quarter start frequency year end frequency business year end frequency year start frequency business year start frequency business hour frequency hourly frequency minutely frequency secondly frequency milliseconds microseconds nanoseconds

Créer un exemple de DataFrame avec MultiIndex

import pandas as pd import numpy as np

Utiliser from_tuples :

np.random.seed(0) tuples = list(zip(*[['bar', 'bar', 'baz', 'baz',
'foo', 'foo', 'qux', 'qux'], ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]))
idx = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])

Utiliser from_product :

idx = pd.MultiIndex.from_product([['bar', 'baz', 'foo', 'qux'],['one','two']])

Ensuite, utilisez ce MultiIndex:

df = pd.DataFrame(np.random.randn(8, 2), index=idx, columns=['A', 'B'])

print (df)

A

B

first second

bar one

1.764052 0.400157

two

0.978738 2.240893

baz one

1.867558 -0.977278

two

0.950088 -0.151357

foo one -0.103219 0.410599

two

0.144044 1.454274

https://riptutorial.com/fr/home

25

qux one two

0.761038 0.121675 0.443863 0.333674

Enregistrer et charger un DataFrame au format pickle (.plk)

import pandas as pd
# Save dataframe to pickled pandas object df.to_pickle(file_name) # where to save it usually as a .plk
# Load dataframe from pickled pandas object df= pd.read_pickle(file_name)

Créer un DataFrame à partir d'une liste de dictionnaires
Un DataFrame peut être créé à partir d'une liste de dictionnaires. Les clés sont utilisées comme noms de colonne.

import pandas as pd

L = [{'Name': 'John', 'Last Name': 'Smith'},

{'Name': 'Mary', 'Last Name': 'Wood'}]

pd.DataFrame(L)

# Output: Last Name Name

#0

Smith John

#1

Wood Mary

Les valeurs manquantes sont remplies avec NaN s

L = [{'Name': 'John', 'Last Name': 'Smith', 'Age': 37},

{'Name': 'Mary', 'Last Name': 'Wood'}]

pd.DataFrame(L)

# Output:

Age Last Name Name

#

0 37

Smith John

#

1 NaN

Wood Mary

Lire Création de DataFrames en ligne: https://riptutorial.com/fr/pandas/topic/1595/creation-dedataframes

https://riptutorial.com/fr/home

26

Chapitre 6: Données catégoriques

Introduction
Les catégories sont un type de données pandas, qui correspond à des variables catégorielles dans les statistiques: une variable qui ne peut prendre qu’un nombre limité et généralement fixe de valeurs possibles (catégories; niveaux dans R). Les exemples sont le sexe, la classe sociale, les groupes sanguins, les affiliations par pays, le temps d'observation ou les évaluations via les échelles de Likert. Source: Pandas Docs
Examples
Création d'objet

In [188]: s = pd.Series(["a","b","c","a","c"], dtype="category")

In [189]: s Out[189]: 0a 1b 2c 3a 4c dtype: category Categories (3, object): [a, b, c]

In [190]: df = pd.DataFrame({"A":["a","b","c","a", "c"]})

In [191]: df["B"] = df["A"].astype('category')

In [192]: df["C"] = pd.Categorical(df["A"])

In [193]: df Out[193]:
ABC 0aaa 1bbb 2ccc 3aaa 4ccc

In [194]: df.dtypes

Out[194]:

A

object

B category

C category

dtype: object

Création de jeux de données aléatoires volumineux

In [1]: import pandas as pd import numpy as np

https://riptutorial.com/fr/home

27

In [2]: df = pd.DataFrame(np.random.choice(['foo','bar','baz'], size=(100000,3))) df = df.apply(lambda col: col.astype('category'))
In [3]: df.head() Out[3]:
012 0 bar foo baz 1 baz bar baz 2 foo foo bar 3 bar baz baz 4 foo bar baz
In [4]: df.dtypes Out[4]: 0 category 1 category 2 category dtype: object
In [5]: df.shape Out[5]: (100000, 3)
Lire Données catégoriques en ligne: https://riptutorial.com/fr/pandas/topic/3887/donneescategoriques

https://riptutorial.com/fr/home

28

Chapitre 7: Données décalées et décalées

Examples

Décalage ou décalage de valeurs dans un dataframe

import pandas as pd

df = pd.DataFrame({'eggs': [1,2,4,8,], 'chickens': [0,1,2,4,]})

df

# chickens eggs

#0

0

1

#1

1

2

#2

2

4

#3

4

8

df.shift()

# chickens eggs

#0

NaN NaN

#1

0.0 1.0

#2

1.0 2.0

#3

2.0 4.0

df.shift(-2)

# chickens eggs

#0

2.0 4.0

#1

4.0 8.0

#2

NaN NaN

#3

NaN NaN

df['eggs'].shift(1) - df['chickens']

# 0 NaN # 1 0.0 # 2 0.0 # 3 0.0

Le premier argument de .shift() est des periods , le nombre d'espaces pour déplacer les données. Si non spécifié, la valeur par défaut est 1 .
Lire Données décalées et décalées en ligne: https://riptutorial.com/fr/pandas/topic/7554/donneesdecalees-et-decalees

https://riptutorial.com/fr/home

29

Chapitre 8: Données dupliquées

Examples

Sélectionnez dupliqué
Si nécessaire, définissez la valeur 0 sur la colonne B , où dans la colonne A les données dupliquées créent d'abord le masque par Series.duplicated , puis utilisent DataFrame.ix ou Series.mask :

In [224]: df = pd.DataFrame({'A':[1,2,3,3,2],

...:

'B':[1,7,3,0,8]})

In [225]: mask = df.A.duplicated(keep=False)

In [226]: mask

Out[226]:

0 False

1

True

2

True

3

True

4

True

Name: A, dtype: bool

In [227]: df.ix[mask, 'B'] = 0

In [228]: df['C'] = df.A.mask(mask, 0)

In [229]: df Out[229]:
ABC 0111 1200 2300 3300 4200

Si besoin inverser masque utiliser ~ :

In [230]: df['C'] = df.A.mask(~mask, 0)
In [231]: df Out[231]:
ABC 0110 1202 2303 3303 4202

Drop dupliqué
Utilisez drop_duplicates :

https://riptutorial.com/fr/home

30

In [216]: df = pd.DataFrame({'A':[1,2,3,3,2],

...:

'B':[1,7,3,0,8]})

In [217]: df Out[217]:
AB 011 127 233 330 428

# keep only the last value In [218]: df.drop_duplicates(subset=['A'], keep='last') Out[218]:
AB 011 330 428

# keep only the first value, default value In [219]: df.drop_duplicates(subset=['A'], keep='first') Out[219]:
AB 011 127 233

# drop all duplicated values In [220]: df.drop_duplicates(subset=['A'], keep=False) Out[220]:
AB 011

Lorsque vous ne souhaitez pas obtenir une copie d'un bloc de données, mais modifier celle existante:

In [221]: df = pd.DataFrame({'A':[1,2,3,3,2],

...:

'B':[1,7,3,0,8]})

In [222]: df.drop_duplicates(subset=['A'], inplace=True)

In [223]: df Out[223]:
AB 011 127 233

Compter et obtenir des éléments uniques
Nombre d'éléments uniques dans une série:
In [1]: id_numbers = pd.Series([111, 112, 112, 114, 115, 118, 114, 118, 112]) In [2]: id_numbers.nunique() Out[2]: 5

https://riptutorial.com/fr/home

31

Obtenez des éléments uniques dans une série:

In [3]: id_numbers.unique() Out[3]: array([111, 112, 114, 115, 118], dtype=int64)

In [4]: df = pd.DataFrame({'Group': list('ABAABABAAB'), 'ID': [1, 1, 2, 3, 3, 2, 1, 2, 1, 3]})

In [5]: df

Out[5]:

Group ID

0

A1

1

B1

2

A2

3

A3

4

B3

5

A2

6

B1

7

A2

8

A1

9

B3

Nombre d'éléments uniques dans chaque groupe:

In [6]: df.groupby('Group')['ID'].nunique() Out[6]: Group A3 B2 Name: ID, dtype: int64

Obtenez des éléments uniques dans chaque groupe:

In [7]: df.groupby('Group')['ID'].unique()

Out[7]:

Group

A [1, 2, 3]

B

[1, 3]

Name: ID, dtype: object

Obtenez des valeurs uniques dans une colonne.

In [15]: df = pd.DataFrame({"A":[1,1,2,3,1,1],"B":[5,4,3,4,6,7]})
In [21]: df Out[21]:
AB 015 114 223 334 416 517
Pour obtenir des valeurs uniques dans les colonnes A et B.

https://riptutorial.com/fr/home

32

In [22]: df["A"].unique() Out[22]: array([1, 2, 3])
In [23]: df["B"].unique() Out[23]: array([5, 4, 3, 6, 7])
Pour obtenir les valeurs uniques de la colonne A en tant que liste (notez que unique() peut être utilisé de deux manières légèrement différentes)
In [24]: pd.unique(df['A']).tolist() Out[24]: [1, 2, 3]
Voici un exemple plus complexe. Disons que nous voulons trouver les valeurs uniques de la colonne 'B' où 'A' est égal à 1.
D'abord, introduisons un doublon pour voir comment cela fonctionne. Remplaçons le 6 dans la ligne '4', la colonne 'B' avec un 4:
In [24]: df.loc['4', 'B'] = 4 Out[24]:
AB 015 114 223 334 414 517
Sélectionnez maintenant les données:
In [25]: pd.unique(df[df['A'] == 1 ]['B']).tolist() Out[25]: [5, 4, 7]
Cela peut être décomposé en pensant au DataFrame interne en premier:
df['A'] == 1
Cela trouve des valeurs dans la colonne A qui sont égales à 1 et leur applique True ou False. Nous pouvons alors l'utiliser pour sélectionner les valeurs de la colonne 'B' du DataFrame (la sélection externe DataFrame)
Pour comparaison, voici la liste si nous n'utilisons pas unique. Il récupère chaque valeur dans la colonne 'B' où la colonne 'A' est 1
In [26]: df[df['A'] == 1]['B'].tolist() Out[26]: [5, 4, 4, 7]
Lire Données dupliquées en ligne: https://riptutorial.com/fr/pandas/topic/2082/donnees-dupliquees

https://riptutorial.com/fr/home

33

Chapitre 9: Données manquantes

Remarques
Devrions-nous inclure le ffill et le bfill non documentés?
Examples
Remplir les valeurs manquantes

In [11]: df = pd.DataFrame([[1, 2, None, 3], [4, None, 5, 6], [7, 8, 9, 10], [None, None, None, None]])

Out[11]:

012

3

0 1.0 2.0 NaN 3.0

1 4.0 NaN 5.0 6.0

2 7.0 8.0 9.0 10.0

3 NaN NaN NaN NaN

Remplir les valeurs manquantes avec une seule valeur:

In [12]: df.fillna(0)

Out[12]:

012

3

0 1.0 2.0 0.0 3.0

1 4.0 0.0 5.0 6.0

2 7.0 8.0 9.0 10.0

3 0.0 0.0 0.0 0.0

Cela retourne un nouveau DataFrame. Si vous souhaitez modifier le DataFrame d'origine, utilisez le paramètre inplace ( df.fillna(0, inplace=True) ) ou attribuez-le au fichier de données d'origine ( df = df.fillna(0) ).

Remplissez les valeurs manquantes avec les précédentes:

In [13]: df.fillna(method='pad')

Out[13]:

012

3

0 1.0 2.0 NaN 3.0

1 4.0 2.0 5.0 6.0

2 7.0 8.0 9.0 10.0

3 7.0 8.0 9.0 10.0

# this is equivalent to both method='ffill' and .ffill()

Remplissez avec les suivants:

https://riptutorial.com/fr/home

34

In [14]: df.fillna(method='bfill')

Out[14]:

012

3

0 1.0 2.0 5.0 3.0

1 4.0 8.0 5.0 6.0

2 7.0 8.0 9.0 10.0

3 NaN NaN NaN NaN

# this is equivalent to .bfill()

Remplir à l'aide d'un autre DataFrame:

In [15]: df2 = pd.DataFrame(np.arange(100, 116).reshape(4, 4)) df2
Out[15]: 0123
0 100 101 102 103 1 104 105 106 107 2 108 109 110 111 3 112 113 114 115

In [16]: df.fillna(df2) # takes the corresponding cells in df2 to fill df

Out[16]:

0

1

2

3

0 1.0 2.0 102.0 3.0

1 4.0 105.0 5.0 6.0

2 7.0 8.0 9.0 10.0

3 112.0 113.0 114.0 115.0

Supprimer les valeurs manquantes
Lors de la création d'un DataFrame None (valeur manquante de python) est converti en NaN (valeur manquante des pandas):

In [11]: df = pd.DataFrame([[1, 2, None, 3], [4, None, 5, 6], [7, 8, 9, 10], [None, None, None, None]])

Out[11]:

012

3

0 1.0 2.0 NaN 3.0

1 4.0 NaN 5.0 6.0

2 7.0 8.0 9.0 10.0

3 NaN NaN NaN NaN

Supprimer des lignes si au moins une colonne a une valeur manquante

In [12]: df.dropna()

Out[12]:

012

3

2 7.0 8.0 9.0 10.0

Cela retourne un nouveau DataFrame. Si vous souhaitez modifier le DataFrame d'origine, utilisez le paramètre inplace ( df.dropna(inplace=True) ) ou attribuez-le à DataFrame d'origine ( df =

https://riptutorial.com/fr/home

35

df.dropna() ).
Supprimer des lignes si toutes les valeurs de cette ligne sont manquantes

In [13]: df.dropna(how='all')

Out[13]:

012

3

0 1.0 2.0 NaN 3.0

1 4.0 NaN 5.0 6.0

2 7.0 8.0 9.0 10.0

Supprimez les colonnes qui n'ont pas au moins 3 valeurs non manquantes

In [14]: df.dropna(axis=1, thresh=3)

Out[14]:

0

3

0 1.0 3.0

1 4.0 6.0

2 7.0 10.0

3 NaN NaN

Interpolation

import pandas as pd import numpy as np

df = pd.DataFrame({'A':[1,2,np.nan,3,np.nan], 'B':[1.2,7,3,0,8]})

df['C'] = df.A.interpolate() df['D'] = df.A.interpolate(method='spline', order=1)

print (df)

ABC

D

0 1.0 1.2 1.0 1.000000

1 2.0 7.0 2.0 2.000000

2 NaN 3.0 2.5 2.428571

3 3.0 0.0 3.0 3.000000

4 NaN 8.0 3.0 3.714286

Vérification des valeurs manquantes
Afin de vérifier si une valeur est NaN, les fonctions isnull() ou notnull() peuvent être utilisées.
In [1]: import numpy as np In [2]: import pandas as pd In [3]: ser = pd.Series([1, 2, np.nan, 4]) In [4]: pd.isnull(ser) Out[4]:

https://riptutorial.com/fr/home

36

0 False

1 False

2

True

3 False

dtype: bool

Notez que np.nan == np.nan renvoie False, vous devez donc éviter la comparaison avec np.nan:

In [5]: ser == np.nan Out[5]: 0 False 1 False 2 False 3 False dtype: bool

Les deux fonctions sont également définies en tant que méthodes sur les séries et les DataFrames.

In [6]: ser.isnull()

Out[6]:

0 False

1 False

2

True

3 False

dtype: bool

Test sur des DataFrames:

In [7]: df = pd.DataFrame({'A': [1, np.nan, 3], 'B': [np.nan, 5, 6]}) In [8]: print(df) Out[8]:
AB 0 1.0 NaN 1 NaN 5.0 2 3.0 6.0

In [9]: df.isnull()

Out[9]:

A

B

0 False True

1 True False

2 False False

# If the value is NaN, returns True.

In [10]: df.notnull()

Out[10]:

A

B

0 True False

1 False True

2 True True

# Opposite of .isnull(). If the value is not NaN, returns True.

Lire Données manquantes en ligne: https://riptutorial.com/fr/pandas/topic/1896/donneesmanquantes

https://riptutorial.com/fr/home

37

Chapitre 10: Enregistrer les données pandas dans un fichier csv

Paramètres

Paramètre

La description

path_or_buf

handle de chaîne ou de fichier, par défaut Aucun Chemin ou objet de fichier, si aucun est fourni, le résultat est renvoyé sous forme de chaîne.

sep

character, default ',' Délimiteur de champ pour le fichier de sortie.

na_rep

string, default '' Représentation des données manquantes

float_format string, default None Chaîne de format pour les nombres à virgule flottante

colonnes

séquence, colonnes facultatives à écrire

entête

booléen ou liste de chaîne, par défaut True Ecrivez les noms de colonne. Si une liste de chaîne est donnée, elle est supposée être un alias pour les noms de colonne

indice

booléen, par défaut True Write noms de lignes (index)

index_label

string ou sequence, ou False, default Aucun Etiquette de colonne pour les colonnes d'index si vous le souhaitez. Si None est donné et que header et index sont True, les noms d'index sont utilisés. Une séquence doit être donnée si le DataFrame utilise MultiIndex. Si False, n'imprimez pas de champs pour les noms d'index. Utilisez index_label = False pour importer plus facilement dans R

nanRep

Aucun déprécié, utilisez na_rep

mode

str Mode d'écriture Python, par défaut 'w'

codage

string, optionnel Chaîne représentant l'encodage à utiliser dans le fichier de sortie, par défaut, 'ascii' sur Python 2 et 'utf-8' sur Python 3.

compression

string, facultatif une chaîne représentant la compression à utiliser dans le fichier de sortie, les valeurs autorisées sont 'gzip', 'bz2', 'xz', utilisé uniquement lorsque le premier argument est un nom de fichier

string, default 'n' Caractère de nouvelle ligne ou séquence de caractères à line_terminator utiliser dans le fichier de sortie

en citant

constante optionnelle du module csv par défaut à csv.QUOTE_MINIMAL

https://riptutorial.com/fr/home

38

Paramètre

La description

quotechar

string (length 1), caractère par défaut '"' utilisé pour citer les champs

double citation booléen, par défaut True Control citant quotechar dans un champ

escapechar

chaîne (longueur 1), par défaut Aucun caractère utilisé pour échapper à sépare et quotechar, le cas échéant

taille

lignes int ou None à écrire à la fois

tupleize_cols booléen, par défaut False écrit des colonnes multi_index comme liste de tuples (si True) ou nouveau (format développé) si False)

format de date string, default None Chaîne de formatage pour les objets datetime

décimal

string, default '.' Caractère reconnu comme séparateur décimal. Par exemple, utiliser "," pour les données européennes

Examples

Créez un DataFrame aléatoire et écrivez dans .csv
Créez un simple DataFrame.

import numpy as np import pandas as pd

# Set the seed so that the numbers can be reproduced. np.random.seed(0)

df = pd.DataFrame(np.random.randn(5, 3), columns=list('ABC'))

# Another way to set column names is "columns=['column_1_name','column_2_name','column_3_name']"

df

A

B

C

0 1.764052 0.400157 0.978738

1 2.240893 1.867558 -0.977278

2 0.950088 -0.151357 -0.103219

3 0.410599 0.144044 1.454274

4 0.761038 0.121675 0.443863

Maintenant, écrivez dans un fichier CSV:

df.to_csv('example.csv', index=False)

Contenu de exemple.csv:

A,B,C

https://riptutorial.com/fr/home

39

1.76405234597,0.400157208367,0.978737984106 2.2408931992,1.86755799015,-0.977277879876 0.950088417526,-0.151357208298,-0.103218851794 0.410598501938,0.144043571161,1.45427350696 0.761037725147,0.121675016493,0.443863232745

Notez que nous spécifions index=False pour que les index générés automatiquement (n ° de ligne 0,1,2,3,4) ne soient pas inclus dans le fichier CSV. Incluez-le si vous avez besoin de la colonne d'index, comme ceci:

df.to_csv('example.csv', index=True) # Or just leave off the index param; default is True

Contenu de exemple.csv:

,A,B,C 0,1.76405234597,0.400157208367,0.978737984106 1,2.2408931992,1.86755799015,-0.977277879876 2,0.950088417526,-0.151357208298,-0.103218851794 3,0.410598501938,0.144043571161,1.45427350696 4,0.761037725147,0.121675016493,0.443863232745

Notez également que vous pouvez supprimer l'en-tête s'il n'est pas nécessaire avec header=False . C'est la sortie la plus simple:

df.to_csv('example.csv', index=False, header=False)

Contenu de exemple.csv:

1.76405234597,0.400157208367,0.978737984106 2.2408931992,1.86755799015,-0.977277879876 0.950088417526,-0.151357208298,-0.103218851794 0.410598501938,0.144043571161,1.45427350696 0.761037725147,0.121675016493,0.443863232745

Le séparateur peut être défini par sep= argument, bien que le séparateur standard pour les fichiers csv soit ',' .
df.to_csv('example.csv', index=False, header=False, sep='\t')

1.76405234597 2.2408931992 0.950088417526 0.410598501938 0.761037725147

0.400157208367 0.978737984106 1.86755799015 -0.977277879876
-0.151357208298 -0.103218851794 0.144043571161 1.45427350696 0.121675016493 0.443863232745

Enregistrer Pandas DataFrame de la liste aux dicts à csv sans index et avec encodage des données

import pandas as pd data = [
{'name': 'Daniel', 'country': 'Uganda'},

https://riptutorial.com/fr/home

40

{'name': 'Yao', 'country': 'China'}, {'name': 'James', 'country': 'Colombia'}, ] df = pd.DataFrame(data) filename = 'people.csv' df.to_csv(filename, index=False, encoding='utf-8')
Lire Enregistrer les données pandas dans un fichier csv en ligne: https://riptutorial.com/fr/pandas/topic/1558/enregistrer-les-donnees-pandas-dans-un-fichier-csv

https://riptutorial.com/fr/home

41

Chapitre 11: Faire jouer les Pandas avec les types de données Python natifs

Examples
Déplacement de données hors de pandas vers des structures de données natives Python et Numpy

In [1]: df = pd.DataFrame({'A': [1, 2, 3], 'B': [1.0, 2.0, 3.0], 'C': ['a', 'b', 'c'], 'D': [True, False, True]})

In [2]: df Out[2]:
A BC 0 1 1.0 a 1 2 2.0 b 2 3 3.0 c

D True False True

Obtenir une liste de python à partir d'une série:

In [3]: df['A'].tolist() Out[3]: [1, 2, 3]

Les DataFrames n'ont pas de méthode tolist() . Son essai entraîne une erreur d'attribut:

In [4]: df.tolist()

---------------------------------------------------------------------------

AttributeError

Traceback (most recent call last)

<ipython-input-4-fc6763af1ff7> in <module>()

----> 1 df.tolist()

//anaconda/lib/python2.7/site-packages/pandas/core/generic.pyc in __getattr__(self, name)

2742

if name in self._info_axis:

2743

return self[name]

-> 2744

return object.__getattribute__(self, name)

2745

2746

def __setattr__(self, name, value):

AttributeError: 'DataFrame' object has no attribute 'tolist'

Obtenir un tableau numpy d'une série:

In [5]: df['B'].values Out[5]: array([ 1., 2., 3.])

Vous pouvez également obtenir un tableau des colonnes sous forme de tableaux numpy individuels à partir d'un cadre de données complet:

In [6]: df.values

https://riptutorial.com/fr/home

42

Out[6]: array([[1, 1.0, 'a', True],
[2, 2.0, 'b', False], [3, 3.0, 'c', True]], dtype=object)
Obtenir un dictionnaire à partir d'une série (utilise l'index comme clés):
In [7]: df['C'].to_dict() Out[7]: {0: 'a', 1: 'b', 2: 'c'}
Vous pouvez également récupérer l'intégralité de DataFrame en tant que dictionnaire:
In [8]: df.to_dict() Out[8]: {'A': {0: 1, 1: 2, 2: 3},
'B': {0: 1.0, 1: 2.0, 2: 3.0}, 'C': {0: 'a', 1: 'b', 2: 'c'}, 'D': {0: True, 1: False, 2: True}}
La méthode to_dict a quelques paramètres différents pour ajuster le format des dictionnaires. Pour obtenir une liste de dicts pour chaque ligne:
In [9]: df.to_dict('records') Out[9]: [{'A': 1, 'B': 1.0, 'C': 'a', 'D': True},
{'A': 2, 'B': 2.0, 'C': 'b', 'D': False}, {'A': 3, 'B': 3.0, 'C': 'c', 'D': True}]
Voir la documentation pour la liste complète des options disponibles pour créer des dictionnaires.
Lire Faire jouer les Pandas avec les types de données Python natifs en ligne: https://riptutorial.com/fr/pandas/topic/8008/faire-jouer-les-pandas-avec-les-types-de-donneespython-natifs

https://riptutorial.com/fr/home

43

Chapitre 12: Fusionner, rejoindre et concaténer

Syntaxe
• Trame de données. merge ( right, how = 'inner', on = None, left_on = None, right_on = Aucun, left_index = False, right_index = False, sort = False, suffixes = ('_ x', '_y'), copy = True, indicateur = Faux )
• Fusionner des objets DataFrame en effectuant une opération de jointure de style base de données par colonnes ou index.
• Si vous associez des colonnes sur des colonnes, les index DataFrame seront ignorés. Sinon, si vous joignez des index sur des index ou des index sur une ou plusieurs colonnes, l'index sera transmis.
Paramètres

Paramètres Explication

droite

Trame de données

Comment {'gauche', 'droite', 'extérieur', 'intérieur'}, défaut 'intérieur'

à gauche sur

label ou list, ou tableau-like. Noms de champs sur lesquels se joindre dans DataFrame gauche. Peut être un vecteur ou une liste de vecteurs de la longueur du DataFrame pour utiliser un vecteur particulier comme clé de jointure au lieu de colonnes

à droite

label ou list, ou tableau-like. Noms de champs sur lesquels se joindre à droite DataFrame ou vecteur / liste de vecteurs par left_on docs

left_index

booléen, par défaut False. Utilisez l'index du DataFrame de gauche comme clé (s) de jointure. S'il s'agit d'un MultiIndex, le nombre de clés dans l'autre DataFrame (l'index ou un nombre de colonnes) doit correspondre au nombre de niveaux.

right_index

booléen, par défaut False. Utilisez l'index du bon DataFrame comme clé de jointure. Même mise en garde que left_index

Trier

booléen, Fals par défaut. Trier les clés de jointure lexicographiquement dans le résultat DataFrame

suffixes

Séquence de 2 longueurs (tuple, liste, ...). Suffixe à appliquer aux noms de colonnes qui se chevauchent respectivement à gauche et à droite

https://riptutorial.com/fr/home

44

Paramètres Explication

copie

booléen, par défaut True. Si la valeur est False, ne copiez pas inutilement des données

indicateur

booléen ou chaîne, par défaut False. Si True, ajoute une colonne à la sortie DataFrame appelée «_merge» avec des informations sur la source de chaque ligne. Si chaîne, une colonne avec des informations sur la source de chaque ligne sera ajoutée à la sortie DataFrame, et la colonne sera nommée valeur de la chaîne. La colonne d'information est de type catégorique et prend la valeur «left_only» pour les observations dont la clé de fusion n'apparaît que dans le cadre «left» DataFrame, «right_only» pour les observations dont la clé de fusion n'apparaît que dans «right» DataFrame et «both» si La clé de fusion de l'observation se trouve dans les deux.

Examples

Fusionner
Par exemple, deux tables sont données, T1

id x

y

8 42

1.9

9 30

1.9

T2

id signal 8 55 8 56 8 59 9 57 9 58 9 60

L'objectif est d'obtenir la nouvelle table T3:

id x 8 42 9 30

y

s1

s2

s3

1.9

55

56

58

1.9

57

58

60

Ce qui consiste à créer les colonnes s1 , s2 et s3 , chacune correspondant à une ligne (le nombre de lignes par id est toujours fixe et égal à 3)
En appliquant la join (qui prend un argument facultatif qui peut être une colonne ou plusieurs noms de colonne, ce qui spécifie que le DataFrame transmis doit être aligné sur cette colonne dans le DataFrame). La solution peut donc être la suivante:

https://riptutorial.com/fr/home

45

df = df1.merge (df2.groupby ('id') ['signal']. apply (lambda x: x.reset_index (drop = True)). unstack (). reset_index ())
df Out[63]:
id x y 0 1 2 0 8 42 1.9 55 56 59 1 9 30 1.9 57 58 60
Si je les sépare:
df2t = df2.groupby('id')['signal'].apply(lambda x: x.reset_index(drop=True)).unstack().reset_index()
df2t Out[59]:
id 0 1 2 0 8 55 56 59 1 9 57 58 60
df = df1.merge(df2t)
df Out[61]:
id x y 0 1 2 0 8 42 1.9 55 56 59 1 9 30 1.9 57 58 60
Fusion de deux DataFrames
In [1]: df1 = pd.DataFrame({'x': [1, 2, 3], 'y': ['a', 'b', 'c']})
In [2]: df2 = pd.DataFrame({'y': ['b', 'c', 'd'], 'z': [4, 5, 6]})
In [3]: df1 Out[3]:
xy 01a 12b 23c
In [4]: df2 Out[4]:
yz 0b4 1c5 2d6
Jointure interne:
Utilise l'intersection des clés de deux DataFrames.
In [5]: df1.merge(df2) # by default, it does an inner join on the common column(s)

https://riptutorial.com/fr/home

46

Out[5]: xyz
02b4 13c5
Vous pouvez également spécifier l'intersection des clés à partir de deux cadres de données.
In [5]: merged_inner = pd.merge(left=df1, right=df2, left_on='y', right_on='y') Out[5]:
xyz 02b4 13c5
Jointure externe:
Utilise l'union des clés de deux DataFrames.
In [6]: df1.merge(df2, how='outer') Out[6]:
xy z 0 1.0 a NaN 1 2.0 b 4.0 2 3.0 c 5.0 3 NaN d 6.0
Joint gauche:
Utilise uniquement les clés de gauche DataFrame.
In [7]: df1.merge(df2, how='left') Out[7]:
xy z 0 1 a NaN 1 2 b 4.0 2 3 c 5.0
Droit rejoindre
Utilise uniquement les clés du droit DataFrame.
In [8]: df1.merge(df2, how='right') Out[8]:
xyz 0 2.0 b 4 1 3.0 c 5 2 NaN d 6

https://riptutorial.com/fr/home

47

Fusion / concaténation / jonction de plusieurs blocs de données (horizontalement et verticalement)
générer des exemples de trames de données:
In [57]: df3 = pd.DataFrame({'col1':[211,212,213], 'col2': [221,222,223]})
In [58]: df1 = pd.DataFrame({'col1':[11,12,13], 'col2': [21,22,23]})
In [59]: df2 = pd.DataFrame({'col1':[111,112,113], 'col2': [121,122,123]})
In [60]: df3 = pd.DataFrame({'col1':[211,212,213], 'col2': [221,222,223]})
In [61]: df1 Out[61]:
col1 col2 0 11 21 1 12 22 2 13 23
In [62]: df2 Out[62]:
col1 col2 0 111 121 1 112 122 2 113 123
In [63]: df3 Out[63]:
col1 col2 0 211 221 1 212 222 2 213 223
fusionner / joindre / concaténer des trames de données [df1, df2, df3] verticalement - ajouter des lignes
In [64]: pd.concat([df1,df2,df3], ignore_index=True) Out[64]:
col1 col2 0 11 21 1 12 22 2 13 23 3 111 121 4 112 122 5 113 123 6 211 221 7 212 222 8 213 223
fusionner / joindre / concaténer des blocs de données horizontalement (alignement par index):
In [65]: pd.concat([df1,df2,df3], axis=1) Out[65]:
col1 col2 col1 col2 col1 col2 0 11 21 111 121 211 221

https://riptutorial.com/fr/home

48

1 12 22 112 122 212 222 2 13 23 113 123 213 223
Fusionner, rejoindre et concat
La fusion des noms de clés est la même
pd.merge(df1, df2, on='key')
La fusion des noms de clés est différente
pd.merge(df1, df2, left_on='l_key', right_on='r_key')
Différents types d'adhésion
pd.merge(df1, df2, on='key', how='left')
Fusion sur plusieurs clés
pd.merge(df1, df2, on=['key1', 'key2'])
Traitement des colonnes superposées
pd.merge(df1, df2, on='key', suffixes=('_left', '_right'))
Utilisation de l'index de lignes au lieu de fusionner des clés
pd.merge(df1, df2, right_index=True, left_index=True)
Évitez d'utiliser la syntaxe .join car elle donne une exception pour les colonnes qui se chevauchent Fusion sur l'index de dataframe gauche et la colonne dataframe droite
pd.merge(df1, df2, right_index=True, left_on='l_key')
Concentrer les dataframes Collé verticalement
pd.concat([df1, df2, df3], axis=0)
Collé horizontalement
pd.concat([df1, df2, df3], axis=1)

https://riptutorial.com/fr/home

49

Quelle est la différence entre rejoindre et fusionner
Considérons les données à left et à right
left = pd.DataFrame([['a', 1], ['b', 2]], list('XY'), list('AB')) left
AB Xa1 Yb2

right = pd.DataFrame([['a', 3], ['b', 4]], list('XY'), list('AC')) right
AC Xa3 Yb4

join
Pensez à join que de vouloir combiner pour dataframes en fonction de leurs indices respectifs. S'il y a des colonnes qui se chevauchent, join voudra que vous ajoutiez un suffixe au nom de la colonne qui se chevauchent à partir du dataframe gauche. Nos deux cadres de données ont un nom de colonne qui se chevauche A
left.join(right, lsuffix='_')
A_ B A C Xa1a3 Yb2b4
Notez que l'index est conservé et que nous avons 4 colonnes. 2 colonnes de left et 2 de right .
Si les index ne sont pas alignés

left.join(right.reset_index(), lsuffix='_', how='outer')

A_ B index A C

0 NaN NaN

X a 3.0

1 NaN NaN

Y b 4.0

X a 1.0 NaN NaN NaN

Y b 2.0 NaN NaN NaN

J'ai utilisé une jointure externe pour mieux illustrer ce point. Si les index ne sont pas alignés, le résultat sera l'union des index.
Nous pouvons dire à join d'utiliser une colonne spécifique dans le dataframe de gauche à utiliser comme clé de jointure, mais elle utilisera toujours l'index depuis la droite.

left.reset_index().join(right, on='index', lsuffix='_')

index A_ B A C

0

Xa1a3

https://riptutorial.com/fr/home

50

1

Yb2b4

merge
Pensez à merge en alignant sur des colonnes. Par défaut, la merge recherche les colonnes qui se chevauchent dans lesquelles fusionner. merge donne un meilleur contrôle sur les clés de fusion en permettant à l'utilisateur de spécifier un sous - ensemble des colonnes qui se chevauchent à utiliser avec le paramètre on ou pour permettre séparément dont la spécification des colonnes à gauche et les colonnes sur le droit de fusionner par.
merge renverra un dataframe combiné dans lequel l'index sera détruit.
Cet exemple simple recherche la colonne qui se chevauche pour être 'A' et combine en fonction de celle-ci.

left.merge(right)
ABC 0a13 1b24

Notez que l'index est [0, 1] et non plus ['X', 'Y']
Vous pouvez spécifier explicitement que vous fusionnez sur l'indice avec le left_index ou right_index paramter

left.merge(right, left_index=True, right_index=True, suffixes=['_', ''])
A_ B A C Xa1a3 Yb2b4

Et cela ressemble exactement à l'exemple de join ci-dessus.
Lire Fusionner, rejoindre et concaténer en ligne: https://riptutorial.com/fr/pandas/topic/1966/fusionner--rejoindre-et-concatener

https://riptutorial.com/fr/home

51

Chapitre 13: Gotchas de pandas

Remarques
Gotcha en général est une construction bien que documentée, mais pas intuitive. Les Gotchas produisent des résultats normalement inattendus en raison de leur caractère contre-intuitif. Le paquet Pandas a plusieurs pièges, qui peuvent induire en erreur quelqu'un, qui n'en a pas connaissance, et certains d'entre eux sont présentés sur cette page de documentation.
Examples
Détecter les valeurs manquantes avec np.nan
Si vous voulez détecter les manquements avec
df=pd.DataFrame({'col':[1,np.nan]}) df==np.nan
vous obtiendrez le résultat suivant:
col 0 False 1 False
C’est parce que la comparaison des valeurs manquantes avec quelque chose donne lieu à un False - au lieu de cela, vous devez utiliser
df=pd.DataFrame({'col':[1,np.nan]}) df.isnull()
qui se traduit par:
col 0 False 1 True
Entier et NA
Les pandas ne prennent pas en charge les attributs manquants du type entier. Par exemple si vous avez des manques dans la colonne de notation:
df= pd.read_csv("data.csv", dtype={'grade': int}) error: Integer column has NA values
Dans ce cas, vous devez simplement utiliser float au lieu de nombres entiers ou définir le type

https://riptutorial.com/fr/home

52

d'objet.
Alignement automatique des données (comportement indexé)
Si vous souhaitez ajouter une série de valeurs [1,2] à la colonne de dataframe df, vous obtiendrez NaN:

import pandas as pd
series=pd.Series([1,2]) df=pd.DataFrame(index=[3,4]) df['col']=series df
col 3 NaN 4 NaN

car la définition d'une nouvelle colonne aligne automatiquement les données par l'index et vos valeurs 1 et 2 obtiendront les index 0 et 1 et non 3 et 4 comme dans votre bloc de données:

df=pd.DataFrame(index=[1,2]) df['col']=series df

col

1

2.0

2

NaN

Si vous voulez ignorer l'index, vous devez définir les valeurs à la fin:

df['col']=series.values
col 31 42

Lire Gotchas de pandas en ligne: https://riptutorial.com/fr/pandas/topic/6425/gotchas-de-pandas

https://riptutorial.com/fr/home

53

Chapitre 14: Graphes et Visualisations
Examples
Graphiques de données de base
Pandas utilise plusieurs méthodes pour créer des graphiques des données dans le bloc de données. Il utilise du matplotlib à cette fin. Les graphiques de base ont leurs enveloppes pour les objets DataFrame et Series: Graphique linéaire
df = pd.DataFrame({'x': [10, 8, 10, 7, 7, 10, 9, 9], 'y': [6, 4, 5, 5, 7, 10, 9, 9]})
df.plot()

Vous pouvez appeler la même méthode pour un objet Series pour tracer un sous-ensemble du Data Frame:
df['x'].plot()

https://riptutorial.com/fr/home

54

Diagramme à bandes Si vous souhaitez explorer la distribution de vos données, vous pouvez utiliser la méthode hist() .
df['x'].hist()

Méthode générale pour tracer un tracé () Tous les graphiques possibles sont disponibles via la méthode de tracé. Le type de graphique est sélectionné par l'argument kind .
df['x'].plot(kind='pie')

https://riptutorial.com/fr/home

55

Remarque Dans de nombreux environnements, le graphique à secteurs sortira un ovale. Pour en faire un cercle, utilisez ce qui suit:
from matplotlib import pyplot
pyplot.axis('equal') df['x'].plot(kind='pie')
Styling l'intrigue
plot() peut prendre des arguments transmis à matplotlib pour donner un style à l'intrigue de différentes manières.
df.plot(style='o') # plot as dots, not lines df.plot(style='g--') # plot as green dashed line df.plot(style='o', markeredgecolor='white') # plot as dots with white edge
Tracer sur un axe matplotlib existant
Par défaut, plot() crée un nouveau chiffre à chaque appel. Il est possible de tracer un axe existant en passant le paramètre ax .
plt.figure() # create a new figure ax = plt.subplot(121) # create the left-side subplot df1.plot(ax=ax) # plot df1 on that subplot ax = plt.subplot(122) # create the right-side subplot df2.plot(ax=ax) # and plot df2 there plt.show() # show the plot
Lire Graphes et Visualisations en ligne: https://riptutorial.com/fr/pandas/topic/3839/graphes-etvisualisations

https://riptutorial.com/fr/home

56

Chapitre 15: Indexation booléenne des dataframes
Introduction
Accès aux lignes d'un fichier de données à l'aide des objets d'indexeur .ix , .loc , .iloc et de la manière dont il se différencie de l'utilisation d'un masque booléen.
Examples
Accéder à un DataFrame avec un index booléen
Ce sera notre exemple de trame de données:
df = pd.DataFrame({"color": ['red', 'blue', 'red', 'blue']}, index=[True, False, True, False])
color True red False blue True red False blue
Accéder avec .loc
df.loc[True] color
True red True red
Accéder avec .iloc
df.iloc[True] >> TypeError
df.iloc[1] color blue dtype: object
Il est important de noter que les anciennes versions de pandas ne faisaient pas de distinction entre les entrées booléennes et les entrées entières, donc .iloc[True] renverrait la même chose que .iloc[1]
Accéder avec .ix
df.ix[True] color
True red

https://riptutorial.com/fr/home

57

True red df.ix[1] color blue dtype: object
Comme vous pouvez le voir, .ix a deux comportements. Ceci est une très mauvaise pratique en code et doit donc être évité. Veuillez utiliser .iloc ou .loc pour être plus explicite.
Application d'un masque booléen à un dataframe
Ce sera notre exemple de trame de données:

color 0 red 1 blue 2 red 3 blue

name rose violet tulip harebell

size big big
small small

En utilisant l’ __getitem__ magique __getitem__ ou [] . En lui donnant une liste de True et False de la même longueur que le dataframe vous donnera:

df[[True, False, True, False]] color name size
0 red rose big 2 red tulip small

Masquage des données en fonction de la valeur de la colonne
Ce sera notre exemple de trame de données:

color 0 red 1 blue 2 red 3 blue

name rose violet tulip harebell

size big
small small small

En accédant à une seule colonne à partir d'un pd.Series de données, nous pouvons utiliser une simple comparaison == pour comparer chaque élément de la colonne à la variable donnée, produisant une pd.Series de True et False.

df['size'] == 'small'

0 False

1

True

2

True

3

True

Name: size, dtype: bool

Cette pd.Series est une extension d'un np.array qui est une extension d'une list simple. Ainsi, nous pouvons transmettre ceci à l' __getitem__ ou [] comme dans l'exemple ci-dessus.

https://riptutorial.com/fr/home

58

size_small_mask = df['size'] == 'small'

df[size_small_mask]

color

name size

1 blue violet small

2 red

tulip small

3 blue harebell small

Masquage des données en fonction de la valeur d'index
Ce sera notre exemple de trame de données:

color

name

rose

red

violet blue

tulip

red

harebell blue

size
big small small small

Nous pouvons créer un masque basé sur les valeurs d'index, comme sur une valeur de colonne.

rose_mask = df.index == 'rose' df[rose_mask]
color size name rose red big
Mais faire cela est presque la même que

df.loc['rose']

color red

size

big

Name: rose, dtype: object

La différence importante étant que, lorsque .loc ne rencontre qu'une ligne dans l'index correspondant, il retournera un pd.Series , s'il rencontre plus de lignes qui correspondent, il retournera un pd.DataFrame . Cela rend cette méthode plutôt instable.
Ce comportement peut être contrôlé en donnant à la .loc une liste d'une seule entrée. Cela l'obligera à retourner un bloc de données.

df.loc[['rose']]

color size

name

rose

red big

Lire Indexation booléenne des dataframes en ligne: https://riptutorial.com/fr/pandas/topic/9589/indexation-booleenne-des-dataframes

https://riptutorial.com/fr/home

59

Chapitre 16: Indexation et sélection de données

Examples

Sélectionnez colonne par étiquette

# Create a sample DF df = pd.DataFrame(np.random.randn(5, 3), columns=list('ABC'))

# Show DF

df

A

B

C

0 -0.467542 0.469146 -0.861848

1 -0.823205 -0.167087 -0.759942

2 -1.508202 1.361894 -0.166701

3 0.394143 -0.287349 -0.978102

4 -0.160431 1.054736 -0.785250

# Select column using a single label, 'A' df['A'] 0 -0.467542 1 -0.823205 2 -1.508202 3 0.394143 4 -0.160431

# Select multiple columns using an array of labels, ['A', 'C']

df[['A', 'C']]

A

C

0 -0.467542 -0.861848

1 -0.823205 -0.759942

2 -1.508202 -0.166701

3 0.394143 -0.978102

4 -0.160431 -0.785250

Détails supplémentaires sur: http://pandas.pydata.org/pandasdocs/version/0.18.0/indexing.html#selection-by-label
Sélectionner par position
La iloc méthode (abréviation de l' emplacement entier) permet de sélectionner les lignes d'une trame de données en fonction de leur indice de position. De cette façon, on peut découper des cadres de données comme on le fait avec le découpage des listes de Python.

df = pd.DataFrame([[11, 22], [33, 44], [55, 66]], index=list("abc"))

df

# Out:

#

01

# a 11 22

https://riptutorial.com/fr/home

60

# b 33 44 # c 55 66

df.iloc[0] # the 0th index (row) # Out: # 0 11 # 1 22 # Name: a, dtype: int64

df.iloc[1] # the 1st index (row) # Out: # 0 33 # 1 44 # Name: b, dtype: int64

df.iloc[:2] # the first 2 rows

#

01

# a 11 22

# b 33 44

df[::-1] # reverse order of rows

#

01

# c 55 66

# b 33 44

# a 11 22

L'emplacement des lignes peut être combiné avec l'emplacement des colonnes

df.iloc[:, 1] # the 1st column # Out[15]: # a 22 # b 44 # c 66 # Name: 1, dtype: int64

Voir aussi: Sélection par position
Trancher avec des étiquettes
Lorsque vous utilisez des étiquettes, le début et la fin sont inclus dans les résultats.

import pandas as pd import numpy as np np.random.seed(5) df = pd.DataFrame(np.random.randint(100, size=(5, 5)), columns = list("ABCDE"),
index = ["R" + str(i) for i in range(5)])

# Out:

#

ABCDE

# R0 99 78 61 16 73

# R1 8 62 27 30 80

# R2 7 76 15 53 80

# R3 27 44 77 75 65

# R4 47 30 84 86 18

Lignes R0 à R2 :

https://riptutorial.com/fr/home

61

df.loc['R0':'R2']

# Out:

#

ABCDE

# R0 9 41 62 1 82

# R1 16 78 5 58 0

# R2 80 4 36 51 27

Notez que loc diffère d' iloc car iloc exclut l'index de fin

df.loc['R0':'R2'] # rows labelled R0, R1, R2

# Out:

#

ABCDE

# R0 9 41 62 1 82

# R1 16 78 5 58 0

# R2 80 4 36 51 27

# df.iloc[0:2] # rows indexed by 0, 1

#

ABCDE

# R0 99 78 61 16 73

# R1 8 62 27 30 80

Colonnes C à E :

df.loc[:, 'C':'E']

# Out:

#

CDE

# R0 62 1 82

# R1 5 58 0

# R2 36 51 27

# R3 68 38 83

# R4 7 30 62

Sélection mixte et sélection basée sur une étiquette
Trame de données:
import pandas as pd import numpy as np np.random.seed(5) df = pd.DataFrame(np.random.randint(100, size=(5, 5)), columns = list("ABCDE"),
index = ["R" + str(i) for i in range(5)])

df Out[12]:
ABCDE R0 99 78 61 16 73 R1 8 62 27 30 80 R2 7 76 15 53 80 R3 27 44 77 75 65 R4 47 30 84 86 18
Sélectionnez les lignes par position et les colonnes par libellé:

https://riptutorial.com/fr/home

62

df.ix[1:3, 'C':'E'] Out[19]:
CDE R1 5 58 0 R2 36 51 27
Si l'index est entier, .ix utilisera des libellés plutôt que des positions:
df.index = np.arange(5, 10)
df Out[22]:
ABCDE 5 9 41 62 1 82 6 16 78 5 58 0 7 80 4 36 51 27 8 31 2 68 38 83 9 19 18 7 30 62
#same call returns an empty DataFrame because now the index is integer df.ix[1:3, 'C':'E'] Out[24]: Empty DataFrame Columns: [C, D, E] Index: []

Indexation booléenne

On peut sélectionner des lignes et des colonnes d'un dataframe en utilisant des tableaux booléens.

import pandas as pd

import numpy as np

np.random.seed(5)

df = pd.DataFrame(np.random.randint(100, size=(5, 5)), columns = list("ABCDE"),

index = ["R" + str(i) for i in range(5)])

print (df)

#

ABCDE

# R0 99 78 61 16 73

# R1 8 62 27 30 80

# R2 7 76 15 53 80

# R3 27 44 77 75 65

# R4 47 30 84 86 18

mask = df['A'] > 10

print (mask)

# R0

True

# R1 False

# R2 False

# R3

True

# R4

True

# Name: A, dtype: bool

print (df[mask])

#

ABCDE

# R0 99 78 61 16 73

# R3 27 44 77 75 65

https://riptutorial.com/fr/home

63

# R4 47 30 84 86 18

print (df.ix[mask, 'C']) # R0 61 # R3 77 # R4 84 # Name: C, dtype: int32

print(df.ix[mask, ['C', 'D']])

#

CD

# R0 61 16

# R3 77 75

# R4 84 86

Plus dans la documentation des pandas .

Filtrage des colonnes (en sélectionnant "intéressant", en supprimant des données inutiles, en utilisant RegEx, etc.)

générer un échantillon DF

In [39]: df = pd.DataFrame(np.random.randint(0, 10, size=(5, 6)), columns=['a10','a20','a25','b','c','d'])
In [40]: df Out[40]:
a10 a20 a25 b c d 0 2 3 7547 1 3 1 5726 2 7 4 9087 3 5 8 8968 4 8 1 0449

affiche les colonnes contenant la lettre 'a'

In [41]: df.filter(like='a') Out[41]:
a10 a20 a25 0237 1315 2749 3588 4810

affiche les colonnes à l'aide du filtre RegEx

- ou ou : (b|c|d) b

c

d

In [42]: df.filter(regex='(b|c|d)')

https://riptutorial.com/fr/home

64

Out[42]: bcd
0547 1726 2087 3968 4449
afficher toutes les colonnes sauf celles commençant par a (en d'autres termes, supprimer / supprimer toutes les colonnes satisfaisant à RegEx donné)
In [43]: df.ix[:, ~df.columns.str.contains('^a')] Out[43]:
bcd 0547 1726 2087 3968 4449
Filtrage / sélection de lignes en utilisant la méthode `.query ()`
import pandas as pd
générer des DF aléatoires
df = pd.DataFrame(np.random.randint(0,10,size=(10, 3)), columns=list('ABC'))
In [16]: print(df) ABC
0414 1020 2788 3219 4738 5407 6155 7678 8673 9645
sélectionnez les lignes où les valeurs de la colonne A > 2 et les valeurs de la colonne B < 5

https://riptutorial.com/fr/home

65

In [18]: df.query('A > 2 and B < 5') Out[18]:
ABC 0414 4738 5407 9645
utiliser la méthode .query() avec des variables pour le filtrage
In [23]: B_filter = [1,7]
In [24]: df.query('B == @B_filter') Out[24]:
ABC 0414 3219 7678 8673
In [25]: df.query('@B_filter in B') Out[25]:
ABC 0414
Tranchage dépendant du chemin
Il peut être nécessaire de parcourir les éléments d’une série ou les lignes d’un dataframe de manière à ce que l’élément suivant ou la ligne suivante dépende de l’élément ou de la ligne précédemment sélectionnés. Ceci s'appelle la dépendance de chemin.
Considérons la série chronologique suivant s avec une fréquence irrégulière.
#starting python community conventions import numpy as np import pandas as pd
# n is number of observations n = 5000
day = pd.to_datetime(['2013-02-06']) # irregular seconds spanning 28800 seconds (8 hours) seconds = np.random.rand(n) * 28800 * pd.Timedelta(1, 's') # start at 8 am start = pd.offsets.Hour(8) # irregular timeseries tidx = day + start + seconds tidx = tidx.sort_values()
s = pd.Series(np.random.randn(n), tidx, name='A').cumsum() s.plot();

https://riptutorial.com/fr/home

66

Supposons une condition dépendant du chemin. En commençant par le premier membre de la série, je veux saisir chaque élément suivant de sorte que la différence absolue entre cet élément et l'élément actuel soit supérieure ou égale à x .
Nous allons résoudre ce problème en utilisant des générateurs de python.
Fonction générateur
def mover(s, move_size=10): """Given a reference, find next value with an absolute difference >= move_size""" ref = None for i, v in s.iteritems(): if ref is None or (abs(ref - v) >= move_size): yield i, v ref = v
Ensuite, nous pouvons définir une nouvelle série de moves comme ça
moves = pd.Series({i:v for i, v in mover(s, move_size=10)}, name='_{}_'.format(s.name))
Les tracer tous les deux
moves.plot(legend=True) s.plot(legend=True)

https://riptutorial.com/fr/home

67

L'analogique pour les dataframes serait:
def mover_df(df, col, move_size=2): ref = None for i, row in df.iterrows(): if ref is None or (abs(ref - row.loc[col]) >= move_size): yield row ref = row.loc[col]
df = s.to_frame() moves_df = pd.concat(mover_df(df, 'A', 10), axis=1).T
moves_df.A.plot(label='_A_', legend=True) df.A.plot(legend=True)

Récupère les premières / dernières n lignes d'un dataframe
Pour voir le premier ou le dernier enregistrement d’un dataframe, vous pouvez utiliser les méthodes head et tail Pour renvoyer les n premières lignes, utilisez DataFrame.head([n])
df.head(n)

https://riptutorial.com/fr/home

68

Pour retourner les n dernières lignes, utilisez DataFrame.tail([n])
df.tail(n)
Sans l'argument n, ces fonctions renvoient 5 lignes. Notez que la notation de tranche pour head / tail serait:
df[:10] # same as df.head(10) df[-10:] # same as df.tail(10)

Sélectionnez des lignes distinctes sur l'ensemble des données
Laisser

df = pd.DataFrame({'col_1':['A','B','A','B','C'], 'col_2':[3,4,3,5,6]})

df

# Output:

# col_1 col_2

#0

A

3

#1

B

4

#2

A

3

#3

B

5

#4

C

6

Pour obtenir les valeurs distinctes dans col_1 vous pouvez utiliser Series.unique()

df['col_1'].unique() # Output: # array(['A', 'B', 'C'], dtype=object)

Mais Series.unique () ne fonctionne que pour une seule colonne.
Pour simuler la sélection unique col_1, col_2 de SQL, vous pouvez utiliser DataFrame.drop_duplicates() :

df.drop_duplicates()

# col_1 col_2

#0

A

3

#1

B

4

#3

B

5

#4

C

6

Cela vous donnera toutes les lignes uniques dans le dataframe. Donc si

df = pd.DataFrame({'col_1':['A','B','A','B','C'], 'col_2':[3,4,3,5,6],

'col_3':[0,0.1,0.2,0.3,0.4]})

df

# Output:

# col_1 col_2 col_3

#0

A

3 0.0

#1

B

4 0.1

https://riptutorial.com/fr/home

69

#2

A

#3

B

#4

C

3 0.2 5 0.3 6 0.4

df.drop_duplicates()

# col_1 col_2 col_3

#0

A

3 0.0

#1

B

4 0.1

#2

A

3 0.2

#3

B

5 0.3

#4

C

6 0.4

Pour spécifier les colonnes à prendre en compte lors de la sélection d'enregistrements uniques, transmettez-les comme arguments

df = pd.DataFrame({'col_1':['A','B','A','B','C'], 'col_2':[3,4,3,5,6],

'col_3':[0,0.1,0.2,0.3,0.4]})

df.drop_duplicates(['col_1','col_2'])

# Output:

# col_1 col_2 col_3

#0

A

3 0.0

#1

B

4 0.1

#3

B

5 0.3

#4

C

6 0.4

# skip last column

# df.drop_duplicates(['col_1','col_2'])[['col_1','col_2']]

# col_1 col_2

#0

A

3

#1

B

4

#3

B

5

#4

C

6

Source: Comment «sélectionner distinct» sur plusieurs colonnes de trames de données dans les pandas? .
Filtrer les lignes avec les données manquantes (NaN, None, NaT)
Si vous avez un dataframe avec des données manquantes ( NaN , pd.NaT , None ), vous pouvez filtrer les lignes incomplètes

df = pd.DataFrame([[0,1,2,3],

[None,5,None,pd.NaT],

[8,None,10,None],

[11,12,13,pd.NaT]],columns=list('ABCD'))

df

# Output:

#

ABC

D

#0 0 1 2

3

# 1 NaN 5 NaN NaT

# 2 8 NaN 10 None

# 3 11 12 13 NaT

DataFrame.dropna supprime toutes les lignes contenant au moins un champ avec des données manquantes

https://riptutorial.com/fr/home

70

df.dropna() # Output: # ABCD #0 0 1 2 3

Pour simplement supprimer les lignes pour lesquelles il manque des données à des colonnes spécifiées, utilisez le subset

df.dropna(subset=['C'])

# Output:

#

ABC

D

#0 0 1 2

3

# 2 8 NaN 10 None

# 3 11 12 13 NaT

Utilisez l'option inplace = True pour le remplacement sur place avec le cadre filtré.
Lire Indexation et sélection de données en ligne: https://riptutorial.com/fr/pandas/topic/1751/indexation-et-selection-de-donnees

https://riptutorial.com/fr/home

71

Chapitre 17: IO pour Google BigQuery

Examples
Lecture des données de BigQuery avec les informations d'identification du compte utilisateur

In [1]: import pandas as pd

Pour exécuter une requête dans BigQuery, vous devez avoir votre propre projet BigQuery. Nous pouvons demander des exemples de données publiques:

In [2]: data = pd.read_gbq('''SELECT title, id, num_characters

...:

FROM [publicdata:samples.wikipedia]

...:

LIMIT 5'''

...:

, project_id='<your-project-id>')

Cela va imprimer:

Your browser has been opened to visit: https://accounts.google.com/o/oauth2/v2/auth...[looong url cutted]
If your browser is on a different machine then exit and re-run this application with the command-line parameter
--noauth_local_webserver

Si vous opérez depuis un ordinateur local, le navigateur apparaîtra. Après avoir accordé des privilèges, les pandas continueront avec la sortie:

Authentication successful. Requesting query... ok. Query running... Query done. Processed: 13.8 Gb
Retrieving results... Got 5 rows.
Total time taken 1.5 s. Finished at 2016-08-23 11:26:03.

Résultat:

In [3]: data

Out[3]:

title

0

Fusidic acid

1

Clark Air Base

id num_characters

935328

1112

426241

8257

https://riptutorial.com/fr/home

72

2 Watergate scandal 52382

3

2005 35984

4

.BLP 2664340

25790 75813
1659

Comme effet secondaire, les pandas créeront le fichier json bigquery_credentials.dat qui vous permettra d'exécuter d'autres requêtes sans avoir à accorder de privilèges:

In [9]: pd.read_gbq('SELECT count(1) cnt FROM [publicdata:samples.wikipedia]' , project_id='<your-project-id>')
Requesting query... ok. [rest of output cutted]
Out[9]: cnt
0 313797035

Lecture des données de BigQuery avec les informations d'identification du compte de service
Si vous avez créé un compte de service et que vous avez un fichier json de clé privée, vous pouvez utiliser ce fichier pour vous authentifier avec des pandas

In [5]: pd.read_gbq('''SELECT corpus, sum(word_count) words FROM [bigquery-public-data:samples.shakespeare] GROUP BY corpus ORDER BY words desc LIMIT 5'''
, project_id='<your-project-id>' , private_key='<private key json contents or file path>') Requesting query... ok. [rest of output cutted]

Out[5]:

corpus

0

hamlet

1 kingrichardiii

2

coriolanus

3

cymbeline

4 2kinghenryiv

words 32446 31868 29535 29231 28241

Lire IO pour Google BigQuery en ligne: https://riptutorial.com/fr/pandas/topic/5610/io-pour-googlebigquery

https://riptutorial.com/fr/home

73

Chapitre 18: JSON
Examples
Lire JSON
peut soit transmettre une chaîne de json, soit un chemin de fichier à un fichier avec json valide

In [99]: pd.read_json('[{"A": 1, "B": 2}, {"A": 3, "B": 4}]') Out[99]:
AB 012 134
Sinon, conservez de la mémoire:
with open('test.json') as f: data = pd.DataFrame(json.loads(line) for line in f)
Dataframe dans JSON imbriqué comme dans les fichiers flare.js utilisés dans D3.js
def to_flare_json(df, filename): """Convert dataframe into nested JSON as in flare files used for D3.js""" flare = dict() d = {"name":"flare", "children": []}
for index, row in df.iterrows(): parent = row[0] child = row[1] child_size = row[2]
# Make a list of keys key_list = [] for item in d['children']:
key_list.append(item['name'])
#if 'parent' is NOT a key in flare.JSON, append it if not parent in key_list:
d['children'].append({"name": parent, "children":[{"value": child_size, "name": child}]})
# if parent IS a key in flare.json, add a new child to it else:
d['children'][key_list.index(parent)]['children'].append({"value": child_size, "name": child})

https://riptutorial.com/fr/home

74

flare = d # export the final result to a json file with open(filename +'.json', 'w') as outfile:
json.dump(flare, outfile, indent=4) return ("Done")
Lire JSON à partir du fichier
Contenu de file.json (un objet JSON par ligne):
{"A": 1, "B": 2} {"A": 3, "B": 4}
Comment lire directement depuis un fichier local:
pd.read_json('file.json', lines=True) # Output: # AB #0 1 2 #1 3 4
Lire JSON en ligne: https://riptutorial.com/fr/pandas/topic/4752/json

https://riptutorial.com/fr/home

75

Chapitre 19: Lecture de fichiers dans des pandas DataFrame

Examples

Lire la table dans DataFrame

Fichier de table avec en-tête, pied de page, noms de ligne et colonne d'index:

fichier: table.txt

This is a header that discusses the table file to show space in a generic table file

index 1 2 3

name Alice Bob Charlie

occupation Salesman Engineer Janitor

This is a footer because your boss does not understand data files

code:

import pandas as pd # index_col=0 tells pandas that column 0 is the index and not data pd.read_table('table.txt', delim_whitespace=True, skiprows=3, skipfooter=2, index_col=0)

sortie:

index 1 2 3

name occupation

Alice Bob
Charlie

Salesman Engineer
Janitor

Fichier de table sans noms de lignes ou index:

fichier: table.txt

Alice Salesman

Bob

Engineer

Charlie Janitor

code:

https://riptutorial.com/fr/home

76

import pandas as pd pd.read_table('table.txt', delim_whitespace=True, names=['name','occupation'])

sortie:

name occupation

0 Alice Salesman

1

Bob Engineer

2 Charlie Janitor

Toutes les options peuvent être trouvées dans la documentation des pandas ici
Lire un fichier CSV

Données avec en-tête, séparées par des points-virgules au lieu de virgules

fichier: table.csv

index;name;occupation 1;Alice;Saleswoman 2;Bob;Engineer 3;Charlie;Janitor

code:

import pandas as pd pd.read_csv('table.csv', sep=';', index_col=0)

sortie :

index 1 2 3

name occupation

Alice Bob
Charlie

Salesman Engineer
Janitor

Table sans noms de lignes ou index et virgules comme séparateurs
fichier: table.csv
Alice,Saleswoman Bob,Engineer Charlie,Janitor
code:

https://riptutorial.com/fr/home

77

import pandas as pd pd.read_csv('table.csv', names=['name','occupation'])

sortie:

name occupation

0 Alice Salesman

1

Bob Engineer

2 Charlie Janitor

des précisions supplémentaires peuvent être trouvées dans la page de documentation de read_csv
Recueillez les données de la feuille de calcul google dans les données pandas
Parfois, nous devons collecter des données à partir de feuilles de calcul google. Nous pouvons utiliser les bibliothèques gspread et oauth2client pour collecter des données à partir de feuilles de calcul google. Voici un exemple pour collecter des données:
Code:

from __future__ import print_function import gspread from oauth2client.client import SignedJwtAssertionCredentials import pandas as pd import json
scope = ['https://spreadsheets.google.com/feeds']
credentials = ServiceAccountCredentials.from_json_keyfile_name('your-authorization-file.json', scope)
gc = gspread.authorize(credentials)
work_sheet = gc.open_by_key("spreadsheet-key-here") sheet = work_sheet.sheet1 data = pd.DataFrame(sheet.get_all_records())
print(data.head())

Lire Lecture de fichiers dans des pandas DataFrame en ligne: https://riptutorial.com/fr/pandas/topic/1988/lecture-de-fichiers-dans-des-pandas-dataframe

https://riptutorial.com/fr/home

78

Chapitre 20: Lire MySQL sur DataFrame
Examples
Utiliser sqlalchemy et PyMySQL
from sqlalchemy import create_engine
cnx = create_engine('mysql+pymysql://username:password@server:3306/database').connect() sql = 'select * from mytable' df = pd.read_sql(sql, cnx)
Pour lire mysql sur dataframe, en cas de grande quantité de données
Pour récupérer des données volumineuses, nous pouvons utiliser des générateurs dans les pandas et charger des données en morceaux.
import pandas as pd from sqlalchemy import create_engine from sqlalchemy.engine.url import URL
# sqlalchemy engine engine = create_engine(URL(
drivername="mysql" username="user", password="password" host="host" database="database" ))
conn = engine.connect()
generator_df = pd.read_sql(sql=query, # mysql query con=conn, chunksize=chunksize) # size you want to fetch each time
for dataframe in generator_df: for row in dataframe: pass # whatever you want to do
Lire Lire MySQL sur DataFrame en ligne: https://riptutorial.com/fr/pandas/topic/8809/lire-mysqlsur-dataframe

https://riptutorial.com/fr/home

79

Chapitre 21: Lire SQL Server vers Dataframe

Examples
Utiliser pyodbc
import pandas.io.sql import pyodbc import pandas as pd
Spécifiez les paramètres
# Parameters server = 'server_name' db = 'database_name' UID = 'user_id'
Créer la connexion
# Create the connection conn = pyodbc.connect('DRIVER={SQL Server};SERVER=' + server + ';DATABASE=' + db + '; UID = ' + UID + '; PWD = ' + UID + 'Trusted_Connection=yes')
Requête en pandas dataframe
# Query into dataframe df= pandas.io.sql.read_sql('sql_query_string', conn)
Utiliser pyodbc avec boucle de connexion
import os, time import pyodbc import pandas.io.sql as pdsql
def todf(dsn='yourdsn', uid=None, pwd=None, query=None, params=None): ''' if `query` is not an actual query but rather a path to a text file containing a query, read it in instead ''' if query.endswith('.sql') and os.path.exists(query): with open(query,'r') as fin: query = fin.read()
connstr = "DSN={};UID={};PWD={}".format(dsn,uid,pwd) connected = False while not connected:
try: with pyodbc.connect(connstr,autocommit=True) as con: cur = con.cursor() if params is not None: df = pdsql.read_sql(query, con, params=params) else: df = pdsql.read_sql(query, con) cur.close()

https://riptutorial.com/fr/home

80

break except pyodbc.OperationalError:
time.sleep(60) # one minute could be changed return df
Lire Lire SQL Server vers Dataframe en ligne: https://riptutorial.com/fr/pandas/topic/2176/lire-sqlserver-vers-dataframe

https://riptutorial.com/fr/home

81

Chapitre 22: Manipulation de cordes

Examples

Expressions régulières

# Extract strings with a specific regex df= df['col_name'].str.extract[r'[Aa-Zz]']
# Replace strings within a regex df['col_name'].str.replace('Replace this', 'With this')

Pour plus d'informations sur la manière de faire correspondre les chaînes à l'aide de l'expression rationnelle, voir Mise en route avec les expressions régulières .
Ficelle
Les chaînes d'une série peuvent être découpées en utilisant la méthode .str.slice() , ou plus facilement, entre parenthèses ( .str[] ).

In [1]: ser = pd.Series(['Lorem ipsum', 'dolor sit amet', 'consectetur adipiscing elit'])

In [2]: ser

Out[2]:

0

Lorem ipsum

1

dolor sit amet

2 consectetur adipiscing elit

dtype: object

Obtenez le premier caractère de chaque chaîne:

In [3]: ser.str[0] Out[3]: 0L 1d 2c dtype: object

Obtenez les trois premiers caractères de chaque chaîne:

In [4]: ser.str[:3] Out[4]: 0 Lor 1 dol 2 con dtype: object

Récupère le dernier caractère de chaque chaîne:

In [5]: ser.str[-1]

https://riptutorial.com/fr/home

82

Out[5]: 0m 1t 2t dtype: object

Obtenez les trois derniers caractères de chaque chaîne:

In [6]: ser.str[-3:] Out[6]: 0 sum 1 met 2 lit dtype: object

Obtenez tous les autres caractères des 10 premiers caractères:

In [7]: ser.str[:10:2] Out[7]: 0 Lrmis 1 dlrst 2 cnett dtype: object

Pandas se comporte de la même manière que Python lors de la manipulation de tranches et d’index. Par exemple, si un index est en dehors de la plage, Python génère une erreur:

In [8]:'Lorem ipsum'[12] # IndexError: string index out of range

Cependant, si une tranche est en dehors de la plage, une chaîne vide est renvoyée:

In [9]: 'Lorem ipsum'[12:15] Out[9]: ''

Pandas renvoie NaN lorsqu'un index est hors limites:

In [10]: ser.str[12]

Out[10]:

0 NaN

1

e

2

a

dtype: object

Et renvoie une chaîne vide si une tranche est hors limites:

In [11]: ser.str[12:15]

Out[11]:

0

1

et

2 adi

dtype: object

https://riptutorial.com/fr/home

83

Vérification du contenu d'une chaîne
str.contains() méthode str.contains() peut être utilisée pour vérifier si un motif se produit dans chaque chaîne d'une série. str.startswith() et str.endswith() peuvent également être utilisées comme versions plus spécialisées.

In [1]: animals = pd.Series(['cat', 'dog', 'bear', 'cow', 'bird', 'owl', 'rabbit', 'snake'])

Vérifiez si les chaînes contiennent la lettre 'a':

In [2]: animals.str.contains('a')

Out[2]:

0

True

1

False

2

True

3

False

4

False

5

False

6

True

7

True

8

True

dtype: bool

Ceci peut être utilisé comme un index booléen pour ne renvoyer que les animaux contenant la lettre 'a':

In [3]: animals[animals.str.contains('a')]

Out[3]:

0

cat

2

bear

6 rabbit

7

snake

dtype: object

str.startswith méthodes str.startswith et str.endswith fonctionnent de manière similaire, mais elles acceptent également les tuples comme entrées.

In [4]: animals[animals.str.startswith(('b', 'c'))]

# Returns animals starting with 'b' or 'c'

Out[4]:

0

cat

2 bear

3

cow

4 bird

dtype: object

Capitalisation de chaînes

In [1]: ser = pd.Series(['lORem ipSuM', 'Dolor sit amet', 'Consectetur Adipiscing Elit'])
Convertir tout en majuscule:

https://riptutorial.com/fr/home

84

In [2]: ser.str.upper()

Out[2]:

0

LOREM IPSUM

1

DOLOR SIT AMET

2 CONSECTETUR ADIPISCING ELIT

dtype: object

Tout en minuscule:

In [3]: ser.str.lower()

Out[3]:

0

lorem ipsum

1

dolor sit amet

2 consectetur adipiscing elit

dtype: object

Capitaliser le premier caractère et minuscule le reste:

In [4]: ser.str.capitalize()

Out[4]:

0

Lorem ipsum

1

Dolor sit amet

2 Consectetur adipiscing elit

dtype: object

Convertissez chaque chaîne en une titlecase (mettez en majuscule le premier caractère de chaque mot dans chaque chaîne, minuscule le reste):

In [5]: ser.str.title()

Out[5]:

0

Lorem Ipsum

1

Dolor Sit Amet

2 Consectetur Adipiscing Elit

dtype: object

Permuter les cas (convertir les minuscules en majuscules et vice versa):

In [6]: ser.str.swapcase()

Out[6]:

0

LorEM IPsUm

1

dOLOR SIT AMET

2 cONSECTETUR aDIPISCING eLIT

dtype: object

Outre ces méthodes qui modifient la capitalisation, plusieurs méthodes peuvent être utilisées pour vérifier la capitalisation des chaînes.

In [7]: ser = pd.Series(['LOREM IPSUM', 'dolor sit amet', 'Consectetur Adipiscing Elit'])

Vérifiez si tout est en minuscule:

In [8]: ser.str.islower() Out[8]:

https://riptutorial.com/fr/home

85

0 False

1

True

2 False

dtype: bool

Est-ce tout en majuscule:

In [9]: ser.str.isupper()

Out[9]:

0

True

1 False

2 False

dtype: bool

Est-ce une chaîne titlecased:

In [10]: ser.str.istitle()

Out[10]:

0 False

1 False

2

True

dtype: bool

Lire Manipulation de cordes en ligne: https://riptutorial.com/fr/pandas/topic/2372/manipulation-decordes

https://riptutorial.com/fr/home

86

Chapitre 23: Manipulation simple de DataFrames

Examples

Supprimer une colonne dans un DataFrame
Il existe plusieurs façons de supprimer une colonne dans un DataFrame.

import numpy as np import pandas as pd

np.random.seed(0)

pd.DataFrame(np.random.randn(5, 6), columns=list('ABCDEF'))

print(df)

# Output:

#

A

B

C

D

E

F

# 0 -0.895467 0.386902 -0.510805 -1.180632 -0.028182 0.428332

# 1 0.066517 0.302472 -0.634322 -0.362741 -0.672460 -0.359553

# 2 -0.813146 -1.726283 0.177426 -0.401781 -1.630198 0.462782

# 3 -0.907298 0.051945 0.729091 0.128983 1.139401 -1.234826

# 4 0.402342 -0.684810 -0.870797 -0.578850 -0.311553 0.056165

1) Utiliser del

del df['C']

print(df)

# Output:

#

A

B

D

E

F

# 0 -0.895467 0.386902 -1.180632 -0.028182 0.428332

# 1 0.066517 0.302472 -0.362741 -0.672460 -0.359553

# 2 -0.813146 -1.726283 -0.401781 -1.630198 0.462782

# 3 -0.907298 0.051945 0.128983 1.139401 -1.234826

# 4 0.402342 -0.684810 -0.578850 -0.311553 0.056165

2) Utiliser drop

df.drop(['B', 'E'], axis='columns', inplace=True) # or df = df.drop(['B', 'E'], axis=1) without the option inplace=True

print(df)

# Output:

#

A

D

F

# 0 -0.895467 -1.180632 0.428332

# 1 0.066517 -0.362741 -0.359553

# 2 -0.813146 -0.401781 0.462782

# 3 -0.907298 0.128983 -1.234826

# 4 0.402342 -0.578850 0.056165

https://riptutorial.com/fr/home

87

3) Utilisation de drop avec les numéros de colonne
Pour utiliser des nombres entiers de colonne au lieu de noms (rappelez-vous que les index de colonne commencent à zéro):

df.drop(df.columns[[0, 2]], axis='columns')

print(df)

# Output:

#

D

# 0 -1.180632

# 1 -0.362741

# 2 -0.401781

# 3 0.128983

# 4 -0.578850

Renommer une colonne

df = pd.DataFrame({'old_name_1': [1, 2, 3], 'old_name_2': [5, 6, 7]})

print(df)

# Output:

# old_name_1

#0

1

#1

2

#2

3

old_name_2 5 6 7

Pour renommer une ou plusieurs colonnes, transmettez les anciens noms et les nouveaux noms en tant que dictionnaire:

df.rename(columns={'old_name_1': 'new_name_1', 'old_name_2': 'new_name_2'}, inplace=True)

print(df)

# Output:

# new_name_1 new_name_2

#0

1

5

#1

2

6

#2

3

7

Ou une fonction:

df.rename(columns=lambda x: x.replace('old_', '_new'), inplace=True)

print(df)

# Output:

# new_name_1 new_name_2

#0

1

5

#1

2

6

#2

3

7

Vous pouvez également définir df.columns comme liste des nouveaux noms:

df.columns = ['new_name_1','new_name_2'] print(df) # Output: # new_name_1 new_name_2

https://riptutorial.com/fr/home

88

#0

1

5

#1

2

6

#2

3

7

Plus de détails peuvent être trouvés ici .
Ajouter une nouvelle colonne

df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})
print(df) # Output: # AB #0 1 4 #1 2 5 #2 3 6

Directement attribuer

df['C'] = [7, 8, 9]
print(df) # Output: # ABC #0 1 4 7 #1 2 5 8 #2 3 6 9

Ajouter une colonne constante

df['C'] = 1
print(df)
# Output: # ABC #0 1 4 1 #1 2 5 1 #2 3 6 1

Colonne comme expression dans les autres colonnes

df['C'] = df['A'] + df['B']
# print(df) # Output: # ABC #0 1 4 5 #1 2 5 7 #2 3 6 9
df['C'] = df['A']**df['B']

https://riptutorial.com/fr/home

89

